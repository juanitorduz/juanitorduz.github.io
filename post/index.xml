<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Dr. Juan Camilo Orduz</title>
    <link>/post/</link>
    <description>Recent content in Posts on Dr. Juan Camilo Orduz</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 10 Jan 2020 00:00:00 +0000</lastBuildDate>
    
	<atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    
    
    <item>
      <title>Drawing Manifolds in LaTeX with TikZ</title>
      <link>/manifold_fig_latex/</link>
      <pubDate>Fri, 10 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/manifold_fig_latex/</guid>
      <description>We give some LaTex code to create figures of manifolds with boundaries.</description>
    </item>
    
    <item>
      <title>Open Data: Germany Maps Viz</title>
      <link>/germany_plots/</link>
      <pubDate>Tue, 07 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/germany_plots/</guid>
      <description>In this post I want to show how to use public available (open) data to create geo visualizations in python. Maps are a great way to communicate and compare information when working with geolocation data. There are many frameworks to plot maps, here I focus on matplotlib and geopandas (and give a glimpse of mplleaflet).
Reference: A very good introduction to matplotlib is the chapter on Visualization with Matplotlib from the Python Data Science Handbook by Jake VanderPlas.</description>
    </item>
    
    <item>
      <title>The Graph Laplacian &amp; Semi-Supervised Clustering</title>
      <link>/semi_supervised_clustering/</link>
      <pubDate>Thu, 05 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/semi_supervised_clustering/</guid>
      <description>In this post we want to explore the semi-supervided algorithm presented Eldad Haber in the BMS Summer School 2019: Mathematics of Deep Learning, during 19 - 30 August 2019, at the Zuse Institute Berlin. He developed an implementation in Matlab which you can find in this GitHub repository. In addition, please find the corresponding slides here.
Prepare Notebook import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns; sns.</description>
    </item>
    
    <item>
      <title>The Lapacian on the 2-Torus</title>
      <link>/laplacian_2torus/</link>
      <pubDate>Sun, 13 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/laplacian_2torus/</guid>
      <description>In this blog post I want to describe the explicit computation of the Laplacian on differential forms on the \(2\)-Torus \(T^2\subset \mathbb{R}^3\). This surface can be obtained by rotating the circle \((x-a)^2+y^2=r^2\) around the \(z\)-axis (\(0&amp;lt;r&amp;lt;a\)). Locally, this surface can be parametrized by the equations \[ x = (a+r\cos u)\cos v,\\ y = (a+r\cos u)\sin v,\\ z = r\sin u, \]
where \(0&amp;lt;u,v&amp;lt;2\pi\).</description>
    </item>
    
    <item>
      <title>PyData Berlin 2019: Gaussian Processes for Timeseries Forecasting</title>
      <link>/gaussian_process_time_series/</link>
      <pubDate>Thu, 10 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/gaussian_process_time_series/</guid>
      <description>In this notebook we run some experiments to demonstrate how we can use Gaussian Processes in the context of time series forecasting.
This material is part of a talk on Gaussian Process for Time Series Analysis presented at the PyCon DE &amp;amp; PyData 2019 Conference in Berlin.
Video    Slides
Here you can find the slides of the talk.
Suggestions and comments are always welcome!
References:
 Gaussian Processes for Machine Learning, by Carl Edward Rasmussen and Christopher K.</description>
    </item>
    
    <item>
      <title>satRday Berlin 2019: Remedies for Severe Class Imbalance</title>
      <link>/class_imbalance/</link>
      <pubDate>Sat, 15 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/class_imbalance/</guid>
      <description>In this post I present a concrete case study illustrating some techniques to improve model performance in class-imbalanced classification problems. The methodologies described here are based on Chapter 16: Remedies for Severe Class Imbalance of the (great!) book Applied Predictive Modeling by Max Kuhn and Kjell Johnson. I absolutely recommend this reference to anyone interested in predictive modeling.
This notebook should serve as an extension of my talk given at satRday Berlin 2019: A conference for R users in Berlin.</description>
    </item>
    
    <item>
      <title>Seasonal Bump Functions</title>
      <link>/bump_func/</link>
      <pubDate>Thu, 11 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/bump_func/</guid>
      <description>Motivated by the nice talk on Winning with Simple, even Linear, Models by Vincent D. Warmerdam, I briefly describe how to construct certain class of bump functions to encode seasonal variables in R.
Prepare Notebook library(glue) library(lubridate) library(magrittr) library(tidyverse)  Generate Data Let us generate a time sequence variable stored in a tibble.
# Define time sequence. t &amp;lt;- seq.Date(from = as.Date(&amp;quot;2017-07-01&amp;quot;), to = as.Date(&amp;quot;2019-04-01&amp;quot;), by = &amp;quot;day&amp;quot;) # Store it in a tibble.</description>
    </item>
    
    <item>
      <title>An Introduction to Gaussian Process Regression</title>
      <link>/gaussian_process_reg/</link>
      <pubDate>Mon, 08 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/gaussian_process_reg/</guid>
      <description>Updated Version: 2019/09/21 (Extension + Minor Corrections)
After a sequence of preliminary posts (Sampling from a Multivariate Normal Distribution and Regularized Bayesian Regression as a Gaussian Process), I want to explore a concrete example of a gaussian process regression. We continue following Gaussian Processes for Machine Learning, Ch 2.
Other recommended references are:
 Gaussian Processes for Timeseries Modeling by S. Roberts, M. Osborne, M. Ebden, S. Reece, N. Gibson &amp;amp; S.</description>
    </item>
    
    <item>
      <title>Bayesian Regression as a Gaussian Process</title>
      <link>/reg_bayesian_regression/</link>
      <pubDate>Mon, 01 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/reg_bayesian_regression/</guid>
      <description>In this post we study the Bayesian Regression model to explore and compare the weight and function space and views of Gaussian Process Regression as described in the book Gaussian Processes for Machine Learning, Ch 2. We follow this reference very closely (and encourage to read it!). Our main objective is to illustrate the concepts and results through a concrete example. We use PyMC3 to run bayesian sampling.
References:</description>
    </item>
    
    <item>
      <title>Sampling from a Multivariate Normal Distribution</title>
      <link>/multivariate_normal/</link>
      <pubDate>Sat, 23 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/multivariate_normal/</guid>
      <description>In this post I want to describe how to sample from a multivariate normal distribution following section A.2 Gaussian Identities of the book Gaussian Processes for Machine Learning. This is a first step towards exploring and understanding Gaussian Processes methods in machine learning.
Multivariate Normal Distribution Recall that a random vector \(X = (X_1, \cdots, X_d)\) has a multivariate normal (or Gaussian) distribution if every linear combination
$$ \sum_{i=1}^{d} a_iX_i, \quad a_i\in\mathbb{R} $$ is normally distributed.</description>
    </item>
    
    <item>
      <title>Dockerize a ShinyApp</title>
      <link>/dockerize-a-shinyapp/</link>
      <pubDate>Sat, 02 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/dockerize-a-shinyapp/</guid>
      <description>In this post I want to describe how to dockerize a simple Shiny App. Docker is a great way of sharing and deploying projects. You can download it here.
I highly recommend the R Docker tutorial.
Assume you have a project folder structure as follows:
. +-- project.Rproj +-- app.R +-- R | +-- script_1.R | +-- script_2.R +-- data | +-- data_df.rds | +-- raw_data.csv  The script app.</description>
    </item>
    
    <item>
      <title>The Spectral Theorem for Matrices</title>
      <link>/the-spectral-theorem-for-matrices/</link>
      <pubDate>Sat, 02 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/the-spectral-theorem-for-matrices/</guid>
      <description>When working in data analysis it is almost impossible to avoid using linear algebra, even if it is on the background, e.g. simple linear regression. In this post I want to discuss one of the most important theorems of finite dimensional vector spaces: the spectral theorem. The objective is not to give a complete and rigorous treatment of the subject, but rather show the main ingredientes, some examples and applications.</description>
    </item>
    
    <item>
      <title>Movie Plots Text Generation with Keras</title>
      <link>/movie_plot_text_gen/</link>
      <pubDate>Sun, 13 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/movie_plot_text_gen/</guid>
      <description>In this post I show some text generation experiments I ran using LSTM with Keras. For the preprocessing and tokenization I used SpaCy. The aim is not to present a completed project, but rather a first step which should be then iterated.
Resources There are many great resources and blog posts about the subject (and similar experiments). Here I mention the ones I found particularly useful for the general theory:</description>
    </item>
    
    <item>
      <title>Exploring the Curse of Dimensionality - Part II.</title>
      <link>/exploring-the-curse-of-dimensionality-part-ii./</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/exploring-the-curse-of-dimensionality-part-ii./</guid>
      <description>I continue exploring the curse of dimensionality. Following the analysis form Part I., I want to discuss another consequence of sparse sampling in high dimensions: sample points are close to an edge of the sample. This post is based on The Elements of Statistical Learning, Section 2.5, which I encourage to read!
Uniform Sampling Consider \(N\) data points uniformly distributed in a \(p\)-dimensional unit ball centered at the origin. Suppose we consider a nearest-neighbor estimate at the origin.</description>
    </item>
    
    <item>
      <title>Text Mining, Networks and Visualization: Plebiscito Tweets</title>
      <link>/text-mining-networks-and-visualization-plebiscito-tweets/</link>
      <pubDate>Thu, 20 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/text-mining-networks-and-visualization-plebiscito-tweets/</guid>
      <description>Nowadays social media generates a vast amount of raw data (text, images, videos, etc). It is a very interesting challenge to discover techniques to get insights on the content and development of social media data. In addition, as a fundamental component of the analysis, it is important to find ways of communicating the results, i.e. data visualization. In this post I want to present a small case study where I analyze Twitter text data.</description>
    </item>
    
    <item>
      <title>Exploring the Curse of Dimensionality - Part I.</title>
      <link>/exploring-the-curse-of-dimensionality-part-i./</link>
      <pubDate>Sun, 09 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/exploring-the-curse-of-dimensionality-part-i./</guid>
      <description>In this post I want to present the notion of curse of dimensionality following a suggested exercise (Chapter 4 - Ex. 4) of the book An Introduction to Statistical Learning, written by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani.
When the number of features \(p\) is large, there tends to be a deterioration in the performance of KNN and other local approaches that perform prediction using only observations that are near the test observation for which a prediction must be made.</description>
    </item>
    
    <item>
      <title>From Pelican to Blogdown</title>
      <link>/pelican_to_blogdown/</link>
      <pubDate>Sun, 02 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/pelican_to_blogdown/</guid>
      <description>Here I want to discuss my transition from Pelican to Blogdown and present some personal learnings. In June 2017 I decided to build a personal website/portafolio. I chose Pelican, because:
 It is written in Python, which was the programing language I was mainly working on.
 I wanted to include some Jupyter notebook I had already written.
 A great post: Building a data science portfolio: Making a data science blog explaining the procedure and using GitHub Pages to publist it.</description>
    </item>
    
    <item>
      <title>\(S^1\)-Equivariant Dirac operators on the Hopf Fibration</title>
      <link>/hopf_fibration/</link>
      <pubDate>Sun, 11 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/hopf_fibration/</guid>
      <description>In this expository article I discuss the definition and basic properties of the Hopf fibration, with particular emphasis on Dirac-type operators induced, in the sense of Brüning and Heintze, by the Hodge-de Rham and spin-Dirac operators. In addition, we compute the Dirac-Schrödinger type operator introduced in my PhD thesis.</description>
    </item>
    
    <item>
      <title>Introduction to R Plumber : Expose a Caret model to a web API</title>
      <link>/intro_plumber/</link>
      <pubDate>Fri, 12 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/intro_plumber/</guid>
      <description>In this post we present a simple example of how to expose a prediction model to a web API using the Plumber package.</description>
    </item>
    
    <item>
      <title>Circle Radius Fit for a Cloud of Points</title>
      <link>/circle-radius-fit-for-a-cloud-of-points/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/circle-radius-fit-for-a-cloud-of-points/</guid>
      <description>We explore how to include an R notebook into a pelican post. As an example, we describe how to fit a circle onto a cloud of points.</description>
    </item>
    
    <item>
      <title>From Bachelor to PhD: Geometric and Topological Methods for Quantum Field Theory</title>
      <link>/vdl_experience/</link>
      <pubDate>Thu, 02 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/vdl_experience/</guid>
      <description>We give an introduction to PyMC3, a probabilistic programming framework written in Python. We revise the basic mahematical theory and present two concrete examples.</description>
    </item>
    
    <item>
      <title>PyData Berlin 2018: On Laplacian Eigenmaps for Dimensionality Reduction</title>
      <link>/laplacian_eigenmaps_dim_red/</link>
      <pubDate>Sun, 08 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/laplacian_eigenmaps_dim_red/</guid>
      <description>This post contains the slides and material from a talk I gave at PyData Berlin 2018. I presented the paper &lt;em&gt;Laplacian Eigenmaps for Dimensionality Reduction and Data Representation&lt;/em&gt; by &lt;a href=&#34;http://web.cse.ohio-state.edu/~belkin.8/&#34;&gt;Mikhail Belkin&lt;/a&gt; and &lt;a href=&#34;http://people.cs.uchicago.edu/~niyogi/&#34;&gt;Partha Niyogi&lt;/a&gt;.</description>
    </item>
    
    <item>
      <title>Probability that a given observation is part of a bootstrap sample?</title>
      <link>/bootstrap/</link>
      <pubDate>Wed, 29 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/bootstrap/</guid>
      <description>We study the problem of computing the probability that a given observation is part of a bootstrap sample. We include some numerical simulations.</description>
    </item>
    
    <item>
      <title>Induced Dirac-Schrödinger operators on semi-free circle quotients</title>
      <link>/phd/</link>
      <pubDate>Sat, 11 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/phd/</guid>
      <description>I present the content of my PhD Thesis in mathematics, which has now been published in The Journal of Geometric Analysis.</description>
    </item>
    
    <item>
      <title>Introduction to Bayesian Modeling with PyMC3</title>
      <link>/intro_pymc3/</link>
      <pubDate>Sun, 13 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/intro_pymc3/</guid>
      <description>We give an introduction to PyMC3, a probabilistic programming framework written in Python. We revise the basic mahematical theory and present two concrete examples.</description>
    </item>
    
    <item>
      <title>Web scraping with Beautiful Soup: Plebiscito Colombia (October 2nd)</title>
      <link>/plebiscito/</link>
      <pubDate>Sun, 09 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/plebiscito/</guid>
      <description>We describe how to use Beautiful Soup to scrape the official goverment website in order to get the results of the peace referendum  in Colombia.</description>
    </item>
    
    <item>
      <title>The Dirac operator on the 2-sphere</title>
      <link>/the-dirac-operator-on-the-2-sphere/</link>
      <pubDate>Thu, 29 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/the-dirac-operator-on-the-2-sphere/</guid>
      <description>The objective of this post is to explore MathJax, a JavaScript display engine for LaTeX. Being my first post writen with this tool, I want to present a short but fun example: I will give a description of the explicit computation of the spin-Dirac operator (of the unique complex spinor bundle!) on the 2-sphere \(S^2\) equipped with the standar round metric. A more detailed treatment can be found in my expository paper.</description>
    </item>
    
    <item>
      <title>Python Exercise: Distance to Rectangle</title>
      <link>/rectangle/</link>
      <pubDate>Wed, 28 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/rectangle/</guid>
      <description>In this first post we get started with a small python script to explore the basic capabilities of Pelican.</description>
    </item>
    
  </channel>
</rss>