<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Posts on Dr. Juan Camilo Orduz</title>
    <link>/post/</link>
    <description>Recent content in Posts on Dr. Juan Camilo Orduz</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Wed, 07 Jun 2023 00:00:00 +0000</lastBuildDate><atom:link href="/post/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>ATE Estimation for Count Data</title>
      <link>/causal_inference_negative_binomial/</link>
      <pubDate>Wed, 07 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>/causal_inference_negative_binomial/</guid>
      <description>This notebook is a continuation of the previous notebook on ATE estimation for binary data with logistic regression based on the sequence of (great!) posts by Solomon Kurz. In this notebook, we will focus on count data. We reproduce in python an example presented in the post Causal inference with count regression by Solomon Kurz. Our intention is to simply show how to port these type of model to bambi.</description>
    </item>
    
    <item>
      <title>ATE Estimation with Logistic Regression</title>
      <link>/causal_inference_logistic/</link>
      <pubDate>Sun, 04 Jun 2023 00:00:00 +0000</pubDate>
      
      <guid>/causal_inference_logistic/</guid>
      <description>In this notebook, I want to reproduce some components of the extensive blog post Causal inference with Bayesian models by Solomon Kurz. Specifically, I want to deep dive into the logistic regression model used to estimate the average treatment effect (ATE) of the study Internet-accessed sexually transmitted infection (e-STI) testing and results service: A randomised, single-blind, controlled trial by Wilson, et.al. I can only recommend to read the original sequence of posts Solomon has written on causal inference.</description>
    </item>
    
    <item>
      <title>Bayesian Methods in Modern Marketing Analytics Webinar with PyMC Labs</title>
      <link>/marketing_bayes_webinar/</link>
      <pubDate>Wed, 31 May 2023 00:00:00 +0000</pubDate>
      
      <guid>/marketing_bayes_webinar/</guid>
      <description>Here I want to share the recording and slides of the webinar Bayesian Methods in Modern Marketing Analytics in collaboration with PyMC Labs.
Abstract: During the webinar, we will discuss some of the most crucial topics in marketing analytics: media spend optimization through media mix models and experimentation, and customer lifetime value estimation. We will approach these topics from a Bayesian perspective, as it gives us great tools to have better models and more actionable insights.</description>
    </item>
    
    <item>
      <title>How to vectorize an scikit-learn transformer over a numpy array?</title>
      <link>/vectorize_sklearn_transformer/</link>
      <pubDate>Mon, 01 May 2023 00:00:00 +0000</pubDate>
      
      <guid>/vectorize_sklearn_transformer/</guid>
      <description>In this short post, I show how to vectorize an scikit-learn transformer over a numpy array. That is, how to apply a transformer along a specific axes of a numpy array. I have found this to be particularly useful when working with output sample posterior distributions from a bayesian model where I want to apply a transformer to each sample. This is not particularly difficult, but I always forget how to do it, so I thought I would write it down once and for all 😄.</description>
    </item>
    
    <item>
      <title>Counting the Number of Kitas per PLZ in Berlin using a Hierarchical Bayesian Model</title>
      <link>/kitas-hierarchical/</link>
      <pubDate>Fri, 28 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>/kitas-hierarchical/</guid>
      <description>This notebook is the continuation of data gathering and data analysis post Open Data: Berlin Kitas. In this second part we use the data gathered to model the number of Kitas per PLZ in Berlin using a hierarchical bayesian model. The hierarchy is defined by the Berlin districts. The objective is to develop a sound basic model which can be enhanced in the future with a richer data set.</description>
    </item>
    
    <item>
      <title>Simple Hierarchical Model with NumPyro: Cookie Chips Example</title>
      <link>/cookies_example_numpyro/</link>
      <pubDate>Mon, 24 Apr 2023 00:00:00 +0000</pubDate>
      
      <guid>/cookies_example_numpyro/</guid>
      <description>This notebook presents a simple example of a hierarchical model using NumPyro. The example is based on the cookie chips example in presented in the post Introduction to Bayesian Modeling with PyMC3. There are many great resources regarding bayesian hierarchical model and probabilistic programming NumPyro. This notebook aims to provide a succinct simple example to get started.
Remark: Well, the real reason is that I want to get acquainted other probabilistic programming libraries in order to abstract the core principles of probabilistic programming.</description>
    </item>
    
    <item>
      <title>Experimentation, Non-Compliance and Instrumental Variables with PyMC</title>
      <link>/iv_pymc/</link>
      <pubDate>Mon, 20 Feb 2023 00:00:00 +0000</pubDate>
      
      <guid>/iv_pymc/</guid>
      <description>In this notebook we present an example of how to use PyMC to estimate the effect of a treatment in an experiment where there is non-compliance through the use of instrumental variables.
By non-compliance we mean that the treatment assignment does not guarantee that the treatment is actually received by the treated. The main challenge is that we can not simply estimate the treatment effect as a difference in means since the non-compliance mechanism is most of the time not at random and may introduce confounders.</description>
    </item>
    
    <item>
      <title>Cohort Revenue &amp; Retention Analysis: A Bayesian Approach</title>
      <link>/revenue_retention/</link>
      <pubDate>Mon, 23 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/revenue_retention/</guid>
      <description>In this notebook we extend the cohort retention model presented in the post Cohort Retention Analysis with BART so that we just model retention and per cohort simultaneously (we recommend reading the referenced post before this one). The idea is to keep modeling the retention using a Bayesian Additive Regression Tree (BART) model (see pymc-bart) and linearly model the revenue per cohort using a Gamma distribution. We couple the retention and revenue components in a similar way as presented in the notebook Introduction to Bayesian A/B Testing.</description>
    </item>
    
    <item>
      <title>Cohort Retention Analysis with BART</title>
      <link>/retention_bart/</link>
      <pubDate>Mon, 02 Jan 2023 00:00:00 +0000</pubDate>
      
      <guid>/retention_bart/</guid>
      <description>In this notebook we study an alternative approach for the cohort analysis problem presented in A Simple Cohort Retention Analysis in PyMC. Instead of using a linear model to estimate the retention rate, we use a Bayesian Additive Regression Tree (BART) model(see pymc-bart). The BART model is a flexible non-parametric model that can be used to model complex relationships between the response and the predictors.
Prepare Notebook import arviz as az import matplotlib.</description>
    </item>
    
    <item>
      <title>A Simple Cohort Retention Analysis in PyMC</title>
      <link>/retention/</link>
      <pubDate>Tue, 20 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>/retention/</guid>
      <description>In this notebook we present a simple approach to study cohort retention analysis through a simulated data set. The aim is to understand how retention rates change over time and provide a simple model to predict them (with uncertainty estimates!). We do not expect this technique to be a silver bullet for all retention problems, but rather a simple approach to get started with the problem.
Remark: A motivation for this notebook was the great post Bayesian Age/Period/Cohort Models in Python with PyMC by Austin Rochford.</description>
    </item>
    
    <item>
      <title>Geo-Experimentation via Time Based Regression in PyMC</title>
      <link>/time_based_regression_pymc/</link>
      <pubDate>Thu, 01 Dec 2022 00:00:00 +0000</pubDate>
      
      <guid>/time_based_regression_pymc/</guid>
      <description>Introduction In this notebook I describe and present an implementation of the time based regression (TBR) approach to marketing campaign analysis in the context of geo experimentation presented in the paper Estimating Ad Effectiveness using Geo Experiments in a Time-Based Regression Framework by Jouni Kerman, Peng Wang and Jon Vaver (Google, Inc. 2017). I strongly recommend reading the paper as it is quite clear in the exposition of the approach and presents some simulation results.</description>
    </item>
    
    <item>
      <title>Offline Campaign Analysis Measurement: A journey through causal impact, geo-experimentation and synthetic control</title>
      <link>/wolt_ds_meetup/</link>
      <pubDate>Tue, 25 Oct 2022 00:00:00 +0000</pubDate>
      
      <guid>/wolt_ds_meetup/</guid>
      <description>In October 2022 I had the opportunity to give a talk at the Helsinki Data Science Meetup hosted by Wolt. Here I want to share the recording of my talk.
Abstract: The talk will show how to measure offline campaigns using causal inference techniques. In particular it’ll focus on tapping into the potential of synthetic control, geo-experiments via time-based regression, and Google’s Causal-Impact Method.
   Code to generate data You can find the raw data here and the code here.</description>
    </item>
    
    <item>
      <title>Scikit-Learn Example in PyMC: Gaussian Process Classifier</title>
      <link>/sklearn_pymc_classifier/</link>
      <pubDate>Sat, 24 Sep 2022 00:00:00 +0000</pubDate>
      
      <guid>/sklearn_pymc_classifier/</guid>
      <description>In this notebook we want to describe how to port a couple of classification examples from scikit-learn’s documentation (classifier comparison) to PyMC. We focus in the classical moons synthetic dataset.
Prepare Notebook import arviz as az import matplotlib.pyplot as plt import numpy as np import pandas as pd import pymc as pm import pymc.sampling_jax import seaborn as sns from sklearn.datasets import make_moons from sklearn.metrics import accuracy_score from sklearn.model_selection import train_test_split plt.</description>
    </item>
    
    <item>
      <title>Synthetic Control in PyMC</title>
      <link>/synthetic_control_pymc/</link>
      <pubDate>Tue, 09 Aug 2022 00:00:00 +0000</pubDate>
      
      <guid>/synthetic_control_pymc/</guid>
      <description>Synthetic control can be considered “the most important innovation in the policy evaluation literature in the last few years” (see The State of Applied Econometrics: Causality and Policy Evaluation by Susan Athey and Guido W. Imbens).
 In this notebook we provide an example of how to implement a synthetic control problem in PyMC to answer a “what if this had happened?” type of question in the context of causal inference.</description>
    </item>
    
    <item>
      <title>Modeling Short Time Series with Prior Knowledge in PyMC</title>
      <link>/short_time_series_pymc/</link>
      <pubDate>Tue, 19 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>/short_time_series_pymc/</guid>
      <description>In this notebook I want to reproduce in PyMC the methodology described in the amazing blog post Modeling Short Time Series with Prior Knowledge by Tim Radtke to forecast short time series using bayesian transfer learning 🚀. The main idea is to transfer information (e.g. long term seasonality) from a long time series to a short time series via prior distributions. Tim’s blog post treats a very concrete example where all the concepts become very concrete.</description>
    </item>
    
    <item>
      <title>Time-Varying Regression Coefficients via Gaussian Random Walk in PyMC</title>
      <link>/bikes_pymc/</link>
      <pubDate>Sun, 03 Jul 2022 00:00:00 +0000</pubDate>
      
      <guid>/bikes_pymc/</guid>
      <description>In this notebook we want to illustrate how to use PyMC to fit a time-varying coefficient regression model. The motivation comes from post Exploring Tools for Interpretable Machine Learning where we studied a time series problem, regarding the prediction of the number of bike rentals, from a machine learning perspective. Concretely, we fitted and compared two machine learning models: a linear regression with interactions and a gradient boost model (XGBoost).</description>
    </item>
    
    <item>
      <title>Data Talks Club: Machine Learning in Marketing</title>
      <link>/machine_learning_marketing/</link>
      <pubDate>Tue, 17 May 2022 00:00:00 +0000</pubDate>
      
      <guid>/machine_learning_marketing/</guid>
      <description>On Friday 13th of May 2022 I was invited to join Alexey Grigorev in an event organised by DataTalks.Club to talk about Machine Learning in Marketing. It was a really insightful discussion and i would like to thanks the organizers who make it possible. Here is the recording:
   Here are some useful links and resources about the subject:
 Relevant blog post I have written about the subject:</description>
    </item>
    
    <item>
      <title>PyConDE &amp; PyData Berlin 2022: Introduction to Uplift Modeling</title>
      <link>/uplift/</link>
      <pubDate>Mon, 11 Apr 2022 00:00:00 +0000</pubDate>
      
      <guid>/uplift/</guid>
      <description>In this notebook we present a simple example of uplift modeling estimation via meta-models using causalml and scikit-uplift. For a more detailed introduction to uplift modeling, see:
 Diemert, Eustache, et.al. (2020) “A Large Scale Benchmark for Uplift Modeling”
 Gutierrez, P., &amp;amp; Gérardy, J. Y. (2017). “Causal Inference and Uplift Modelling: A Review of the Literature”
 Karlsson, H. (2019) “Uplift Modeling: Identifying Optimal Treatment Group Allocation and Whom to Contact to Maximize Return on Investment”</description>
    </item>
    
    <item>
      <title>Gamma-Gamma Model of Monetary Value in PyMC</title>
      <link>/gamma_gamma_pymc/</link>
      <pubDate>Tue, 29 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>/gamma_gamma_pymc/</guid>
      <description>In this notebook we describe how to fit Fader’s and Hardie’s gamma-gamma model presented in the paper “RFM and CLV: Using Iso-value Curves for Customer Base Analysis” and the note “The Gamma-Gamma Model of Monetary Value”. The approach is very similar as the one presented in the previous post BG/NBD Model in PyMC where we simply ported the log-likelihood of the lifetimes package from numpy to theano.
Prepare Notebook import arviz as az import matplotlib.</description>
    </item>
    
    <item>
      <title>BG/NBD Model in PyMC</title>
      <link>/bg_nbd_pymc/</link>
      <pubDate>Thu, 03 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>/bg_nbd_pymc/</guid>
      <description>In this notebook we show how to port the BG/NBD model from the the lifetimes (developed mainly by Cameron Davidson-Pilon) package to pymc. The BG/NBD model, introduced in the seminal paper “Counting Your Customers” the Easy Way: An Alternative to the Pareto/NBD Model by Peter S. Fader, Bruce G. S. Hardie and Ka Lok Lee in 2005, is used to
 predict future purchasing patterns, which can then serve as an input into “lifetime value” calculations, in the “non-contractual” setting (i.</description>
    </item>
    
    <item>
      <title>Media Effect Estimation with PyMC: Adstock, Saturation &amp; Diminishing Returns</title>
      <link>/pymc_mmm/</link>
      <pubDate>Fri, 11 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>/pymc_mmm/</guid>
      <description>In this notebook we present a concrete example of estimating the media effects via bayesian methods, following the strategy outlined in Google’s paper Jin, Yuxue, et al. “Bayesian methods for media mix modeling with carryover and shape effects.” (2017). This example can be considered the continuation of the post Media Effect Estimation with Orbit’s KTR Model. However, it is not strictly necessary to read before as we make this notebook self-contained.</description>
    </item>
    
    <item>
      <title>Media Effect Estimation with Orbit&#39;s KTR Model</title>
      <link>/orbit_mmm/</link>
      <pubDate>Fri, 04 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>/orbit_mmm/</guid>
      <description>In this notebook we want to experiment to the new KTR model included in the new orbit’s release (1.1). In particular, we are interested in its applications to media effects estimation in the context of media mix modeling. This is one of the applications for the KTR model by the Uber’s team, see the corresponding paper Edwin, Ng, et al. “Bayesian Time Varying Coefficient Model with Applications to Marketing Mix Modeling”.</description>
    </item>
    
    <item>
      <title>Unobserved Components Model as a Bayesian Model with PyMC</title>
      <link>/uc_pymc/</link>
      <pubDate>Fri, 10 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>/uc_pymc/</guid>
      <description>In this notebook I want to deep-dive into the idea of wrapping a statsmodels UnobservedComponents model as a bayesian model with PyMC described in the (great!) post Fast Bayesian estimation of SARIMAX models. This is a nice excuse to get into some internals of how PyMC works. I hope this can serve as a complement to the original post mentioned above. This post has two parts: In the first one we fit a UnobservedComponents model to a simulated time series.</description>
    </item>
    
    <item>
      <title>ISLR2 - Survival Analysis Lab (lifelines)</title>
      <link>/islr2_survival_analysis/</link>
      <pubDate>Wed, 01 Sep 2021 00:00:00 +0000</pubDate>
      
      <guid>/islr2_survival_analysis/</guid>
      <description>In this notebook we provide a python implementation of the lab from the Survival Analysis - Chapter 11 of the second edition of the book An Introduction to Statistical Learning (Second Edition). You can find a free pdf version of the book here. We will use the lifelines python package, which you can find in this repository. There is a nice introduction into survival analysis on the documentation. There are also many concrete examples and guidelines to use the package.</description>
    </item>
    
    <item>
      <title>Exploring Tools for Interpretable Machine Learning</title>
      <link>/interpretable_ml/</link>
      <pubDate>Thu, 01 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>/interpretable_ml/</guid>
      <description>In this notebook we want to test various ways of getting a better understanding on how non-trivial machine learning models generate predictions and how features interact with each other. This is in general not straight forward and key components are (1) understanding on the input data and (2) domain knowledge on the problem. Two great references on the subject are:
 Interpretable Machine Learning, A Guide for Making Black Box Models Explainable by Christoph Molnar Interpretable Machine Learning with Python by Serg Masís  Note that the methods discussed in this notebook are not related with causality.</description>
    </item>
    
    <item>
      <title>Feature Engineering: patsy as FormulaTransformer</title>
      <link>/formula_transformer/</link>
      <pubDate>Sat, 01 May 2021 00:00:00 +0000</pubDate>
      
      <guid>/formula_transformer/</guid>
      <description>In this notebook I want to describe how to create features inside scikit-learn pipelines using patsy-like formulas. I have used this approach to generate features in a previous post: GLM in PyMC3: Out-Of-Sample Predictions, so I will consider the same data set here for the sake of comparison.
Remark: Very recently (2021-09-01) I discovered there is an implementation of this transformer in scikit-lego, see PatsyTransformer. In addition, please refer to the great tutorial on patsy in calmcode.</description>
    </item>
    
    <item>
      <title>GLM in PyMC3: Out-Of-Sample Predictions</title>
      <link>/glm_pymc3/</link>
      <pubDate>Mon, 04 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/glm_pymc3/</guid>
      <description>In this notebook I explore the glm module of PyMC3. I am particularly interested in the model definition using patsy formulas, as it makes the model evaluation loop faster (easier to include features and/or interactions). There are many good resources on this subject, but most of them evaluate the model in-sample. For many applications we require doing predictions on out-of-sample data. This experiment was motivated by the discussion of the thread “Out of sample” predictions with the GLM sub-module on the (great!</description>
    </item>
    
    <item>
      <title>Gaussian Processes for Time Series Forecasting with PyMC3</title>
      <link>/gp_ts_pymc3/</link>
      <pubDate>Sat, 02 Jan 2021 00:00:00 +0000</pubDate>
      
      <guid>/gp_ts_pymc3/</guid>
      <description>In this notebook we translate the forecasting models developed for the post on Gaussian Processes for Time Series Forecasting with Scikit-Learn to the probabilistic Bayesian framework PyMC3. I strongly recommend looking into the following references for more details and examples:
References:
 An Introduction to Gaussian Process Regression PyMC3 Docs: Gaussian Processes PyMC3 Docs Example: CO2 at Mauna Loa Bayesian Analysis with Python (Second edition) - Chapter 7 Statistical Rethinking - Chapter 14  Prepare Notebook1 import numpy as np import pandas as pd import matplotlib.</description>
    </item>
    
    <item>
      <title>Simple Bayesian Linear Regression with TensorFlow Probability</title>
      <link>/tfp_lm/</link>
      <pubDate>Tue, 06 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/tfp_lm/</guid>
      <description>In this post we show how to fit a simple linear regression model using TensorFlow Probability by replicating the first example on the getting started guide for PyMC3. We are going to use Auto-Batched Joint Distributions as they simplify the model specification considerably. Moreover, there is a great resource to get deeper into this type of distribution: Auto-Batched Joint Distributions: A Gentle Tutorial, which I strongly recommend (see this post to get a brief introduction on TensorFlow probability distributions).</description>
    </item>
    
    <item>
      <title>Open Data: Berlin Kitas</title>
      <link>/kitas_berlin/</link>
      <pubDate>Sat, 19 Sep 2020 00:00:00 +0000</pubDate>
      
      <guid>/kitas_berlin/</guid>
      <description>In this notebook I want to explore some data I found on the Berlin Open Data portal daten.berlin.de. The data source contains information of Kitas (Kindertagesstätte, i.e. kindergartens) in Berlin. This is a big topic as finding a spot in a Kita in Berlin is extremely difficult. We first provide an initial exploratory data analysis of the data set, then we merge it with population data to create some geo-location maps.</description>
    </item>
    
    <item>
      <title>A Simple Hamiltonian Monte Carlo Example with TensorFlow Probability</title>
      <link>/tfp_hcm/</link>
      <pubDate>Fri, 24 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/tfp_hcm/</guid>
      <description>In this post we want to revisit a simple bayesian inference example worked out in this blog post. This time we want to use TensorFlow Probability (TFP) instead of PyMC3.
References:
 Statistical Rethinking is an amazing reference for Bayesian analysis. It also has a sequence of online lectures freely available on YouTube.
 An introduction to probabilistic programming, now available in TensorFlow Probability
 There are many examples on the TensorFlow’s GitHub repository.</description>
    </item>
    
    <item>
      <title>Regression Analysis &amp; Visualization</title>
      <link>/lm_viz/</link>
      <pubDate>Fri, 26 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/lm_viz/</guid>
      <description>In this notebook I want to collect some useful visualizations which can help model development and model evaluation in the context of regression analysis. I use many visualization resources not just only to share results but as a key component of my workflow: data QA, EDA, feature engineering, model development, model evaluation and communicating results. In this notebook I focus on a simple regression model (time series) with statsmodels and visualization with matplotlib and seaborn.</description>
    </item>
    
    <item>
      <title>A Glimpse into TensorFlow Probability Distributions</title>
      <link>/intro_tfd/</link>
      <pubDate>Tue, 16 Jun 2020 00:00:00 +0000</pubDate>
      
      <guid>/intro_tfd/</guid>
      <description>In this notebook we want to go take a look into the distributions module of TensorFlow probability. The aim is to understand the fundamentals and then explore further this probabilistic programming framework. Here you can find an overview of TensorFlow Probability. We will concentrate on the first part of Layer 1: Statistical Building Blocks. As you could see from the distributions module documentation, there are many classes of distributions. We will explore a small sample of them in order to get an overall overview.</description>
    </item>
    
    <item>
      <title>Disease Spread Simulation (Animation)</title>
      <link>/infection_sim/</link>
      <pubDate>Tue, 28 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/infection_sim/</guid>
      <description>We describe how to generate a basic disease spread simulation. We explore how to do animations in Matplotlib.</description>
    </item>
    
    <item>
      <title>Getting Started with Spectral Clustering</title>
      <link>/spectral_clustering/</link>
      <pubDate>Sat, 04 Apr 2020 00:00:00 +0000</pubDate>
      
      <guid>/spectral_clustering/</guid>
      <description>In this post I want to explore the ideas behind spectral clustering. I do not intend to develop the theory. Instead, I will unravel a practical example to illustrate and motivate the intuition behind each step of the spectral clustering algorithm. I particularly recommend two references:
 For an introduction/overview on the theory, see the lecture notes A Tutorial on Spectral Clustering by Prof. Dr. Ulrike von Luxburg. For a concrete application of this clustering method you can see the PyData’s talk: Extracting relevant Metrics with Spectral Clustering by Dr.</description>
    </item>
    
    <item>
      <title>The Volume of the d-Ball via Monte Carlo Simulation</title>
      <link>/vol_d_ball/</link>
      <pubDate>Mon, 24 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/vol_d_ball/</guid>
      <description>In this notebook we run Monte Carlo simulations to estimate the volume of the \(d\)-ball \[ B^{d}:=\{x \in \mathbb{R}^d : ||x|| \leq 1\}. \] There are many ways to obtain a closed formula for this volume , see for example this Wikipedia article. Here we do it via sampling just for fun!
Main Idea Consider a square \(A_{d}\subset \mathbb{R}\) centered at the origin with side length \(2\). We estimate the volume of the \(d\)-ball \(B^{d}:=\{x \in \mathbb{R}^d : ||x|| \leq 1\}\subset A^{d}\) by sampling uniformly from \(A\) and computing the proportions of vectors having length less or equal than one.</description>
    </item>
    
    <item>
      <title>Forecasting Weekly Data with Prophet</title>
      <link>/fb_prophet/</link>
      <pubDate>Fri, 21 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/fb_prophet/</guid>
      <description>In this notebook we are present an initial exploration of the Prophet package by Facebook. From the documentation:
Prophet is a procedure for forecasting time series data based on an additive model where non-linear trends are fit with yearly, weekly, and daily seasonality, plus holiday effects. It works best with time series that have strong seasonal effects and several seasons of historical data. Prophet is robust to missing data and shifts in the trend, and typically handles outliers well.</description>
    </item>
    
    <item>
      <title>Exploring TensorFlow Probability STS Forecasting</title>
      <link>/intro_sts_tfp/</link>
      <pubDate>Tue, 11 Feb 2020 00:00:00 +0000</pubDate>
      
      <guid>/intro_sts_tfp/</guid>
      <description>In this notebook we explore the Structural Time Series (STS) Module of TensorFlow Probability. We follow closely the use cases presented in their Medium blog. As described there: An STS model expresses an observed time series as the sum of simpler components 1:
\[ f(t) = \sum_{k=1}^{N}f_{k}(t) + \varepsilon, \quad \text{where}\quad \varepsilon \sim N(0, \sigma^2). \]
Each summand \(f_{k}(t)\) has a particular structure, e.g. specific seasonality, trend, autoregressive terms, etc.</description>
    </item>
    
    <item>
      <title>Intro ML in Production: Flask, Docker and GitHub Actions</title>
      <link>/ml_prod_intro/</link>
      <pubDate>Tue, 28 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/ml_prod_intro/</guid>
      <description>We describe how to set up a toy-model repository to train and dockerize a machine learning model with data store on aws s3.</description>
    </item>
    
    <item>
      <title>Drawing Manifolds in LaTeX with TikZ</title>
      <link>/manifold_fig_latex/</link>
      <pubDate>Fri, 10 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/manifold_fig_latex/</guid>
      <description>We give some LaTex code to create figures of manifolds with boundaries.</description>
    </item>
    
    <item>
      <title>Open Data: Germany Maps Viz</title>
      <link>/germany_plots/</link>
      <pubDate>Tue, 07 Jan 2020 00:00:00 +0000</pubDate>
      
      <guid>/germany_plots/</guid>
      <description>In this post I want to show how to use public available (open) data to create geo visualizations in python. Maps are a great way to communicate and compare information when working with geolocation data. There are many frameworks to plot maps, here I focus on matplotlib and geopandas (and give a glimpse of mplleaflet).
Reference: A very good introduction to matplotlib is the chapter on Visualization with Matplotlib from the Python Data Science Handbook by Jake VanderPlas.</description>
    </item>
    
    <item>
      <title>The Graph Laplacian &amp; Semi-Supervised Clustering</title>
      <link>/semi_supervised_clustering/</link>
      <pubDate>Thu, 05 Dec 2019 00:00:00 +0000</pubDate>
      
      <guid>/semi_supervised_clustering/</guid>
      <description>In this post we want to explore the semi-supervided algorithm presented Eldad Haber in the BMS Summer School 2019: Mathematics of Deep Learning, during 19 - 30 August 2019, at the Zuse Institute Berlin. He developed an implementation in Matlab which you can find in this GitHub repository. In addition, please find the corresponding slides here.
Prepare Notebook import numpy as np import pandas as pd import matplotlib.pyplot as plt import seaborn as sns; sns.</description>
    </item>
    
    <item>
      <title>The Lapacian on the 2-Torus</title>
      <link>/laplacian_2torus/</link>
      <pubDate>Sun, 13 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/laplacian_2torus/</guid>
      <description>In this blog post I want to describe the explicit computation of the Laplacian on differential forms on the \(2\)-Torus \(T^2\subset \mathbb{R}^3\). This surface can be obtained by rotating the circle \((x-a)^2+y^2=r^2\) around the \(z\)-axis (\(0&amp;lt;r&amp;lt;a\)). Locally, this surface can be parametrized by the equations \[ x = (a+r\cos u)\cos v,\\ y = (a+r\cos u)\sin v,\\ z = r\sin u, \]
where \(0&amp;lt;u,v&amp;lt;2\pi\).</description>
    </item>
    
    <item>
      <title>PyData Berlin 2019: Gaussian Processes for Time Series Forecasting (scikit-learn)</title>
      <link>/gaussian_process_time_series/</link>
      <pubDate>Thu, 10 Oct 2019 00:00:00 +0000</pubDate>
      
      <guid>/gaussian_process_time_series/</guid>
      <description>In this notebook we run some experiments to demonstrate how we can use Gaussian Processes in the context of time series forecasting with scikit-learn. This material is part of a talk on Gaussian Process for Time Series Analysis presented at the PyCon DE &amp;amp; PyData 2019 Conference in Berlin.
Update: Additional material and plots were included for the Second Symposium on Machine Learning and Dynamical Systems at The Fields Institute (virtual event).</description>
    </item>
    
    <item>
      <title>satRday Berlin 2019: Remedies for Severe Class Imbalance</title>
      <link>/class_imbalance/</link>
      <pubDate>Sat, 15 Jun 2019 00:00:00 +0000</pubDate>
      
      <guid>/class_imbalance/</guid>
      <description>In this post I present a concrete case study illustrating some techniques to improve model performance in class-imbalanced classification problems. The methodologies described here are based on Chapter 16: Remedies for Severe Class Imbalance of the (great!) book Applied Predictive Modeling by Max Kuhn and Kjell Johnson. I absolutely recommend this reference to anyone interested in predictive modeling.
This notebook should serve as an extension of my talk given at satRday Berlin 2019: A conference for R users in Berlin.</description>
    </item>
    
    <item>
      <title>Seasonal Bump Functions</title>
      <link>/bump_func/</link>
      <pubDate>Thu, 11 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/bump_func/</guid>
      <description>Motivated by the nice talk on Winning with Simple, even Linear, Models by Vincent D. Warmerdam, I briefly describe how to construct certain class of bump functions to encode seasonal variables in R.
Prepare Notebook library(glue) library(lubridate) library(magrittr) library(tidyverse)  Generate Data Let us generate a time sequence variable stored in a tibble.
# Define time sequence. t &amp;lt;- seq.Date(from = as.Date(&amp;quot;2017-07-01&amp;quot;), to = as.Date(&amp;quot;2019-04-01&amp;quot;), by = &amp;quot;day&amp;quot;) # Store it in a tibble.</description>
    </item>
    
    <item>
      <title>An Introduction to Gaussian Process Regression</title>
      <link>/gaussian_process_reg/</link>
      <pubDate>Mon, 08 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/gaussian_process_reg/</guid>
      <description>Updated Version: 2019/09/21 (Extension + Minor Corrections)
After a sequence of preliminary posts (Sampling from a Multivariate Normal Distribution and Regularized Bayesian Regression as a Gaussian Process), I want to explore a concrete example of a gaussian process regression. We continue following Gaussian Processes for Machine Learning, Ch 2.
Other recommended references are:
 Gaussian Processes for Timeseries Modeling by S. Roberts, M. Osborne, M. Ebden, S. Reece, N. Gibson &amp;amp; S.</description>
    </item>
    
    <item>
      <title>Bayesian Regression as a Gaussian Process</title>
      <link>/reg_bayesian_regression/</link>
      <pubDate>Mon, 01 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/reg_bayesian_regression/</guid>
      <description>In this post we study the Bayesian Regression model to explore and compare the weight and function space and views of Gaussian Process Regression as described in the book Gaussian Processes for Machine Learning, Ch 2. We follow this reference very closely (and encourage to read it!). Our main objective is to illustrate the concepts and results through a concrete example. We use PyMC3 to run bayesian sampling.
References:</description>
    </item>
    
    <item>
      <title>Sampling from a Multivariate Normal Distribution</title>
      <link>/multivariate_normal/</link>
      <pubDate>Sat, 23 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/multivariate_normal/</guid>
      <description>In this post I want to describe how to sample from a multivariate normal distribution following section A.2 Gaussian Identities of the book Gaussian Processes for Machine Learning. This is a first step towards exploring and understanding Gaussian Processes methods in machine learning.
Multivariate Normal Distribution Recall that a random vector \(X = (X_1, , X_d)\) has a multivariate normal (or Gaussian) distribution if every linear combination
\[ \sum_{i=1}^{d} a_iX_i, \quad a_i\in\mathbb{R} \] is normally distributed.</description>
    </item>
    
    <item>
      <title>Dockerize a ShinyApp</title>
      <link>/dockerize-a-shinyapp/</link>
      <pubDate>Sat, 02 Mar 2019 00:00:00 +0000</pubDate>
      
      <guid>/dockerize-a-shinyapp/</guid>
      <description>In this post I want to describe how to dockerize a simple Shiny App. Docker is a great way of sharing and deploying projects. You can download it here.
Resources:
 R Docker tutorial, recommended for Docker beginners. Running a shiny app in a docker container by Mark Sellors (which is an updated and more complete version of this post).  Assume you have a project folder structure as follows:</description>
    </item>
    
    <item>
      <title>The Spectral Theorem for Matrices</title>
      <link>/the-spectral-theorem-for-matrices/</link>
      <pubDate>Sat, 02 Feb 2019 00:00:00 +0000</pubDate>
      
      <guid>/the-spectral-theorem-for-matrices/</guid>
      <description>When working in data analysis it is almost impossible to avoid using linear algebra, even if it is on the background, e.g. simple linear regression. In this post I want to discuss one of the most important theorems of finite dimensional vector spaces: the spectral theorem. The objective is not to give a complete and rigorous treatment of the subject, but rather show the main ingredientes, some examples and applications.</description>
    </item>
    
    <item>
      <title>Movie Plots Text Generation with Keras</title>
      <link>/movie_plot_text_gen/</link>
      <pubDate>Sun, 13 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/movie_plot_text_gen/</guid>
      <description>In this post I show some text generation experiments I ran using LSTM with Keras. For the preprocessing and tokenization I used SpaCy. The aim is not to present a completed project, but rather a first step which should be then iterated.
Resources There are many great resources and blog posts about the subject (and similar experiments). Here I mention the ones I found particularly useful for the general theory:</description>
    </item>
    
    <item>
      <title>Exploring the Curse of Dimensionality - Part II.</title>
      <link>/exploring-the-curse-of-dimensionality-part-ii./</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      
      <guid>/exploring-the-curse-of-dimensionality-part-ii./</guid>
      <description>I continue exploring the curse of dimensionality. Following the analysis form Part I., I want to discuss another consequence of sparse sampling in high dimensions: sample points are close to an edge of the sample. This post is based on The Elements of Statistical Learning, Section 2.5, which I encourage to read!
Uniform Sampling Consider \(N\) data points uniformly distributed in a \(p\)-dimensional unit ball centered at the origin. Suppose we consider a nearest-neighbor estimate at the origin.</description>
    </item>
    
    <item>
      <title>Text Mining, Networks and Visualization: Plebiscito Tweets</title>
      <link>/text-mining-networks-and-visualization-plebiscito-tweets/</link>
      <pubDate>Thu, 20 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/text-mining-networks-and-visualization-plebiscito-tweets/</guid>
      <description>Nowadays social media generates a vast amount of raw data (text, images, videos, etc). It is a very interesting challenge to discover techniques to get insights on the content and development of social media data. In addition, as a fundamental component of the analysis, it is important to find ways of communicating the results, i.e. data visualization. In this post I want to present a small case study where I analyze Twitter text data.</description>
    </item>
    
    <item>
      <title>Exploring the Curse of Dimensionality - Part I.</title>
      <link>/exploring-the-curse-of-dimensionality-part-i./</link>
      <pubDate>Sun, 09 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/exploring-the-curse-of-dimensionality-part-i./</guid>
      <description>In this post I want to present the notion of curse of dimensionality following a suggested exercise (Chapter 4 - Ex. 4) of the book An Introduction to Statistical Learning, written by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani.
When the number of features \(p\) is large, there tends to be a deterioration in the performance of KNN and other local approaches that perform prediction using only observations that are near the test observation for which a prediction must be made.</description>
    </item>
    
    <item>
      <title>From Pelican to Blogdown</title>
      <link>/pelican_to_blogdown/</link>
      <pubDate>Sun, 02 Dec 2018 00:00:00 +0000</pubDate>
      
      <guid>/pelican_to_blogdown/</guid>
      <description>Here I want to discuss my transition from Pelican to Blogdown and present some personal learnings. In June 2017 I decided to build a personal website/portafolio. I chose Pelican, because:
 It is written in Python, which was the programing language I was mainly working on.
 I wanted to include some Jupyter notebook I had already written.
 A great post: Building a data science portfolio: Making a data science blog explaining the procedure and using GitHub Pages to publist it.</description>
    </item>
    
    <item>
      <title>\(S^1\)-Equivariant Dirac operators on the Hopf Fibration</title>
      <link>/hopf_fibration/</link>
      <pubDate>Sun, 11 Nov 2018 00:00:00 +0000</pubDate>
      
      <guid>/hopf_fibration/</guid>
      <description>In this expository article I discuss the definition and basic properties of the Hopf fibration, with particular emphasis on Dirac-type operators induced, in the sense of Brüning and Heintze, by the Hodge-de Rham and spin-Dirac operators. In addition, we compute the Dirac-Schrödinger type operator introduced in my PhD thesis.</description>
    </item>
    
    <item>
      <title>Introduction to R Plumber : Expose a Caret model to a web API</title>
      <link>/intro_plumber/</link>
      <pubDate>Fri, 12 Oct 2018 00:00:00 +0000</pubDate>
      
      <guid>/intro_plumber/</guid>
      <description>In this post we present a simple example of how to expose a prediction model to a web API using the Plumber package.</description>
    </item>
    
    <item>
      <title>Circle Radius Fit for a Cloud of Points</title>
      <link>/circle-radius-fit-for-a-cloud-of-points/</link>
      <pubDate>Sun, 09 Sep 2018 00:00:00 +0000</pubDate>
      
      <guid>/circle-radius-fit-for-a-cloud-of-points/</guid>
      <description>We explore how to include an R notebook into a pelican post. As an example, we describe how to fit a circle onto a cloud of points.</description>
    </item>
    
    <item>
      <title>From Bachelor to PhD: Geometric and Topological Methods for Quantum Field Theory</title>
      <link>/vdl_experience/</link>
      <pubDate>Thu, 02 Aug 2018 00:00:00 +0000</pubDate>
      
      <guid>/vdl_experience/</guid>
      <description>We give an introduction to PyMC3, a probabilistic programming framework written in Python. We revise the basic mathematical theory and present two concrete examples.</description>
    </item>
    
    <item>
      <title>PyData Berlin 2018: On Laplacian Eigenmaps for Dimensionality Reduction</title>
      <link>/laplacian_eigenmaps_dim_red/</link>
      <pubDate>Sun, 08 Jul 2018 00:00:00 +0000</pubDate>
      
      <guid>/laplacian_eigenmaps_dim_red/</guid>
      <description>This post contains the slides and material from a talk I gave at PyData Berlin 2018. I presented the paper &lt;em&gt;Laplacian Eigenmaps for Dimensionality Reduction and Data Representation&lt;/em&gt; by &lt;a href=&#34;http://web.cse.ohio-state.edu/~belkin.8/&#34;&gt;Mikhail Belkin&lt;/a&gt; and &lt;a href=&#34;http://people.cs.uchicago.edu/~niyogi/&#34;&gt;Partha Niyogi&lt;/a&gt;.</description>
    </item>
    
    <item>
      <title>Probability that a given observation is part of a bootstrap sample?</title>
      <link>/bootstrap/</link>
      <pubDate>Wed, 29 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/bootstrap/</guid>
      <description>We study the problem of computing the probability that a given observation is part of a bootstrap sample. We include some numerical simulations.</description>
    </item>
    
    <item>
      <title>Induced Dirac-Schrödinger operators on semi-free circle quotients</title>
      <link>/phd/</link>
      <pubDate>Sat, 11 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/phd/</guid>
      <description>I present the content of my PhD Thesis in mathematics, which has now been published in The Journal of Geometric Analysis.</description>
    </item>
    
    <item>
      <title>Introduction to Bayesian Modeling with PyMC3</title>
      <link>/intro_pymc3/</link>
      <pubDate>Sun, 13 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/intro_pymc3/</guid>
      <description>We give an introduction to PyMC3, a probabilistic programming framework written in Python. We revise the basic mahematical theory and present two concrete examples.</description>
    </item>
    
    <item>
      <title>Web scraping with Beautiful Soup: Plebiscito Colombia (October 2nd)</title>
      <link>/plebiscito/</link>
      <pubDate>Sun, 09 Jul 2017 00:00:00 +0000</pubDate>
      
      <guid>/plebiscito/</guid>
      <description>We describe how to use Beautiful Soup to scrape the official goverment website in order to get the results of the peace referendum  in Colombia.</description>
    </item>
    
    <item>
      <title>The Dirac operator on the 2-sphere</title>
      <link>/the-dirac-operator-on-the-2-sphere/</link>
      <pubDate>Thu, 29 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/the-dirac-operator-on-the-2-sphere/</guid>
      <description>The objective of this post is to explore MathJax, a JavaScript display engine for LaTeX. Being my first post writen with this tool, I want to present a short but fun example: I will give a description of the explicit computation of the spin-Dirac operator (of the unique complex spinor bundle!) on the 2-sphere \(S^2\) equipped with the standar round metric. A more detailed treatment can be found in my expository paper.</description>
    </item>
    
    <item>
      <title>Python Exercise: Distance to Rectangle</title>
      <link>/rectangle/</link>
      <pubDate>Wed, 28 Jun 2017 00:00:00 +0000</pubDate>
      
      <guid>/rectangle/</guid>
      <description>In this first post we get started with a small python script to explore the basic capabilities of Pelican.</description>
    </item>
    
  </channel>
</rss>
