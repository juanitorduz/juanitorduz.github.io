<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.58.3" />


<title>Exploring the Curse of Dimensionality - Part I. - Dr. Juan Camilo Orduz</title>
<meta property="og:title" content="Exploring the Curse of Dimensionality - Part I. - Dr. Juan Camilo Orduz">




  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/github.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="../css/fonts.css" media="all">
<link rel="stylesheet" href="../css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="../" class="nav-logo">
    <img src="../images/sphere2.gif"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="../about/">About</a></li>
    
    <li><a href="https://github.com/juanitorduz">GitHub</a></li>
    
    <li><a href="https://www.linkedin.com/in/juanitorduz/">LinkedIn</a></li>
    
    <li><a href="https://twitter.com/juanitorduz">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">9 min read</span>
    

    <h1 class="article-title">Exploring the Curse of Dimensionality - Part I.</h1>

    
    <span class="article-date">2018-12-09</span>
    

    <div class="article-content">
      


<p>In this post I want to present the notion of <strong>curse of dimensionality</strong> following a suggested excercise (Chapter 4 - Ex. 4) of the book <a href="http://www-bcf.usc.edu/~gareth/ISL/">An Introduction to Statistical Learning</a>, writen by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani.</p>
<p><em>When the number of features <span class="math inline">\(p\)</span> is large, there tends to be a deterioration in the performance of KNN and other local approaches that perform prediction using only observations that are near the test observation for which a prediction must be made. This phenomenon is known as the <strong>curse of dimensionality</strong>, and it ties into the fact that non-parametric approaches often perform poorly when <span class="math inline">\(p\)</span> is large. We will now investigate this curse.</em></p>
<ol style="list-style-type: lower-alpha">
<li><em>Suppose that we have a set of observations, each with measurements on <span class="math inline">\(p = 1\)</span> feature, <span class="math inline">\(X\)</span>. We assume that <span class="math inline">\(X\)</span> is uniformly (evenly) distributed on <span class="math inline">\([0,1]\)</span>. Associated with each observation is a response value. Suppose that we wish to predict a test observation’s response using only observations that are within <span class="math inline">\(10\)</span>% of the range of <span class="math inline">\(X\)</span> closest to that test observation. For instance, in order to predict the response for a test observation with <span class="math inline">\(X = 0.6\)</span> we will use observations in the range <span class="math inline">\([0.55,0.65]\)</span>. On average, what fraction of the available observations will we use to make the prediction?</em></li>
</ol>
<p>Let us prepare the notebook.</p>
<pre class="r"><code>library(glue)
library(tidyverse)</code></pre>
<p>Let <span class="math inline">\(\lambda:= 0.1\)</span> represent the <em>locality</em> input parameter. i.e <span class="math inline">\(10\)</span>%.</p>
<pre class="r"><code>lambda &lt;- 0.1</code></pre>
<p>Let us generate <span class="math inline">\(X\)</span>, i.e. the sample observations.</p>
<pre class="r"><code># Set dimension.
p &lt;- 1

# Number of sample points.
n0 = 100

# Generate observation matrix
X &lt;- runif(n = n0, min = 0, max = 1)  %&gt;% matrix(ncol = p)</code></pre>
<p>Given <span class="math inline">\(z_0 \in [0,1]\)</span>, let us define a function which returns the <span class="math inline">\(\lambda\)</span>-interval. We need to consider separate cases, depending wether the <span class="math inline">\(z_0\)</span> has distance less than <span class="math inline">\(\lambda/2\)</span> to some of the boundary points.</p>
<pre class="r"><code>GenerateInterval &lt;- function (z0, lambda) {
  #&#39;Generate lambda-interval 
  #&#39;
  #&#39;@param z0 point in the unit interval. 
  #&#39;@param lambda locality parameter.
  #&#39;@return lambda-interval of z0.
  
    if (z0 &lt; lambda/2) {
    
    u.min &lt;- 0
    u.max &lt;- lambda
    
  } else if (z0 &lt; 1 - lambda/2) {
    
    u.min &lt;- z0 - lambda/2
    u.max &lt;- z0 + lambda/2
    
  } else {
    
    u.min &lt;- 1 - lambda 
    u.max &lt;- 1
    
  }
  
  interval &lt;- c(u.min, u.max)
  
  return(interval)
}</code></pre>
<p>Let us consider some examples:</p>
<pre class="r"><code>z0 &lt;- 0.6

GenerateInterval(z0 = z0, lambda = lambda)</code></pre>
<pre><code>## [1] 0.55 0.65</code></pre>
<pre class="r"><code>z0 &lt;- 0

GenerateInterval(z0 = z0, lambda = lambda)</code></pre>
<pre><code>## [1] 0.0 0.1</code></pre>
<p>Now let us write a function which verifies if a point <span class="math inline">\(x \in [0,1]\)</span> belongs to a given interval.</p>
<pre class="r"><code>IsInInterval &lt;- function (x, interval) {
  #&#39;Evaluate wether a given point belongs to the given interval. 
  #&#39;
  #&#39;@param x point in the unit interval. 
  #&#39;@param interval vector of length two indicating interval limits. 
  #&#39;@return boolean indicating wether x belongs to the given interval.
  
  (( x &gt; interval[1] ) &amp; ( x &lt; interval[2] ))
}</code></pre>
<p>For example,</p>
<pre class="r"><code>z0 &lt;- 0.6

interval &lt;- GenerateInterval(z0 = z0, lambda = lambda)</code></pre>
<pre class="r"><code>IsInInterval(x = 0.5, interval = interval)</code></pre>
<pre><code>## [1] FALSE</code></pre>
<pre class="r"><code>IsInInterval(x = 0.57, interval = interval)</code></pre>
<pre><code>## [1] TRUE</code></pre>
<p>Next, given any point and an observation matrix <span class="math inline">\(X\)</span> we want to count the number of points in the sample which belong to the <span class="math inline">\(\lambda\)</span>-interval of the point.</p>
<pre class="r"><code>CountAvailableObservations &lt;- function (z0, X, lambda) {
  #&#39; Count observations in the lambda-interval of a point.  
  #&#39;
  #&#39;@param z0 point in the unit interval. 
  #&#39;@param X observation matrix.
  #&#39;@return number of samples in the lambda-interval of z0.
  
  interval &lt;- GenerateInterval(z0 = z0, lambda = lambda)
  
  condition &lt;- IsInInterval(x = X, interval = interval)
  
  count.obs &lt;-  X[condition] %&gt;% length
  
  return(count.obs)
}</code></pre>
<p>Now we write a simulation function.</p>
<pre class="r"><code>RunSimulation1 &lt;- function(lambda, n0, n1, N) {
  #&#39; Run simulation. 
  #&#39;
  #&#39;@param lambda locality parameter.
  #&#39;@param z0 point in the unit interval. 
  #&#39;@param n0 number of samples in the observation matrix.
  #&#39;@param n1 number of samples in the prediction matrix.
  #&#39;@param N number of times to run the count.
  #&#39;@return vector of fraction counts. 
  
  # Run over simulation iterations. 
  1:N %&gt;% map_dbl(.f = function(k) {
    # Generate random observation samples. 
    X &lt;- runif(n = n0, min = 0, max = 1)
    # Generate prediction grid. 
    sample.grid &lt;- runif(n = n1, min = 0, max = 1)
    
    sample.grid %&gt;% 
      map_dbl(.f = ~ CountAvailableObservations(z0 = .x, X= X, lambda = lambda)) %&gt;% 
      mean
    
  }) / n1
}</code></pre>
<p>Let us plot the distribution.</p>
<pre class="r"><code>sim.1 &lt;- RunSimulation1(lambda = 0.1, 
                        n0 = 100, 
                        n1 = 100, 
                        N = 100)

mean.sim.1 &lt;- mean(sim.1)

sim.1 %&gt;% 
  as_tibble() %&gt;% 
  ggplot() +
  theme_light() +
  geom_histogram(mapping = aes(x = value), 
                 fill = &#39;blue&#39;, 
                 alpha = 0.7, 
                 bins = 20) +
  geom_vline(xintercept = mean.sim.1) +
  ggtitle(label = &#39;Fraction Simulation Distribution for p = 1&#39;)</code></pre>
<p><img src="../post/curse_dim_files/figure-html/unnamed-chunk-13-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>This result is not surprising. Since the samples are uniformly distributed on <span class="math inline">\([0,1]\)</span>, the fraction is going to be, on average, equal to the length of an interval of length <span class="math inline">\(\lambda\)</span>, which in this case is just <span class="math inline">\(0.1\)</span>.</p>
<pre class="r"><code>sim.1 %&gt;% 
  as_tibble() %&gt;% 
  ggplot() +
  theme_light() +
  geom_histogram(mapping = aes(x = value), 
                 fill = &#39;blue&#39;, 
                 alpha = 0.7, 
                 bins = 20) +
  geom_vline(xintercept = mean.sim.1) +
  geom_vline(xintercept = lambda, color = &#39;red&#39;) +
  ggtitle(label = &#39;Fraction Simulation Distribution for p = 1&#39;)</code></pre>
<p><img src="../post/curse_dim_files/figure-html/unnamed-chunk-14-1.png" width="672" style="display: block; margin: auto;" /></p>
<ol start="2" style="list-style-type: lower-alpha">
<li><em>Now suppose that we have a set of observations, each with measurements on <span class="math inline">\(p = 2\)</span> features, <span class="math inline">\(X_1\)</span> and <span class="math inline">\(X_2\)</span>. We assume that <span class="math inline">\((X_1,X_2)\)</span> are uniformly distributed on <span class="math inline">\([0,1]\times [0,1]\)</span>. We wish to predict a test observation’s response using only observations that are within <span class="math inline">\(10\)</span>% of the range of <span class="math inline">\(X_1\)</span> and within <span class="math inline">\(10\)</span>% of the range of <span class="math inline">\(X_2\)</span> closest to that test observation. For instance, in order to predict the response for a test observation with <span class="math inline">\(X_1 = 0.6\)</span> and <span class="math inline">\(X2 = 0.35\)</span>, we will use observations in the range <span class="math inline">\([0.55, 0.65]\)</span> for <span class="math inline">\(X_1\)</span> and in the range <span class="math inline">\([0.3, 0.4]\)</span> for <span class="math inline">\(X_2\)</span>. On average, what fraction of the available observations will we use to make the prediction?</em></li>
</ol>
<p>We argue similarly as above. We write a simulation function for all <span class="math inline">\(p&gt;0\)</span>.</p>
<pre class="r"><code>RunSimulationP &lt;- function(lambda, p, n0, n1, N) {
  #&#39; Run simulation. 
  #&#39;
  #&#39;@param lambda locality parameter.
  #&#39;@param z0 point in the unit interval. 
  #&#39;@param n0 number of samples in the observation matrix.
  #&#39;@param n1 number of samples in the prediction matrix.
  #&#39;@param N number of times to run the count.
  #&#39;@return vector of fraction counts. 
  
  # Run over simulation iterations. 
  1:N %&gt;% map_dbl(.f = function(k) {
    # Generate random observation samples. 
    X &lt;- runif(n = p*n0, min = 0, max = 1) %&gt;% matrix(ncol = p)
    # Generate prediction grid. 
    sample.grid &lt;- runif(n = p*n1, min = 0, max = 1) %&gt;% matrix(ncol = p)

    fraction.vect &lt;- sapply(X = 1:n0, FUN = function(i) {
      
      condition.matrix &lt;- sapply(X = 1:p, FUN = function(j) {
        # Select a point on the grid. 
        z0 &lt;- sample.grid[i, j]
        # Compute its interval.
        interval&lt;- GenerateInterval(z0 = z0, lambda = lambda)
        # Check which values in the grid belong to the lambda-interval. 
        IsInInterval(x = X[, j], interval = interval) 
      })
      # Define condition matrix for each dimension.
      condition.vect &lt;- condition.matrix %&gt;% 
                          as_tibble %&gt;% 
                          mutate_all(.funs = as.numeric) %&gt;% 
                          rowSums
      # Select points which belong to the lambda-hypercube 
      # and then compute the fraction.
      (condition.vect[condition.vect == p] %&gt;% length) / n0
    })
    
    return(mean(fraction.vect))
  })
}</code></pre>
<p>Let us run the simulation.</p>
<pre class="r"><code>p &lt;- 2

sim.2 &lt;- RunSimulationP(lambda = lambda, 
                        p = p, 
                        n0 = 100, 
                        n1 = 100,
                        N = 100)

mean.sim.2 &lt;- mean(sim.2)

mean.sim.2 </code></pre>
<pre><code>## [1] 0.009875</code></pre>
<p>Again, since the samples are uniformly distributed on <span class="math inline">\([0,1]\times [0,1]\)</span>, this fraction is going to be, on average, equal to the are of a square of length <span class="math inline">\(\lambda\)</span>, which in this case is just <span class="math inline">\(0.1^2 = 0.01\)</span>.</p>
<p>Let us plot the distribution.</p>
<pre class="r"><code>sim.2%&gt;% 
  as_tibble() %&gt;% 
  ggplot() +
  theme_light() +
  geom_histogram(mapping = aes(x = value), 
                 fill = &#39;blue&#39;, 
                 alpha = 0.7, 
                 bins = 20) +
  geom_vline(xintercept = mean.sim.2) +
  geom_vline(xintercept = lambda^p, color = &#39;red&#39;) +
  ggtitle(label = glue(&#39;Fraction Simulation Distribution for p = {p}&#39;))</code></pre>
<p><img src="../post/curse_dim_files/figure-html/unnamed-chunk-17-1.png" width="672" style="display: block; margin: auto;" /></p>
<ol start="3" style="list-style-type: lower-alpha">
<li><em>Now suppose that we have a set of observations on <span class="math inline">\(p = 100\)</span> features. Again the observations are uniformly distributed on each feature, and again each feature ranges in value from <span class="math inline">\(0\)</span> to <span class="math inline">\(1\)</span>. We wish to predict a test observation’s response using observations within the <span class="math inline">\(10\)</span>% of each feature’s range that is closest to that test observation. What fraction of the available observations will we use to make the prediction?</em></li>
</ol>
<p>We run the simulation first for <span class="math inline">\(p=5\)</span>.</p>
<pre class="r"><code>p &lt;- 5

sim.p &lt;- RunSimulationP(lambda = lambda, 
                        p = p, 
                        n0 = 100, 
                        n1 = 100, 
                        N = 100)

mean.sim.p &lt;- mean(sim.p)</code></pre>
<p>Let us plot the distribution.</p>
<pre class="r"><code>sim.p %&gt;% 
  as_tibble() %&gt;% 
  ggplot() +
  theme_light() +
  geom_histogram(mapping = aes(x = value), 
                 fill = &#39;blue&#39;, 
                 alpha = 0.7, 
                 bins = 20) +
  geom_vline(xintercept = mean.sim.p) +
  geom_vline(xintercept = lambda^p, color = &#39;red&#39;) +
  ggtitle(label = glue(&#39;Fraction Simulation Distribution for p = {p}&#39;))</code></pre>
<p><img src="../post/curse_dim_files/figure-html/unnamed-chunk-19-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>For <span class="math inline">\(p=100\)</span> we will just get zero (a very small number).</p>
<ol start="4" style="list-style-type: lower-alpha">
<li><em>Using your answers to parts (a) - (c), argue that a drawback of KNN when <span class="math inline">\(p\)</span> is large is that there are very few training observations “near” any given test observation.</em></li>
</ol>
<p>As we have seen from (a) - (c), on average, the fraction of available observations used to make the prediction equals the volume of a <span class="math inline">\(p\)</span>-dimensional hypercube of size <span class="math inline">\(\lambda\)</span>, which equals <span class="math inline">\(\lambda^p\)</span>. As this volume satisfies,</p>
<p><span class="math display">\[
\lim_{p\rightarrow \infty} \lambda^p = 0, 
\]</span> this means that when <span class="math inline">\(p\)</span> is large there are very few training observations “near” any given test observation. This fact makes the prediction not robust.</p>
<ol start="5" style="list-style-type: lower-alpha">
<li><em>Now suppose that we wish to make a prediction for a test observation by creating a <span class="math inline">\(p\)</span>-dimensional hypercube centered around the test observation that contains, on average, <span class="math inline">\(10\)</span>% of the training observations. For <span class="math inline">\(p = 1,2\)</span>, and <span class="math inline">\(100\)</span>, what is the length of each side of the hypercube?</em></li>
</ol>
<p>In this case we want to solve for <span class="math inline">\(\lambda\)</span> in the equation <span class="math inline">\(\lambda^p=0.1\)</span>, i.e. <span class="math inline">\(\lambda = 0.1^{1/p}\)</span>.</p>
<ul>
<li><span class="math inline">\(p=1\)</span></li>
</ul>
<pre class="r"><code>p &lt;- 1

0.1^{1/p}</code></pre>
<pre><code>## [1] 0.1</code></pre>
<ul>
<li><span class="math inline">\(p=2\)</span></li>
</ul>
<pre class="r"><code>p &lt;- 2

0.1^{1/p}</code></pre>
<pre><code>## [1] 0.3162278</code></pre>
<ul>
<li><span class="math inline">\(p=100\)</span></li>
</ul>
<pre class="r"><code>p &lt;- 100

0.1^{1/p}</code></pre>
<pre><code>## [1] 0.9772372</code></pre>
<p>Let us see the values of lambda as a function of <span class="math inline">\(p\)</span>.</p>
<pre class="r"><code>tibble(Dimension = 1:100) %&gt;% 
  mutate(Lambda = 0.1^(1/Dimension)) %&gt;% 
  ggplot() +
  theme_light() +
  geom_line(mapping = aes(x = Dimension, y = Lambda)) +
  geom_hline(yintercept = 1, color = &#39;red&#39;)</code></pre>
<p><img src="../post/curse_dim_files/figure-html/unnamed-chunk-23-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>That is, the side of the hypercube should grow almost to 1! This means that the “locality” is lost.</p>
<p><strong>Remark:</strong> It is interesting to see that, even if the volume of the <span class="math inline">\(\lambda\)</span>-hypercube tends to zero when <span class="math inline">\(p\)</span> is large, the length of its diagonal (defined as the distance between two oposite corners) grows to <span class="math inline">\(+\infty\)</span>. Indeed, it is easy to verify that the diagonal length is <span class="math inline">\(\lambda \sqrt{p}\)</span> (e.g. using induction).</p>
<pre class="r"><code>tibble(Dimension = 1:1000) %&gt;% 
  mutate(Diagonal = sqrt(Dimension)*lambda) %&gt;% 
  ggplot() +
  theme_light() +
  geom_line(mapping = aes(x = Dimension, y = Diagonal))</code></pre>
<p><img src="../post/curse_dim_files/figure-html/unnamed-chunk-24-1.png" width="672" style="display: block; margin: auto;" /></p>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="../index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="../images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="../js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-122570825-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

  </body>
</html>

