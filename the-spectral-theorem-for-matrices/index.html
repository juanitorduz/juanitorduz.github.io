<!DOCTYPE html>
<html lang="en-us">
  <head>
    <script defer src="https://use.fontawesome.com/releases/v6.5.1/js/all.js"></script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-5NM5EDH834"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-5NM5EDH834');
</script>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.79.1" />


<title>The Spectral Theorem for Matrices - Dr. Juan Camilo Orduz</title>
<meta property="og:title" content="The Spectral Theorem for Matrices - Dr. Juan Camilo Orduz">


  <link href='../images/favicon.ico' rel='icon' type='image/x-icon'/>



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="../css/fonts.css" media="all">
<link rel="stylesheet" href="../css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="../" class="nav-logo">
    <img src="../images/tattoo_logo.jpg"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="../about/"> About</a></li>
    
    <li><a href="https://github.com/juanitorduz"><i class='fab fa-github fa-2x'></i>  </a></li>
    
    <li><a href="https://www.linkedin.com/in/juanitorduz/"><i class='fab fa-linkedin fa-2x' style='color:#0a66c2;'></i>  </a></li>
    
    <li><a href="https://twitter.com/juanitorduz"><i class='fab fa-twitter fa-2x' style='color:#1DA1F2;'></i>  </a></li>
    
    <li><a href="https://bayes.club/@juanitorduz"><i class='fab fa-mastodon fa-2x' style='color:#6364FF;'></i>  </a></li>
    
    <li><a href="https://ko-fi.com/juanitorduz"><i class='fa-solid fa-mug-hot fa-2x' style='color:#FF5E5B;'></i>  </a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">13 min read</span>
    

    <h1 class="article-title">The Spectral Theorem for Matrices</h1>

    
    <span class="article-date">2019-02-02</span>
    

    <div class="article-content">
      


<p>When working in data analysis it is almost impossible to avoid using linear algebra, even if it is on the background, e.g. simple linear regression. In this post I want to discuss one of the most important theorems of finite dimensional vector spaces: <strong>the spectral theorem</strong>. The objective is not to give a complete and rigorous treatment of the subject, but rather show the main ingredients, some examples and applications.</p>
<ul>
<li>We are going to restrict to <em>real</em> vector spaces for simplicity. The <em>complex</em> case has an analogous treatment.</li>
<li>Because all <span class="math inline">\(n\)</span>-dimensional vector spaces are isomorphic, we will work on <span class="math inline">\(V = \mathbb{R}^n\)</span>.</li>
<li>We consider <span class="math inline">\(\mathbb{R}^n\)</span> as an <strong>inner product space</strong> with respect to the Euclidean metric <span class="math inline">\(\langle \cdot, \cdot \rangle\)</span>.</li>
</ul>
<div id="eigenvalues-and-eigenvectors" class="section level1">
<h1>Eigenvalues and Eigenvectors</h1>
<p>Let <span class="math inline">\(A\in M_n(\mathbb{R})\)</span> be an <span class="math inline">\(n\)</span>-dimensional matrix with real entries. A scalar <span class="math inline">\(\lambda\in\mathbb{C}\)</span> is an <strong>eigenvalue</strong> for <span class="math inline">\(A\)</span> if there exists a <em>non-zero</em> vector <span class="math inline">\(v\in \mathbb{R}^n\)</span> such that <span class="math inline">\(Av = \lambda v\)</span>. The vector <span class="math inline">\(v\)</span> is said to be an <strong>eigenvector</strong> of <span class="math inline">\(A\)</span> associated to <span class="math inline">\(\lambda\)</span>. We denote by <span class="math inline">\(E(\lambda)\)</span> the subspace generated by all the eigenvectors of associated to <span class="math inline">\(\lambda\)</span>. The set of eigenvalues of <span class="math inline">\(A\)</span>, denote by <span class="math inline">\(\text{spec(A)}\)</span>, is called the <strong>spectrum</strong> of <span class="math inline">\(A\)</span>.</p>
<p>We can rewrite the eigenvalue equation as <span class="math inline">\((A - \lambda I)v = 0\)</span>, where <span class="math inline">\(I\in M_n(\mathbb{R})\)</span> denotes the identity matrix. Hence, computing eigenvectors is equivalent to find elements in the kernel of <span class="math inline">\(A - \lambda I\)</span>. A sufficient (and necessary) condition for a non-trivial kernel is <span class="math inline">\(\det (A - \lambda I)=0\)</span>. Thus, in order to find eigenvalues we need to calculate roots of the <strong>characteristic polynomial</strong> <span class="math inline">\(\det (A - \lambda I)=0\)</span>.</p>
<p><strong>Remark:</strong> By the <a href="https://en.wikipedia.org/wiki/Fundamental_theorem_of_algebra">Fundamental Theorem of Algebra</a> eigenvalues always exist and could potentially be complex numbers.</p>
<p><strong>Remark:</strong> Note that <span class="math inline">\(A\)</span> is invertible if and only if <span class="math inline">\(0 \notin \text{spec}(A)\)</span>.</p>
<div id="example-1-part-i" class="section level3">
<h3>Example 1 (Part I)</h3>
<p>Consider the matrix</p>
<p><span class="math display">\[
A =
\left(
\begin{array}{cc}
1 &amp; 2\\
2 &amp; 1
\end{array}
\right)
\]</span></p>
<ul>
<li>Eigenvalues</li>
</ul>
<p>Let us compute and factorize the characteristic polynomial to find the eigenvalues:</p>
<p><span class="math display">\[
\det(A -\lambda I) = (1 - \lambda)^2 - 2^2 = (1 - \lambda + 2) (1 - \lambda - 2) = - (3 - \lambda)(1 + \lambda)
\]</span></p>
<p>Hence, we have two different eigenvalues <span class="math inline">\(\lambda_1 = 3\)</span> and <span class="math inline">\(\lambda_2 = -1\)</span>.</p>
<ul>
<li>Eigenvectors</li>
</ul>
<p>For <span class="math inline">\(\lambda_1 = 3\)</span> we have</p>
<p><span class="math display">\[
A-3I =
\left(
\begin{array}{cc}
-2 &amp; 2\\
2 &amp; - 2
\end{array}
\right)
\]</span></p>
<p>Hence,</p>
<p><span class="math display">\[
E(\lambda_1 = 3) =
\text{span}
\left\{
\left(
\begin{array}{c}
1\\
1
\end{array}
\right)
\right \}
\]</span></p>
<p>Similarly, for <span class="math inline">\(\lambda_2 = -1\)</span> we have</p>
<p><span class="math display">\[
A + I =
\left(
\begin{array}{cc}
2 &amp; 2\\
2 &amp; 2
\end{array}
\right)
\]</span></p>
<p>so we easily see that</p>
<p><span class="math display">\[
E(\lambda_2 = -1) =
\text{span}
\left\{
\left(
\begin{array}{c}
1\\
-1
\end{array}
\right)
\right \}
\]</span></p>
<p>We can find eigenvalues and eigenvector in R as follows:</p>
<pre class="r"><code># Define matrix A.
A &lt;- matrix(c(1,2,2,1), nrow = 2, byrow = TRUE)

A</code></pre>
<pre><code>##      [,1] [,2]
## [1,]    1    2
## [2,]    2    1</code></pre>
<pre class="r"><code>sp_decomp &lt;- eigen(x = A)</code></pre>
<ul>
<li>Eigenvalues</li>
</ul>
<pre class="r"><code>sp_decomp$values</code></pre>
<pre><code>## [1]  3 -1</code></pre>
<ul>
<li>Eigenvectors</li>
</ul>
<pre class="r"><code>sp_decomp$vectors</code></pre>
<pre><code>##           [,1]       [,2]
## [1,] 0.7071068 -0.7071068
## [2,] 0.7071068  0.7071068</code></pre>
<p>Let us verify the eigenvalue equations</p>
<pre class="r"><code>lambda_1 &lt;- sp_decomp$values[1]
lambda_2 &lt;- sp_decomp$values[2]

v_1 &lt;- sp_decomp$vectors[,1]
v_2 &lt;- sp_decomp$vectors[,2]

(A %*% v_1 == lambda_1*v_1) &amp; (A %*% v_2 == lambda_2*v_2)</code></pre>
<pre><code>##      [,1]
## [1,] TRUE
## [2,] TRUE</code></pre>
</div>
</div>
<div id="symmetric-matrices" class="section level1">
<h1>Symmetric Matrices</h1>
<p>We want to restrict now to a certain subspace of matrices, namely symmetric matrices. Recall that a matrix <span class="math inline">\(A\)</span> is <strong>symmetric</strong> if <span class="math inline">\(A^T = A\)</span>, i.e. it is equal to its transpose.</p>
<p>An important property of symmetric matrices is that is spectrum consists of <em>real</em> eigenvalues. To see this let <span class="math inline">\(A\in M_n(\mathbb{R}) \subset M_n(\mathbb{C})\)</span> be a symmetric matrix with eigenvalue <span class="math inline">\(\lambda\)</span> and corresponding eigenvector <span class="math inline">\(v\)</span>. Assume <span class="math inline">\(||v|| = 1\)</span>, then</p>
<p><span class="math display">\[
\lambda = \lambda \langle v, v \rangle = \langle \lambda v, v \rangle =   \langle Av, v \rangle = \langle v, A^T v \rangle =
\langle v, Av \rangle = \langle v, \lambda v \rangle = \bar{\lambda} \langle v, v \rangle = \bar{\lambda}
\]</span>
That is, <span class="math inline">\(\lambda\)</span> is equal to its complex conjugate. In particular, we see that the characteristic polynomial <strong>splits</strong> into a product of degree one polynomials with real coefficients. This property is very important.</p>
<p><strong>Theorem (Schur):</strong> Let <span class="math inline">\(A\in M_n(\mathbb{R})\)</span> be a matrix such that its characteristic polynomial splits (as above), then there exists an orthonormal basis of <span class="math inline">\(\mathbb{R}^n\)</span> such that <span class="math inline">\(A\)</span> is upper-triangular.</p>
<p><em>Proof:</em> One can use induction on the dimension <span class="math inline">\(n\)</span>. We omit the (non-trivial) details.</p>
<p><strong>Remark:</strong> When we say that <em>there exists an orthonormal basis of <span class="math inline">\(\mathbb{R}^n\)</span> such that <span class="math inline">\(A\)</span> is upper-triangular</em>, we see <span class="math inline">\(A:\mathbb{R}^n\longrightarrow \mathbb{R}^n\)</span> as a linear transformation.</p>
<p>The following theorem is a straightforward consequence of Schur’s theorem.</p>
<p><strong>Theorem</strong> A matrix <span class="math inline">\(A\)</span> is symmetric if and only if there exists an orthonormal basis for <span class="math inline">\(\mathbb{R}^n\)</span> consisting of eigenvectors of <span class="math inline">\(A\)</span>.</p>
<div id="example-2" class="section level3">
<h3>Example 2</h3>
<p>Let us see a concrete example where the statement of the theorem above does not hold. Consider the matrix</p>
<p><span class="math display">\[
B =
\left(
\begin{array}{cc}
1 &amp; 2\\
0 &amp; 1
\end{array}
\right)
\]</span></p>
<ul>
<li>Eigenvalues</li>
</ul>
<p><span class="math display">\[
\det(B -\lambda I) = (1 - \lambda)^2
\]</span>
Hence, the spectrum of <span class="math inline">\(B\)</span> consist of the single value <span class="math inline">\(\lambda = 1\)</span>.</p>
<ul>
<li>Eigenvectors</li>
</ul>
<p>The kernel of the matrix</p>
<p><span class="math display">\[
B - I =
\left(
\begin{array}{cc}
0 &amp; 2\\
0 &amp; 0
\end{array}
\right)
\]</span></p>
<p>is the subspace</p>
<p><span class="math display">\[
E(\lambda = 1) =
\text{span}
\left\{
\left(
\begin{array}{c}
1\\
0
\end{array}
\right)
\right \}
\]</span>
In particular, we see that the eigenspace of all the eigenvectors of <span class="math inline">\(B\)</span> has dimension one, so we can not find a basis of eigenvector for <span class="math inline">\(\mathbb{R}^2\)</span>.</p>
<p>Le us see how to compute this in <span class="math inline">\(R\)</span></p>
<pre class="r"><code>B &lt;- matrix(c(1,2,0,1), nrow = 2, byrow = TRUE)

B</code></pre>
<pre><code>##      [,1] [,2]
## [1,]    1    2
## [2,]    0    1</code></pre>
<pre class="r"><code>sp_decomp &lt;- eigen(x = B)</code></pre>
<pre class="r"><code>sp_decomp$values</code></pre>
<pre><code>## [1] 1 1</code></pre>
<pre class="r"><code>sp_decomp$vectors</code></pre>
<pre><code>##      [,1]          [,2]
## [1,]    1 -1.000000e+00
## [2,]    0  1.110223e-16</code></pre>
<p>Observe that these two columns are linerly dependent.</p>
<p>The following is another important result for symmetric matrices.</p>
<p><strong>Proposition:</strong> If <span class="math inline">\(\lambda_1\)</span> and <span class="math inline">\(\lambda_2\)</span> are two distinct eigenvalues of a symmetric matrix <span class="math inline">\(A\)</span> with corresponding eigenvectors <span class="math inline">\(v_1\)</span> and <span class="math inline">\(v_2\)</span> then <span class="math inline">\(v_1\)</span> and <span class="math inline">\(v_2\)</span> are orthogonal.</p>
<p><em>Proof:</em> We argue similarly as before</p>
<p><span class="math display">\[
\lambda_1\langle v_1, v_2 \rangle = \langle \lambda_1 v_1, v_2 \rangle = \langle A v_1, v_2 \rangle = \langle v_1, A v_2 \rangle
= \langle v_1, \lambda_2 v_2 \rangle = \bar{\lambda}_2 \langle v_1, v_2 \rangle = \lambda_2 \langle v_1, v_2 \rangle
\]</span>
which proofs that <span class="math inline">\(\langle v_1, v_2 \rangle\)</span> must be zero.</p>
</div>
</div>
<div id="orthogonal-projections" class="section level1">
<h1>Orthogonal Projections</h1>
<p>Let <span class="math inline">\(W \leq \mathbb{R}^n\)</span> be subspace. We define its <strong>orthogonal complement</strong> as
<span class="math display">\[
W^{\perp} := \{ v \in \mathbb{R} \:|\: \langle v, w \rangle =  0 \:\forall \: w \in W \}
\]</span></p>
<p>A matrix <span class="math inline">\(P\in M_n(\mathbb{R}^n)\)</span> is said to be an <strong>orthogonal projection</strong> if</p>
<ul>
<li><span class="math inline">\(P^2 = P\)</span></li>
<li><span class="math inline">\(\text{ran}(P)^\perp = \ker(P)\)</span></li>
</ul>
<p>Here</p>
<ul>
<li><span class="math inline">\(\ker(P)=\{v \in \mathbb{R}^2 \:|\: Pv = 0\}\)</span> denotes the kernel of <span class="math inline">\(P\)</span>.</li>
<li><span class="math inline">\(\text{ran}(P) = \{ Pv \: | \: v \in \mathbb{R}\}\)</span> is the range (or image) of <span class="math inline">\(P\)</span>.</li>
</ul>
<div id="example-3" class="section level3">
<h3>Example 3</h3>
<p>Let us consider a non-zero vector <span class="math inline">\(u\in\mathbb{R}\)</span>. We can use the inner product to construct the orthogonal projection onto the span of <span class="math inline">\(u\)</span> as follows:</p>
<p><span class="math display">\[
P_{u}:=\frac{1}{\|u\|^2}\langle u, \cdot \rangle u : \mathbb{R}^n \longrightarrow \{\alpha u\: | \: \alpha\in\mathbb{R}\}
\]</span>
Note that:</p>
<p><span class="math display">\[
P^2_u(v) = \frac{1}{\|u\|^4}\langle u, \langle u , v \rangle u  \rangle u = \frac{1}{\|u\|^2}\langle u, v \rangle u = P_u(v)
\]</span></p>
<p>The condition <span class="math inline">\(\text{ran}(P_u)^\perp = \ker(P_u)\)</span> is trivially satisfied. Hence, <span class="math inline">\(P_u\)</span> is an orthogonal projection.</p>
</div>
<div id="example-1-part-ii" class="section level3">
<h3>Example 1 (Part II)</h3>
<p>Let us compute the orthogonal projections onto the eigenspaces of the matrix</p>
<p><span class="math display">\[
A =
\left(
\begin{array}{cc}
1 &amp; 2\\
2 &amp; 1
\end{array}
\right)
\]</span></p>
<p>For <span class="math inline">\(\lambda_1 = 3\)</span>, recall that</p>
<p><span class="math display">\[
E(\lambda_1 = 3) =
\text{span}
\left\{
\left(
\begin{array}{c}
1\\
1
\end{array}
\right)
\right \}
\]</span></p>
<p>From Example 3 above we see that</p>
<p><span class="math display">\[
P(\lambda_1 = 3) =
\frac{1}{2}\left\langle
\left(
\begin{array}{c}
1 \\
1
\end{array}
\right)
, \cdot
\right\rangle
\left(
\begin{array}{c}
1 \\
1
\end{array}
\right)
: \mathbb{R}\longrightarrow E(\lambda_1 = 3)
\]</span></p>
<p>Which in matrix form (with respect to the canonical basis of <span class="math inline">\(\mathbb{R}^2\)</span>) is given by</p>
<p><span class="math display">\[
P(\lambda_1 = 3) =
\frac{1}{2}
\left(
\begin{array}{cc}
1 &amp; 1 \\
1 &amp; 1
\end{array}
\right)
\]</span></p>
<p>A similar argument shows that</p>
<p><span class="math display">\[
P(\lambda_2 = -1) =
\frac{1}{2}
\left(
\begin{array}{cc}
1 &amp; - 1 \\
-1 &amp; 1
\end{array}
\right)
\]</span></p>
<p>Observe that</p>
<p><span class="math display">\[
P(\lambda_1 = 3)P(\lambda_2 = -1) =
\frac{1}{4}
\left(
\begin{array}{cc}
1 &amp; 1 \\
1 &amp; 1
\end{array}
\right)
\left(
\begin{array}{cc}
1 &amp; - 1 \\
-1 &amp; 1
\end{array}
\right)
=
\left(
\begin{array}{cc}
0 &amp; 0 \\
0 &amp; 0
\end{array}
\right)
\]</span></p>
<p>as expected (see Proposition above).</p>
<p>Let us see how to compute the orthogonal projections in R.</p>
<pre class="r"><code>sp_decomp &lt;- eigen(x = A)</code></pre>
<pre class="r"><code># Construct projection function.
proj &lt;- function(v, x) {
  
  inner_product &lt;- as.numeric(v %*% x)

  inner_product*v  
}

# Apply to the canonical basis. 
proj_1 &lt;- cbind(matrix(data = proj(v = sp_decomp$vectors[, 1], x = c(1, 0)), ncol = 1), 
                matrix(data = proj(v = sp_decomp$vectors[, 1], x = c(0, 1)), ncol = 1))

proj_2 &lt;- cbind(matrix(data = proj(v = sp_decomp$vectors[, 2], x = c(1, 0)), ncol = 1), 
                matrix(data = proj(v = sp_decomp$vectors[, 2], x = c(0, 1)), ncol = 1))</code></pre>
<pre class="r"><code>proj_1</code></pre>
<pre><code>##      [,1] [,2]
## [1,]  0.5  0.5
## [2,]  0.5  0.5</code></pre>
<pre class="r"><code>proj_2</code></pre>
<pre><code>##      [,1] [,2]
## [1,]  0.5 -0.5
## [2,] -0.5  0.5</code></pre>
<p>Note for example</p>
<pre class="r"><code>proj_1 %*% proj_1</code></pre>
<pre><code>##      [,1] [,2]
## [1,]  0.5  0.5
## [2,]  0.5  0.5</code></pre>
<p>and</p>
<pre class="r"><code>proj_1 %*% proj_2</code></pre>
<pre><code>##      [,1] [,2]
## [1,]    0    0
## [2,]    0    0</code></pre>
</div>
</div>
<div id="the-spectral-theorem" class="section level1">
<h1>The Spectral Theorem</h1>
<p>Now we are ready to understand the statement of the <strong>spectral theorem</strong>.</p>
<p><strong>Theorem (Spectral Theorem for Matrices)</strong> Let <span class="math inline">\(A\in M_n(\mathbb{R})\)</span> be a symmetric matrix, with distinct eigenvalues <span class="math inline">\(\lambda_1, \lambda_2, \cdots, \lambda_k\)</span>. Let <span class="math inline">\(E(\lambda_i)\)</span> be the eigenspace of <span class="math inline">\(A\)</span> corresponding to the eigenvalue <span class="math inline">\(\lambda_i\)</span>, and let <span class="math inline">\(P(\lambda_i):\mathbb{R}^n\longrightarrow E(\lambda_i)\)</span> be the corresponding orthogonal projection of <span class="math inline">\(\mathbb{R}^n\)</span> onto <span class="math inline">\(E(\lambda_i)\)</span>. Then the following statements are true:</p>
<ol style="list-style-type: lower-alpha">
<li>There is a direct sum decomposition <span class="math inline">\(\mathbb{R}^n = \bigoplus_{i=1}^{k} E(\lambda_i)\)</span>.</li>
<li>If <span class="math inline">\(B(\lambda_i) := \bigoplus_{i\neq j}^{k} E(\lambda_i)\)</span>, then <span class="math inline">\(E(\lambda_i)^{\perp} = B(\lambda_i)\)</span>.</li>
<li>The orthogonal projections satisfy <span class="math inline">\(P(\lambda_i)P(\lambda_j)=\delta_{ij}P(\lambda_i)\)</span> for <span class="math inline">\(1 \leq i, j \leq k\)</span>. Here <span class="math inline">\(\delta_{ij}\)</span> denotes the <a href="https://en.wikipedia.org/wiki/Kronecker_delta">Kronecker delta</a>.</li>
<li>The identity matrix can be decomposed as <span class="math inline">\(I = \sum_{i=i}^{k} P(\lambda_i)\)</span>.</li>
<li>The matrix <span class="math inline">\(A\)</span> can be written as <span class="math inline">\(A = \sum_{i=i}^{k} \lambda_i P(\lambda_i)\)</span>.</li>
</ol>
<div id="comments" class="section level2">
<h2>Comments</h2>
<ol style="list-style-type: lower-alpha">
<li>We can read this first statement as follows: <em>There exists a basis of <span class="math inline">\(\mathbb{R}^n\)</span> consisting of eigenvectors of <span class="math inline">\(A\)</span></em>.</li>
<li>The basis above can chosen to be orthonormal using the <a href="https://en.wikipedia.org/wiki/Gram–Schmidt_process">Gram–Schmidt process</a> within each eigenspace.</li>
</ol>
<p>As a consequence of this theorem we see that there exist an orthogonal matrix <span class="math inline">\(Q\in SO(n)\)</span> (i.e <span class="math inline">\(QQ^T=Q^TQ=I\)</span> and <span class="math inline">\(\det(Q)=1\)</span>) such that</p>
<p><span class="math display">\[
A = QDQ^{-1}
\]</span></p>
<p>where <span class="math inline">\(D\)</span> is a diagonal matrix containing the eigenvalues in <span class="math inline">\(A\)</span> (with multiplicity). The matrix <span class="math inline">\(Q\)</span> is constructed by stacking the normalized orthogonal eigenvectors of <span class="math inline">\(A\)</span> as column vectors.</p>
<div id="example-1-part-iii" class="section level3">
<h3>Example 1 (Part III)</h3>
<p>Let us continue working with the matrix</p>
<p><span class="math display">\[
A =
\left(
\begin{array}{cc}
1 &amp; 2\\
2 &amp; 1
\end{array}
\right)
\]</span></p>
<p>We have already verified the first three statements of the spectral theorem in Part I and Part II. For d. let us simply compute
<span class="math inline">\(P(\lambda_1 = 3) + P(\lambda_2 = -1)\)</span>,</p>
<p><span class="math display">\[
\frac{1}{2}
\left(
\begin{array}{cc}
1 &amp; 1 \\
1 &amp; 1
\end{array}
\right)
+
\frac{1}{2}
\left(
\begin{array}{cc}
1 &amp; -1 \\
-1 &amp; 1
\end{array}
\right)
=
\left(
\begin{array}{cc}
1 &amp; 0 \\
0 &amp; 1
\end{array}
\right)
=
I
\]</span></p>
<p>Finally, for e. we calculate</p>
<p><span class="math display">\[
\frac{3}{2}
\left(
\begin{array}{cc}
1 &amp; 1 \\
1 &amp; 1
\end{array}
\right)
-
\frac{1}{2}
\left(
\begin{array}{cc}
1 &amp; -1 \\
-1 &amp; 1
\end{array}
\right)
=
\left(
\begin{array}{cc}
1 &amp; 2 \\
2 &amp; 1
\end{array}
\right)
= A
\]</span></p>
<p>In R we can easily get the same results:</p>
<pre class="r"><code>proj_1 + proj_2</code></pre>
<pre><code>##      [,1] [,2]
## [1,]    1    0
## [2,]    0    1</code></pre>
<pre class="r"><code>sp_decomp$values[1]*proj_1 + sp_decomp$values[2]*proj_2</code></pre>
<pre><code>##      [,1] [,2]
## [1,]    1    2
## [2,]    2    1</code></pre>
<p>The matrix <span class="math inline">\(Q\)</span> is simply given by</p>
<p><span class="math display">\[
Q =
\frac{1}{\sqrt{2}}
\left(
\begin{array}{cc}
1 &amp; -1 \\
1 &amp; 1
\end{array}
\right)
\]</span>
Obvserve that</p>
<p><span class="math display">\[
\frac{1}{\sqrt{2}}
\left(
\begin{array}{cc}
1 &amp; -1 \\
1 &amp; 1
\end{array}
\right)
\left(
\begin{array}{cc}
1 &amp; 2\\
2 &amp; 1
\end{array}
\right)
\frac{1}{\sqrt{2}}
\left(
\begin{array}{cc}
1 &amp; -1 \\
1 &amp; 1
\end{array}
\right)
=
\left(
\begin{array}{cc}
3 &amp; 0\\
0 &amp; -1
\end{array}
\right)
\]</span>
In R this is an immediate computation</p>
<pre class="r"><code>Q &lt;- sp_decomp$vectors

Q</code></pre>
<pre><code>##           [,1]       [,2]
## [1,] 0.7071068 -0.7071068
## [2,] 0.7071068  0.7071068</code></pre>
<pre class="r"><code>D &lt;- t(Q) %*% A %*% Q

D</code></pre>
<pre><code>##      [,1] [,2]
## [1,]    3    0
## [2,]    0   -1</code></pre>
<p>This completes the verification of the spectral theorem in this simple example.</p>
</div>
</div>
</div>
<div id="a-sketch-of-the-proof" class="section level1">
<h1>A Sketch of the Proof</h1>
<ol style="list-style-type: lower-alpha">
<li><p>This follow easily from the discussion on symmetric matrices above.</p></li>
<li><p>This follows by the Proposition above and the dimension theorem (to prove the two inclusions).</p></li>
<li><p>This also follows from the Proposition above.</p></li>
<li><p>This follows from a. and b.</p></li>
<li><p>For <span class="math inline">\(v\in\mathbb{R}^n\)</span>, let us decompose it as</p></li>
</ol>
<p><span class="math display">\[
v = \sum_{i=1}^{k} v_i
\]</span></p>
<p>where <span class="math inline">\(v_i\in E(\lambda_i)\)</span>. Then,</p>
<p><span class="math display">\[
Av = A\left(\sum_{i=1}^{k} v_i\right) = \sum_{i=1}^{k} A v_i = \sum_{i=1}^{k} \lambda_iv_i = \left( \sum_{i=1}^{k} \lambda_i P(\lambda_i)\right)v
\]</span></p>
</div>
<div id="some-applications" class="section level1">
<h1>Some Applications</h1>
<div id="functional-calculus" class="section level2">
<h2>Functional Calculus</h2>
<p>For manny applications (e.g. compute heat kernel of the graph Laplacian) one is intereted in computing the exponential of a symmetric matrix <span class="math inline">\(A\)</span> defined by the (convergent) series</p>
<p><span class="math display">\[
e^A:= \sum_{k=0}^{\infty}\frac{A^k}{k!}
\]</span></p>
<p>In practice, to compute the exponential we can use the relation A = <span class="math inline">\(Q D Q^{-1}\)</span>,</p>
<p><span class="math display">\[
e^A= \sum_{k=0}^{\infty}\frac{(Q D Q^{-1})^k}{k!} = Q\left(\sum_{k=0}^{\infty}\frac{D^k}{k!}\right)Q^{-1} = Qe^{D}Q^{-1}
\]</span></p>
<p>and since <span class="math inline">\(D\)</span> is diagonal then <span class="math inline">\(e^{D}\)</span> is just again a diagonal matrix with entries <span class="math inline">\(e^{\lambda_i}\)</span>.</p>
<div id="example-1-part-iv" class="section level3">
<h3>Example 1 (Part IV)</h3>
<p>We compute <span class="math inline">\(e^A\)</span>. First let us calculate <span class="math inline">\(e^D\)</span> using the <a href="https://cran.r-project.org/web/packages/expm/index.html"><code>expm</code></a> package.</p>
<pre class="r"><code>expm::expm(D)</code></pre>
<pre><code>##          [,1]      [,2]
## [1,] 20.08554 0.0000000
## [2,]  0.00000 0.3678794</code></pre>
<p>Note that</p>
<pre class="r"><code>exp(3)</code></pre>
<pre><code>## [1] 20.08554</code></pre>
<pre class="r"><code>exp(-1)</code></pre>
<pre><code>## [1] 0.3678794</code></pre>
<p>Now we compute the product</p>
<pre class="r"><code>Q %*% expm::expm(D) %*% t(Q)</code></pre>
<pre><code>##           [,1]      [,2]
## [1,] 10.226708  9.858829
## [2,]  9.858829 10.226708</code></pre>
<p>This coincides with the result obtained using <a href="https://cran.r-project.org/web/packages/expm/index.html"><code>expm</code></a>.</p>
<pre class="r"><code>expm::expm(A)</code></pre>
<pre><code>##           [,1]      [,2]
## [1,] 10.226708  9.858829
## [2,]  9.858829 10.226708</code></pre>
<p>In a similar manner, one can easily show that for any polynomial <span class="math inline">\(p(x)\)</span> one has</p>
<p><span class="math display">\[
p(A) = \sum_{i=1}^{k}p(\lambda_i)P(\lambda_i)
\]</span></p>
<p>Moreover, one can extend this relation to the space of continuous functions <span class="math inline">\(f:\text{spec}(A)\subset\mathbb{R}\longrightarrow \mathbb{C}\)</span>, this is known as the <a href="http://homepages.wmich.edu/~ledyaev/Fall2012/Spectral%20Mapping%20Theorem%20Talk.pdf">spectral mapping theorem</a>.</p>
<p><strong>Remark:</strong> The <a href="https://en.wikipedia.org/wiki/Cayley–Hamilton_theorem"><strong>Cayley–Hamilton theorem</strong></a> says that every square matrix (over a commutative ring) satisfies its own characteristic polynomial.</p>
</div>
<div id="example-1-part-v" class="section level3">
<h3>Example 1 (Part V)</h3>
<p>Let us verify this for the matrix <span class="math inline">\(A\)</span>.</p>
<pre class="r"><code>(diag(2) - A) %*% (diag(2) - A) - 4*diag(2)</code></pre>
<pre><code>##      [,1] [,2]
## [1,]    0    0
## [2,]    0    0</code></pre>
</div>
</div>
<div id="principal-component-analysis" class="section level2">
<h2>Principal Component Analysis</h2>
<p>Given an observation matrix <span class="math inline">\(X\in M_{n\times p}(\mathbb{R})\)</span>, the covariance matrix <span class="math inline">\(A:= X^T X \in M_p(\mathbb{R})\)</span> is clearly symmetric and therefore diagonalizable. In this context, <a href="https://en.wikipedia.org/wiki/Principal_component_analysis">principal component analysis</a> just translates to reducing the dimensionality by projecting onto a subspace generated by a subset of eigenvectors of <span class="math inline">\(A\)</span>.</p>
</div>
<div id="spectrum-of-the-graph-laplacian" class="section level2">
<h2>Spectrum of the Graph Laplacian</h2>
<p>In various applications, like the spectral embedding non-linear dimensionality algorithm or spectral clustering, the spectral decomposition of the grah Laplacian is of much interest (see for example <a href="https://juanitorduz.github.io/laplacian_eigenmaps_dim_red/">PyData Berlin 2018: On Laplacian Eigenmaps for Dimensionality Reduction</a>).</p>
</div>
<div id="many-more" class="section level2">
<h2>Many More …</h2>
</div>
</div>
<div id="spectral-theorem-for-bounded-and-unbounded-operators" class="section level1">
<h1>Spectral Theorem for Bounded and Unbounded Operators</h1>
<p>This is just the begining! There is a beautifull rich theory on the spectral analysis of bounded and unbounded self-adjoint operators on Hilbert spaces with many applications (e.g. Quantum Mechanics, Fourier Decomposition, Signal Processing, …).</p>
</div>
<div id="references" class="section level1">
<h1>References</h1>
<ul>
<li><p><a href="https://www.amazon.com/Linear-Algebra-4th-Stephen-Friedberg/dp/0130084514">Linear Algebra, Friedberg, Insel and Spence</a></p></li>
<li><p><a href="https://www.springer.com/de/book/9783540586616">Perturbation Theory for Linear Operators, Kato</a></p></li>
</ul>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="../index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="../images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/yaml.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/dockerfile.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/bash.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="../js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'G-5NM5EDH834', 'auto');
	
	ga('send', 'pageview');
}
</script>

  </body>
</html>

