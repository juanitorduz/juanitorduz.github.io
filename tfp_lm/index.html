<!DOCTYPE html>
<html lang="en-us">
  <head>
    <script defer src="https://use.fontawesome.com/releases/v6.5.1/js/all.js"></script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-5NM5EDH834"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-5NM5EDH834');
</script>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.79.1" />


<title>Simple Bayesian Linear Regression with TensorFlow Probability - Dr. Juan Camilo Orduz</title>
<meta property="og:title" content="Simple Bayesian Linear Regression with TensorFlow Probability - Dr. Juan Camilo Orduz">


  <link href='../images/favicon.ico' rel='icon' type='image/x-icon'/>



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="../css/fonts.css" media="all">
<link rel="stylesheet" href="../css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="../" class="nav-logo">
    <img src="../images/tattoo_logo.jpg"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="../about/"> About</a></li>
    
    <li><a href="https://github.com/juanitorduz"><i class='fab fa-github fa-2x'></i>  </a></li>
    
    <li><a href="https://www.linkedin.com/in/juanitorduz/"><i class='fab fa-linkedin fa-2x' style='color:#0a66c2;'></i>  </a></li>
    
    <li><a href="https://twitter.com/juanitorduz"><i class='fab fa-twitter fa-2x' style='color:#1DA1F2;'></i>  </a></li>
    
    <li><a href="https://bayes.club/@juanitorduz"><i class='fab fa-mastodon fa-2x' style='color:#6364FF;'></i>  </a></li>
    
    <li><a href="https://ko-fi.com/juanitorduz"><i class='fa-solid fa-mug-hot fa-2x' style='color:#FF5E5B;'></i>  </a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">7 min read</span>
    

    <h1 class="article-title">Simple Bayesian Linear Regression with TensorFlow Probability</h1>

    
    <span class="article-date">2020-10-06</span>
    

    <div class="article-content">
      


<p>In this post we show how to fit a simple linear regression model using <a href="https://www.tensorflow.org/probability">TensorFlow Probability</a> by replicating the first example on the <a href="https://docs.pymc.io/notebooks/getting_started.html">getting started guide for PyMC3</a>. We are going to use <a href="https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/JointDistributionCoroutineAutoBatched">Auto-Batched Joint Distributions</a> as they simplify the model specification considerably. Moreover, there is a great resource to get deeper into this type of distribution: <a href="https://www.tensorflow.org/probability/examples/JointDistributionAutoBatched_A_Gentle_Tutorial">Auto-Batched Joint Distributions: A Gentle Tutorial</a>, which I <strong>strongly recommend</strong> (see <a href="https://juanitorduz.github.io/intro_tfd/">this post</a> to get a brief introduction on TensorFlow probability distributions). In addition the tutorial: <a href="https://www.tensorflow.org/probability/examples/Modeling_with_JointDistribution">Bayesian Modeling with Joint Distribution</a> is also a great reference to get started with linear models in TensorFlow Probability.</p>
<div id="prepare-notebook" class="section level2">
<h2>Prepare Notebook</h2>
<pre class="python"><code>import numpy as np
import pandas as pd
import tensorflow.compat.v2 as tf
tf.enable_v2_behavior()
import tensorflow_probability as tfp
tfd = tfp.distributions

# Data Viz. 
import matplotlib.pyplot as plt
from matplotlib.ticker import FormatStrFormatter
import seaborn as sns
sns.set_style(
    style=&#39;darkgrid&#39;, 
    rc={&#39;axes.facecolor&#39;: &#39;.9&#39;, &#39;grid.color&#39;: &#39;.8&#39;}
)
sns.set_palette(palette=&#39;deep&#39;)
sns_c = sns.color_palette(palette=&#39;deep&#39;)
%matplotlib inline
from pandas.plotting import register_matplotlib_converters
register_matplotlib_converters()

plt.rcParams[&#39;figure.figsize&#39;] = [12, 6]
plt.rcParams[&#39;figure.dpi&#39;] = 100

# Get TensorFlow version.
print(f&#39;TnesorFlow version: {tf.__version__}&#39;)
print(f&#39;TnesorFlow Probability version: {tfp.__version__}&#39;)</code></pre>
<pre><code>TnesorFlow version: 2.3.1
TnesorFlow Probability version: 0.11.1</code></pre>
</div>
<div id="generate-data" class="section level2">
<h2>Generate Data</h2>
<p>We generate the data as in the PyMC3 example:</p>
<p><span class="math display">\[y = \alpha + \beta_0 x_0 + \beta_1 x_1 + \varepsilon \quad \text{where}\quad \varepsilon \sim N(0, \sigma^2)\]</span></p>
<pre class="python"><code>np.random.seed(42)
# True parameter values
alpha, sigma = 1, 1
beta = [1, 2.5]
# Size of dataset
size = 100
# Predictor variable
x0 = np.random.randn(size)
x1 = np.random.randn(size) * 0.2
# Simulate outcome variable
y = alpha + beta[0] * x0 + beta[1] * x1 + np.random.randn(size) * sigma</code></pre>
<p>Let us plot the distribution of the target variable.</p>
<pre class="python"><code>fig, ax = plt.subplots(figsize=(8, 7))
sns.histplot(x=y, kde=True, ax=ax)
ax.set(title=&#39;$y$ distribution&#39;, xlabel=&#39;y&#39;);</code></pre>
<center>
<img src="../images/tfp_lm_files/tfp_lm_6_0.svg" title="fig:" alt="svg" />
</center>
<pre class="python"><code>fig, ax = plt.subplots(nrows=1, ncols=2)
sns.scatterplot(x=x0, y=y, color=sns_c[0], ax=ax[0])
sns.scatterplot(x=x1, y=y, color=sns_c[1], ax=ax[1])
ax[0].set(title=&#39;$x_0$ vs $y$&#39;, xlabel=&#39;$x_0$&#39;, ylabel=&#39;y&#39;)
ax[1].set(title=&#39;$x_1$ vs $y$&#39;, xlabel=&#39;$x_1$&#39;, ylabel=&#39;y&#39;);</code></pre>
<center>
<img src="../images/tfp_lm_files/tfp_lm_7_0.svg" title="fig:" alt="svg" />
</center>
</div>
<div id="define-model" class="section level2">
<h2>Define Model</h2>
<p>Our model is</p>
<p><span class="math display">\[
y \sim N(\mu, \sigma^2) \\
\mu = \alpha + \beta_0 x_0 + \beta_1 x_1
\]</span>
with the priors
<span class="math display">\[
\alpha \sim N(0, 100) \\
\beta_i \sim N(0, 100) \\
\sigma \sim |N(0, 1)|
\]</span></p>
<p>In order to define the model in TensorFlow Probability let us first convert our input into <a href="https://www.tensorflow.org/api_docs/python/tf/Tensor">tf tensors</a>.</p>
<pre class="python"><code># Set seed.
tf.random.set_seed(42)
# Set tensor numeric type.
dtype = &#39;float32&#39;

x = np.stack([x0, x1], axis=1)
x = tf.convert_to_tensor(x, dtype=dtype)

y = tf.convert_to_tensor(y, dtype=dtype)
y = tf.reshape(y, (-1, 1))</code></pre>
<p>Next, we define our model distribution using <a href="https://www.tensorflow.org/probability/api_docs/python/tfp/distributions/JointDistributionCoroutineAutoBatched">Auto-Batched Joint Distributions</a>. Note how similar the model specification is to the mathematical definition.</p>
<pre class="python"><code>jds_ab = tfd.JointDistributionNamedAutoBatched(dict(

    sigma=tfd.HalfNormal(scale=[tf.cast(1.0, dtype)]),

    alpha=tfd.Normal(
        loc=[tf.cast(0.0, dtype)], 
        scale=[tf.cast(10.0, dtype)]
    ),

    beta=tfd.Normal(
        loc=[[tf.cast(0.0, dtype)], [tf.cast(0.0, dtype)]], 
        scale=[[tf.cast(10.0, dtype)], [tf.cast(10.0, dtype)]]
    ),

    y=lambda beta, alpha, sigma: 
        tfd.Normal(
            loc=tf.linalg.matmul(x, beta) + alpha, 
            scale=sigma
        ) 
))</code></pre>
</div>
<div id="prior-simulations" class="section level2">
<h2>Prior Simulations</h2>
<p>Next, before fitting the model, we want to generate predictions with the prior distributions.</p>
<pre class="python"><code># Sample from the prior.
prior_samples = jds_ab.sample(500)[&#39;y&#39;]</code></pre>
<p>Let us plot the predictions against the true values of <span class="math inline">\(y\)</span> with the corresponding credible intervals (<span class="math inline">\(\mu \pm 2\sigma\)</span>).</p>
<pre class="python"><code>prior_samples = tf.squeeze(prior_samples)
prior_mean = tf.math.reduce_mean(prior_samples, axis=0).numpy()
prior_std = tf.math.reduce_std(prior_samples, axis=0).numpy()

fig, ax = plt.subplots(figsize=(9, 8)) 
ax.errorbar(
    x=tf.squeeze(y).numpy(), 
    y=prior_mean, 
    yerr=2*prior_std,
    fmt=&#39;o&#39;,
    ecolor=sns_c[9],
    capsize=2,
    label=&#39;predictions + credible intervals&#39;,
)
sns.regplot(
    x=tf.squeeze(y).numpy(), 
    y=prior_mean, 
    color=sns_c[0], 
    scatter=False,
    line_kws=dict(alpha=0.5), 
    label=&#39;y ~ y_pred&#39;, 
    truncate=False,
    ax=ax
)
ax.axline(xy1=(0,0), slope=1, linestyle=&#39;--&#39;, color=sns_c[3], label=&#39;diagonal (y = y_pred)&#39;)
ax.legend(loc=&#39;lower right&#39;)
ax.set(title=&#39;Model Prior Predictions&#39;, xlabel=&#39;y&#39;, ylabel=&#39;y_pred&#39;);</code></pre>
<center>
<img src="../images/tfp_lm_files/tfp_lm_16_0.svg" title="fig:" alt="svg" />
</center>
<p>We see that the priors are very flat and the range for predictions is very wide.</p>
<p><strong>Remark:</strong> In many applications one would like to restrict the priors a little bit more to encode domain knowledge information.</p>
</div>
<div id="fit-model" class="section level2">
<h2>Fit Model</h2>
<p>Now we fit the model using <a href="https://en.wikipedia.org/wiki/Hamiltonian_Monte_Carlo">Hamiltonian Monte Carlo</a> (see <a href="https://juanitorduz.github.io/tfp_hcm/">this post</a> for another example).</p>
<p>First, we need to define the target function, which in this case is simply the log-probability.</p>
<pre class="python"><code>def target_log_prob_fn(beta=beta, alpha=alpha, sigma=sigma):
    return jds_ab.log_prob(beta=beta, alpha=alpha, sigma=sigma, y=y)</code></pre>
<p>Secondly, we specify the sampling method:</p>
<pre class="python"><code># Size of each chain.
num_results = int(1e4)
# Burn-in steps.
num_burnin_steps = int(1e3)
# Hamiltonian Monte Carlo transition kernel. 
# In TFP a TransitionKernel returns a new state given some old state.
hcm_kernel  = tfp.mcmc.HamiltonianMonteCarlo(
  target_log_prob_fn=target_log_prob_fn,
  step_size=1.0,
  num_leapfrog_steps=3
  
)
# This adapts the inner kernel&#39;s step_size.
adaptive_hmc = tfp.mcmc.SimpleStepSizeAdaptation(
  inner_kernel = hcm_kernel,
  num_adaptation_steps=int(num_burnin_steps * 0.8)
)
# Run the chain (with burn-in).
@tf.function
def run_chain():
  # Run the chain (with burn-in). 
  # Implements MCMC via repeated TransitionKernel steps.
  samples, is_accepted = tfp.mcmc.sample_chain(
      num_results=num_results,
      num_burnin_steps=num_burnin_steps,
      current_state=[
          tf.convert_to_tensor([[1.0], [1.0]], dtype=dtype),
          tf.convert_to_tensor([1.0], dtype=dtype), 
          tf.convert_to_tensor([1.0], dtype=dtype)
      ],
      kernel=adaptive_hmc,
      trace_fn=lambda _, pkr: pkr.inner_results.is_accepted
    )
  return samples</code></pre>
<p>Finally, we run the sampling 5 times.</p>
<pre class="python"><code># Set number of chains. 
num_chains = 5
# Run sampling. 
chains = [run_chain() for i in range(num_chains)]</code></pre>
</div>
<div id="visualize-posterior-distributions" class="section level2">
<h2>Visualize Posterior Distributions</h2>
<p>Let us collect and format the chains output for visualization.</p>
<p><strong>Remark:</strong> There are many (better) ways to format the output of the sampling. In addition, one could use <a href="https://arviz-devs.github.io/arviz/index.html">ArviZ</a> to generate the visualization. For now we just stick to <a href="https://matplotlib.org/">matplotlib</a> and <a href="https://seaborn.pydata.org/">seaborn</a>.</p>
<pre class="python"><code>chains_t = list(map(list, zip(*chains)))

chains_samples = [tf.squeeze(tf.concat(samples, axis=0)) for samples in chains_t]</code></pre>
<pre class="python"><code>chains_df = pd.concat(
    objs=[pd.DataFrame(samples.numpy()) for samples in chains_samples], 
    axis=1
)

params = [&#39;beta_0&#39;, &#39;beta_1&#39;, &#39;alpha&#39;, &#39;sigma&#39;]
chains_df.columns = params

chains_df = chains_df \
    .assign(
        sample_id=lambda x: range(x.shape[0]), 
        chain_sample_id=lambda x: x[&#39;sample_id&#39;] % num_results,
        chain_id=lambda x: (x[&#39;sample_id&#39;] / num_results).astype(int) + 1
    ) \
    .assign(chain_id=lambda x: &#39;c_&#39; + x[&#39;chain_id&#39;].astype(str)) \
    

chains_df.head()</code></pre>
<center>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
beta_0
</th>
<th>
beta_1
</th>
<th>
alpha
</th>
<th>
sigma
</th>
<th>
sample_id
</th>
<th>
chain_sample_id
</th>
<th>
chain_id
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
1.299382
</td>
<td>
2.939734
</td>
<td>
1.154646
</td>
<td>
0.938842
</td>
<td>
0
</td>
<td>
0
</td>
<td>
c_1
</td>
</tr>
<tr>
<th>
1
</th>
<td>
1.177881
</td>
<td>
2.647813
</td>
<td>
1.022244
</td>
<td>
0.979304
</td>
<td>
1
</td>
<td>
1
</td>
<td>
c_1
</td>
</tr>
<tr>
<th>
2
</th>
<td>
1.177881
</td>
<td>
2.647813
</td>
<td>
1.022244
</td>
<td>
0.979304
</td>
<td>
2
</td>
<td>
2
</td>
<td>
c_1
</td>
</tr>
<tr>
<th>
3
</th>
<td>
1.177881
</td>
<td>
2.647813
</td>
<td>
1.022244
</td>
<td>
0.979304
</td>
<td>
3
</td>
<td>
3
</td>
<td>
c_1
</td>
</tr>
<tr>
<th>
4
</th>
<td>
1.271985
</td>
<td>
2.644167
</td>
<td>
1.223968
</td>
<td>
1.049345
</td>
<td>
4
</td>
<td>
4
</td>
<td>
c_1
</td>
</tr>
</tbody>
</table>
</div>
</center>
<p>Let us plot the posterior distributions of the model parameters per chain:</p>
<pre class="python"><code>fig, axes = plt.subplots(nrows=len(params), ncols=2, figsize=(10, 8), constrained_layout=True)

for i, param in enumerate(params):
    sns.histplot(x=param, data=chains_df, hue=&#39;chain_id&#39;, kde=True, ax=axes[i][0])
    sns.lineplot(x=&#39;chain_sample_id&#39;, y=param, data=chains_df, hue=&#39;chain_id&#39;, alpha=0.3, legend=False, ax=axes[i][1])

fig.suptitle(&#39;Posterior Samples per Chain&#39;, y=1.03);</code></pre>
<center>
<img src="../images/tfp_lm_files/tfp_lm_28_0.svg" title="fig:" alt="svg" />
</center>
<p>Now we generate the sample plot but with all the chains combined.</p>
<pre class="python"><code>fig, axes = plt.subplots(nrows=len(params), ncols=2, figsize=(10, 8), constrained_layout=True)

for i, param in enumerate(params):
    sns.histplot(x=param, data=chains_df, color=sns_c[i], kde=True, ax=axes[i][0])
    sns.lineplot(x=&#39;sample_id&#39;, y=param, data=chains_df, color=sns_c[i], alpha=0.5, ax=axes[i][1])

fig.suptitle(&#39;Posterior Samples&#39;, y=1.03);</code></pre>
<center>
<img src="../images/tfp_lm_files/tfp_lm_30_0.svg" title="fig:" alt="svg" />
</center>
<p>The chains seems to have converged.</p>
</div>
<div id="generate-predictions" class="section level2">
<h2>Generate Predictions</h2>
<div id="in-sample" class="section level3">
<h3>In-Sample</h3>
<p>We want to see the model (in-sample) predictions. We begin by sampling from the distribution mean <span class="math inline">\(\mu\)</span> (that is, we ignore <span class="math inline">\(\sigma\)</span>).</p>
<pre class="python"><code># Here we compute mu = alpha + beta x.
mu_posterior_samples = tf.linalg.matmul(tf.reshape(chains_samples[1], (-1, 1)), tf.ones(shape=(1, x.shape[0]))) \
    + tf.linalg.matmul(chains_samples[0], tf.transpose(x)) </code></pre>
<p>Let us plot the results:</p>
<pre class="python"><code>mu_posterior_mean = tf.math.reduce_mean(mu_posterior_samples, axis=0).numpy()
mu_posterior_std = tf.math.reduce_std(mu_posterior_samples, axis=0).numpy()

fig, ax = plt.subplots(figsize=(9, 8)) 
ax.errorbar(
    x=tf.squeeze(y).numpy(), 
    y=mu_posterior_mean, 
    yerr=2*mu_posterior_std,
    fmt=&#39;o&#39;,
    ecolor=sns_c[9],
    capsize=2,
    label=&#39;predictions + credible intervals (of the mean)&#39;
)
sns.regplot(
    x=tf.squeeze(y).numpy(), 
    y=mu_posterior_mean, 
    color=sns_c[0], 
    scatter=False,
    line_kws=dict(alpha=0.5), 
    label=&#39;y ~ y_pred&#39;, 
    truncate=False,
    ax=ax
)
ax.axline(xy1=(0,0), slope=1, linestyle=&#39;--&#39;, color=sns_c[3], label=&#39;diagonal (y = y_pred)&#39;)
ax.legend(loc=&#39;lower right&#39;)
ax.set(title=&#39;Model Predictions&#39;, xlabel=&#39;y&#39;, ylabel=&#39;y_pred&#39;);</code></pre>
<center>
<img src="../images/tfp_lm_files/tfp_lm_35_0.svg" title="fig:" alt="svg" />
</center>
<p>Now let us generate (in-sample) predictions by sampling from <span class="math inline">\(N(\mu, \sigma)\)</span>.</p>
<pre class="python"><code>pred_samples = tf.map_fn(
    fn=lambda z: tfd.Normal(loc=z, scale=chains_samples[2]).sample(1), 
    elems=tf.transpose(mu_posterior_samples)
)

pred_samples = tf.squeeze(pred_samples)

posterior_mean = tf.math.reduce_mean(pred_samples, axis=1).numpy()
posterior_std = tf.math.reduce_std(pred_samples, axis=1).numpy()

fig, ax = plt.subplots(figsize=(9, 8)) 
ax.errorbar(
    x=tf.squeeze(y).numpy(), 
    y=posterior_mean, 
    yerr=2*posterior_std,
    fmt=&#39;o&#39;,
    ecolor=sns_c[9],
    capsize=2,
    label=&#39;predictions + credible intervals&#39;,
)
sns.regplot(
    x=tf.squeeze(y).numpy(), 
    y=mu_posterior_mean, 
    color=sns_c[0], 
    scatter=False,
    line_kws=dict(alpha=0.5), 
    label=&#39;y ~ y_pred&#39;, 
    truncate=False,
    ax=ax
)
ax.axline(xy1=(0,0), slope=1, linestyle=&#39;--&#39;, color=sns_c[3], label=&#39;diagonal (y = y_pred)&#39;)
ax.legend(loc=&#39;lower right&#39;)
ax.set(title=&#39;Model Predictions&#39;, xlabel=&#39;y&#39;, ylabel=&#39;y_pred&#39;);</code></pre>
<center>
<img src="../images/tfp_lm_files/tfp_lm_37_0.svg" title="fig:" alt="svg" />
</center>
</div>
<div id="out-sample" class="section level3">
<h3>Out-Sample</h3>
<p>Finally let us generate an out-sample. prediction for the vector</p>
<p><span class="math display">\[
x_* =
\left(
\begin{array}{c}
3 \\
1
\end{array}
\right)
\]</span></p>
<p>Again, remember we are not interested in the point prediction but rather on the complete posterior distribution.</p>
<pre class="python"><code>x_star = tf.cast([[3.0, 1.0]], dtype)
# Here we compute mu = alpha + beta x.
mu_y_star = tf.linalg.matmul(tf.reshape(chains_samples[1], (-1, 1)), tf.ones(shape=(1, x_star.shape[0]))) \
    + tf.linalg.matmul(chains_samples[0], tf.transpose(x_star)) 
# Compute posterior predictive distribution.
y_star_samples = tf.map_fn(
    fn=lambda z: tfd.Normal(loc=z, scale=chains_samples[2]).sample(1), 
    elems=tf.transpose(mu_y_star)
)</code></pre>
<p>Let us not generate samples from the “true” distribution:</p>
<pre class="python"><code># Point prediction of the mean.
y_star_true = tf.linalg.matmul(x_star, tf.transpose(tf.cast([beta], dtype))) + alpha
y_star_true  = y_star_true.numpy().flatten()
# Sample from the complete posterior predictive distribution.
y_star_true_samples =tfd.Normal(loc=y_star_true, scale=sigma).sample(num_chains * num_results)
y_star_true_samples = y_star_true_samples.numpy().flatten()
# Mean of the complete posterior predictive distribution.
y_star_true_samples_mean = tf.reduce_mean(y_star_samples, axis=2).numpy().flatten()</code></pre>
<p>Now we plot the results:</p>
<pre class="python"><code>fig, ax = plt.subplots(figsize=(10, 6))

sns.histplot(
    x=tf.squeeze(y_star_samples).numpy(), 
    kde=True, 
    color=sns_c[6], 
    alpha=0.4,
    label=&#39;posterior predictive distribution&#39;, 
    ax=ax
)
ax.axvline(
    x=y_star_true_samples_mean, 
    color=sns_c[6], 
    linestyle=&#39;--&#39;, 
    label=f&#39;posterior predictive distribution mean = {y_star_true_samples_mean[0]: 0.2f}&#39;
)
sns.histplot(
    x=mu_y_star.numpy().flatten(), 
    kde=True,
    color=sns_c[4], 
    alpha=0.4,
    label=&#39;$\mu$ posterior predictive distribution&#39;, 
    ax=ax
)
sns.histplot(
    x=y_star_true_samples, 
    kde=True,
    color=sns_c[3], 
    alpha=0.4,
    label=&#39;$y_*$ true distribution&#39;, 
    ax=ax
)
ax.axvline(
    x=y_star_true, 
    color=sns_c[3], 
    linestyle=&#39;--&#39;, 
    label=f&#39;$y_*$ true mean = {y_star_true[0]: 0.2f}&#39;
)
ax.legend(loc=&#39;upper right&#39;)
ax.set(
    title=f&#39;$y_*$ true and (posterior) predictive distributions ({num_chains * num_results} Samples)&#39;,
    xlabel=f&#39;$y_*$&#39;
);</code></pre>
<center>
<img src="../images/tfp_lm_files/tfp_lm_43_0.svg" title="fig:" alt="svg" />
</center>
<ul>
<li><p>The <span style="color:red"><strong>red</strong></span> distribution is the “true” one which we get by sampling from <span class="math inline">\(N(\mu_*, \sigma)\)</span>, where <span class="math inline">\(\mu_* = \alpha + \beta x_*\)</span> (known values of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span>).</p></li>
<li><p>The <span style="color:purple"><strong>purple</strong></span> distribution is the one of the mean posterior samples of <span class="math inline">\(\hat{\mu}_* = \hat{\alpha} + \hat{\beta} x_*\)</span>. Here <span class="math inline">\(\hat{\alpha}\)</span> and <span class="math inline">\(\hat{\beta}\)</span> denote samples from the posterior.</p></li>
<li><p>The <span style="color:pink"><strong>pink</strong></span> distribution is the complete posterior predictive distribution, i.e. <span class="math inline">\(N(\hat{\mu}_*, \hat{\sigma}^2)\)</span>.</p></li>
</ul>
</div>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="../index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="../images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/yaml.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/dockerfile.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/bash.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="../js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'G-5NM5EDH834', 'auto');
	
	ga('send', 'pageview');
}
</script>

  </body>
</html>

