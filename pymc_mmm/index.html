<!DOCTYPE html>
<html lang="en-us">
  <head>
    <script defer src="https://use.fontawesome.com/releases/v5.13.0/js/all.js"></script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-5NM5EDH834"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-5NM5EDH834');
</script>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.79.1" />


<title>Media Effect Estimation with PyMC: Adstock, Saturation &amp; Diminishing Returns - Dr. Juan Camilo Orduz</title>
<meta property="og:title" content="Media Effect Estimation with PyMC: Adstock, Saturation &amp; Diminishing Returns - Dr. Juan Camilo Orduz">


  <link href='../images/favicon.ico' rel='icon' type='image/x-icon'/>



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="../css/fonts.css" media="all">
<link rel="stylesheet" href="../css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="../" class="nav-logo">
    <img src="../images/sphere2.gif"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="../about/"> About</a></li>
    
    <li><a href="https://github.com/juanitorduz"><i class='fab fa-github fa-2x'></i>  </a></li>
    
    <li><a href="https://www.linkedin.com/in/juanitorduz/"><i class='fab fa-linkedin fa-2x' style='color:#0a66c2;'></i>  </a></li>
    
    <li><a href="https://bayes.club/@juanitorduz"><i class='fab fa-mastodon fa-2x' style='color:#6364FF;'></i>  </a></li>
    
    <li><a href="https://twitter.com/juanitorduz"><i class='fab fa-twitter fa-2x' style='color:#1DA1F2;'></i>  </a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">37 min read</span>
    

    <h1 class="article-title">Media Effect Estimation with PyMC: Adstock, Saturation &amp; Diminishing Returns</h1>

    
    <span class="article-date">2022-02-11</span>
    

    <div class="article-content">
      


<p>In this notebook we present a concrete example of estimating the media effects via bayesian methods, following the strategy outlined in Google’s paper <a href="https://research.google/pubs/pub46001/">Jin, Yuxue, et al. “Bayesian methods for media mix modeling with carryover and shape effects.” (2017)</a>. This example can be considered the continuation of the post <a href="https://juanitorduz.github.io/orbit_mmm/">Media Effect Estimation with Orbit’s KTR Model</a>. However, it is not strictly necessary to read before as we make this notebook self-contained. In addition, we provide some remarks and references regarding MMM projects in practice.</p>
<div id="data-generation-process" class="section level2">
<h2>Data Generation Process</h2>
<p>In <a href="https://juanitorduz.github.io/orbit_mmm/">Part I</a> of the post <a href="https://juanitorduz.github.io/orbit_mmm/">Media Effect Estimation with Orbit’s KTR Model</a>, we generated a synthetic dataset where we modeled a target variable <code>y</code> (sales) as a function of a trend, a seasonal component and an external regressor <code>z</code> (media spend). The effect of <code>z</code> on <code>y</code> was specified by the composition two transformations: a carryover effect (adstock) and a shape (saturation) effect.These two transformations have proven successful in practical Media Mix Modeling.</p>
<ul>
<li>The (geometric) <a href="https://en.wikipedia.org/wiki/Advertising_adstock">adstock transformation</a> is parametrized by the decaying parameter <span class="math inline">\(\alpha\)</span> and the carryover parameter <span class="math inline">\(\ell\)</span>. For this specific dataset, we set <span class="math inline">\(\alpha = 0.5\)</span> and <span class="math inline">\(\ell =12\)</span>.</li>
<li>The saturation effect is parametrized by the shape parameter <span class="math inline">\(\lambda\)</span>. In this example we set <span class="math inline">\(\lambda=0.15\)</span>.</li>
</ul>
<p>In the previous post (where we used the greek letter <span class="math inline">\(\mu\)</span> for the shape parameter), we transformed the variable <code>z</code> as:</p>
<p><span class="math display">\[
z \xrightarrow{\text{adstock}(\alpha)} z_{\text{adstock}} \xrightarrow{\text{saturation}(\lambda)} z_{\text{effect}}
\]</span></p>
<p>and generated <code>y</code> as:</p>
<p><span class="math display">\[
y(t) = \beta_{0} + \beta_{\text{trend}}\:\text{trend} + \beta_{\text{seasonality}}\:\text{seasonality} + \beta_{z}(t)\:z_{\text{effect}} + \varepsilon
\]</span></p>
<p>where the beta coefficient <span class="math inline">\(\beta_{z}(t)\)</span> was a (smooth) decaying function encoding the diminishing returns over time.</p>
</div>
<div id="prophet-and-ktr-models" class="section level2">
<h2>Prophet and KTR Models</h2>
<p>In the previous post the <a href="https://juanitorduz.github.io/orbit_mmm/">Media Effect Estimation with Orbit’s KTR Model</a> we fitted two models:</p>
<ul>
<li><p><strong>Prophet:</strong> Given the strong seasonal patter nof the time series, we used a <a href="https://facebook.github.io/prophet/">Prophet</a> model as a baseline. This model was able to successfully capture the trend ans seasonal components. On the other hand, the estimated regression coefficients <span class="math inline">\(\widehat{\beta}_{z}(t)=\widehat{\beta}_{\text{Prophet}}\)</span> was a constant (i.e. constant over time, as expected) very close to the median of <span class="math inline">\(z_{\text{effect}}\)</span>.</p></li>
<li><p><strong>KTR (Kernel-based Time-varying Regression):</strong> The second model we used was <a href="https://github.com/uber/orbit">Orbit</a>’s <a href="https://orbit-ml.readthedocs.io/en/latest/tutorials/ktr1.html">KTR</a> model, on which regression coefficients are allowed to vary over time by using <a href="https://en.wikipedia.org/wiki/Kernel_smoother"><em>kernel smooths</em></a> (see <a href="https://arxiv.org/abs/2106.03322">Edwin, Ng, et al. “Bayesian Time Varying Coefficient Model with Applications to Marketing Mix Modeling”</a> for more details). For this example, the model has able to give a good approximation <span class="math inline">\(\widehat{\beta}_{z}(t)=\widehat{\beta}_{\text{KTR}}\)</span> to the true <span class="math inline">\(\beta_{z}(t)\)</span> coefficient.</p></li>
</ul>
<p>It is important to emphasize that both models where fitted using <code>z_adstock</code> as the external regressor. That is, we assumed the value of alpha was given as it is not straight forward to estimate it using the models above.</p>
</div>
<div id="pymc-model" class="section level2">
<h2>PyMC Model</h2>
<p>Motivated by the results above, we now want to build a bayesian model to estimate <span class="math inline">\(\alpha\)</span>, <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(\beta_z(t)\)</span> simultaneously (as well as the other regression coefficients for the trend and seasonality). We will use the <a href="https://www.pymc.io/projects/docs/en/stable/learn.html">PyMC</a> motivated by the following great resources:</p>
<ol style="list-style-type: decimal">
<li>Simulated Example by <a href="https://dr-robert-kuebler.medium.com/">Dr. Robert Kübler</a>:</li>
</ol>
<ul>
<li><a href="https://towardsdatascience.com/an-upgraded-marketing-mix-modeling-in-python-5ebb3bddc1b6">An Upgraded Marketing Mix Modeling in Python</a></li>
<li><a href="https://towardsdatascience.com/bayesian-marketing-mix-modeling-in-python-via-pymc3-7b2071f6001a">Bayesian Marketing Mix Modeling in Python via PyMC3</a></li>
<li><a href="https://towardsdatascience.com/rockin-rolling-regression-in-python-via-pymc3-e4722b6118cd">Rockin‘ Rolling Regression in Python via PyMC3</a></li>
</ul>
<ol start="2" style="list-style-type: decimal">
<li>HelloFresh’s Media Mix Model: <a href="https://towardsdatascience.com/bayesian-marketing-mix-modeling-in-python-via-pymc3-7b2071f6001a">Bayesian Marketing Mix Modeling in Python via PyMC3</a>. Here are some additional references for this use cases:</li>
</ol>
<ul>
<li>Article: <a href="https://engineering.hellofresh.com/bayesian-media-mix-modeling-using-pymc3-for-fun-and-profit-2bd4667504e6">Bayesian Media Mix Modeling using PyMC3, for Fun and Profit</a></li>
<li>Video: <a href="https://www.youtube.com/watch?v=UznM_-_760Y">A Bayesian Approach to Media Mix Modeling by Michael Johns &amp; Zhenyu Wang</a></li>
<li>Articles by <a href="https://www.pymc-labs.io/">PyMC Labs</a>:
<ul>
<li><a href="https://www.pymc-labs.io/blog-posts/bayesian-media-mix-modeling-for-marketing-optimization/">Bayesian Media Mix Modeling for Marketing Optimization</a></li>
<li><a href="https://www.pymc-labs.io/blog-posts/reducing-customer-acquisition-costs-how-we-helped-optimizing-hellofreshs-marketing-budget/">Improving the Speed and Accuracy of Bayesian Media Mix Models</a></li>
</ul></li>
</ul>
<p>Of course, the main motivation is Google’s paper <a href="https://research.google/pubs/pub46001/">Jin, Yuxue, et al. “Bayesian methods for media mix modeling with carryover and shape effects” (2017)</a>. Moreover, for a discussion of MMM in practice please see <a href="https://research.google/pubs/pub45998/">Chan, David, et al. “Challenges and Opportunities in Media Mix Modeling” (2017)</a></p>
<p>As usual in applied data analysis, we will start from simple models and iterate to add more complexity. Moreover, we will follow the recommended <a href="https://arxiv.org/abs/2011.01808">bayesian workflow</a>.</p>
<hr />
</div>
<div id="some-additional-remarks-on-mmm-in-practice" class="section level2">
<h2>Some Additional Remarks on MMM in Practice</h2>
<div id="mmm-projects" class="section level3">
<h3>MMM Projects</h3>
<p>There are many existing projects regarding media mix models. Two of the most known ones are</p>
<ul>
<li><a href="https://facebookexperimental.github.io/Robyn/">Facebook’s Robyn</a>: This model uses a combination of <a href="https://facebook.github.io/prophet/">Prophet</a> (trend-seasonality model) and a ridge regression to model media data.</li>
<li><a href="https://github.com/google/lightweight_mmm">Google’s LightweightMMM</a>: This project in essence follows the approach we describe in this post. The code is written in <a href="https://github.com/pyro-ppl/numpyro">Numpyro</a>.</li>
</ul>
</div>
<div id="which-media-data-to-use" class="section level3">
<h3>Which Media Data to Use?</h3>
<p>In this simulated example we are using <em>media spend</em> as regressor for the media variable. Nevertheless, in practice this is not the best choice as described in the <a href="https://facebookexperimental.github.io/Robyn/docs/analysts-guide-to-MMM">Analysts guide to MMM</a> from <a href="https://facebookexperimental.github.io/Robyn/">Facebook’s Robyn</a> documentation:</p>
<blockquote>
<p><em>Data collected for media ideally should reflect how many “eyeballs” have seen or been exposed to the media (e.g. impressions, GRPs). Spends should also be collected in order to calculate Return On Investment, however it is best practice to use exposure metrics as direct inputs into the model, as this is a better representation than spends of how media activity has been consumed by consumers.</em></p>
</blockquote>
<p>So, if we use impressions in the model, how to include the cost data? There are various alternatives. For example:</p>
<ul>
<li><p><a href="https://facebookexperimental.github.io/Robyn/">Facebook’s Robyn</a>: Includes the costs data as part of the model selection. They train many models and the user has to select the best among two metrics: model fit (NRMSE) and the predicted cost distribution (DECOMP.RSSD), see <a href="https://facebookexperimental.github.io/Robyn/docs/analysts-guide-to-MMM#modeling-techniques-">here</a> for details.</p></li>
<li><p><a href="https://github.com/google/lightweight_mmm">Googles’s LightweightMMM</a>: Their approach is fully-bayesian so they include the cost data as part of the <em>prior</em> distributions of the model.</p></li>
</ul>
<p><strong>Remark:</strong> In practice, I have used <a href="https://github.com/google/lightweight_mmm">Googles’s LightweightMMM</a> approach to include costs as part of the priors with PyMC models as the one presented in this post. The results are quite good and personally I like it more that the manual-model selection approach.</p>
</div>
<div id="controling-for-seasonality-and-other-factors-causal-graph" class="section level3">
<h3>Controling for Seasonality and other Factors: Causal Graph</h3>
<p>As <a href="https://xcelab.net/rm/">Richard McElreath</a> would say (paraphrasing from his amazing lectures!) “Do not try to be clever, build a causal graph model!” Indeed, it is not worth trying to add all control variables to the media mix models and hope to solve all confounding effects. A much better approach (strongly recommended!) is to build a causal graph. For more details about this approach I recommend <a href="https://xcelab.net/rm/statistical-rethinking/">Statistical Rethinking</a> by <a href="https://xcelab.net/rm/">Richard McElreath</a> (and the corresponding YouTube <a href="https://www.youtube.com/watch?v=BYUykHScxj8&amp;list=PLDcUM9US4XdMROZ57-OIRtIK0aOynbgZN">videos</a>). Once we have a causal model, we can actually use software (for example <a href="https://cran.r-project.org/web/packages/dagitty/index.html"><code>dagitty</code>: Graphical Analysis of Structural Causal Models</a> in R) to help us with the regression model structure. For example, let’s consider a simple causal graph:</p>
<pre class="r"><code>library(dagitty)

dag &lt;- dagitty( x = &quot;dag {
  TV -&gt; Search -&gt; Conversions
  TV -&gt; Facebook -&gt; Conversions
  TV -&gt; Conversions
  Radio -&gt; Search -&gt; Conversions
  Radio -&gt; Facebook -&gt; Conversions
  Radio -&gt; Conversions
  Seasonality -&gt; Conversions
  Seasonality -&gt; Search
  Seasonality -&gt; Facebook
}&quot; )

coordinates( x = dag ) &lt;- list(
  x = c(
    &quot;Seasonality&quot; = 0,
    &quot;TV&quot; = 0,
    &quot;Radio&quot; = 0,
    &quot;Search&quot; = 1,
    &quot;Facebook&quot; = 1,
    &quot;Conversions&quot; = 2
  ),
  y = c(
    &quot;Seasonality&quot; = 1,
    &quot;TV&quot; = -1,
    &quot;Radio&quot; = 0,
    &quot;Search&quot; = 1,
    &quot;Facebook&quot; = -1,
    &quot;Conversions&quot; = 0
  )
)

plot( dag )</code></pre>
<p><img src="../post/pymc_mmm_files/figure-html/unnamed-chunk-1-1.png" width="672" style="display: block; margin: auto;" /></p>
<p>We would like to estimate the effect of <code>TV</code> on <code>Conversions</code>. To begin, we can get the <em>list of conditional independence statements that must hold in every probability distribution compatible with the given model</em>.</p>
<pre class="r"><code>impliedConditionalIndependencies( x = dag )</code></pre>
<pre><code>## Fcbk _||_ Srch | Radi, Ssnl, TV
## Radi _||_ Ssnl
## Radi _||_ TV
## Ssnl _||_ TV</code></pre>
<p>We can use the data to test these implications. Next, we can get the <em>sets of covariates that (asymptotically) allow unbiased estimation of causal effects from observational data, assuming that the input causal graph is correct</em> (by looking into the back-door criterion). For example to estimate the direct effect of <code>TV</code> on <code>Conversions</code> we run:</p>
<pre class="r"><code>adjustmentSets( x = dag , exposure = &quot;TV&quot;, outcome = &quot;Conversions&quot; )</code></pre>
<pre><code>##  {}</code></pre>
<p>This shows that the regression model structure should be</p>
<center>
<code>Conversions ~ Intercept + TV</code>.
</center>
<p>On the other hand, if we want the effect of <code>Facebook</code> to <code>Conversions</code> we run:</p>
<pre class="r"><code>adjustmentSets( x = dag , exposure = &quot;Facebook&quot;, outcome = &quot;Conversions&quot; )</code></pre>
<pre><code>## { Radio, Seasonality, TV }</code></pre>
<p>Hence, the correct model (given the graph causal model!) is</p>
<center>
<code>Conversions ~ Intercept + Facebook + TV + Radio + Seasonality</code>
</center>
<p>Again, no need to be clever, just specify the causal model. One could argue that for digital channels (like <code>Facebook</code>) the impressions data is already driven by seasonality, so adding additional Fourier modes can be counterproductive. However, given a causal graph as above this is actually not the case. One can extend this type of analysis for additional external factors to be consider in the media mix model.</p>
<p><strong>Remark:</strong> There are of course other strategies to model seasonality. Certain modelers suggest to add time-variant coefficients to encode seasonality via Gaussian processes (see for example the post <a href="https://getrecast.com/seasonality/">You’re probably modeling seasonality the wrong way</a>).</p>
<p><strong>Remark</strong>: To estimate the cost of incremental sales by considering the funnels effects (causal graph) please refer to the excellent blog post <a href="https://engineering.hellofresh.com/bayesian-media-mix-modeling-using-pymc3-for-fun-and-profit-2bd4667504e6">Bayesian Media Mix Modeling using PyMC3, for Fun and Profit</a></p>
<p>For more examples and details about <em>good and bad controls</em> you can look into the article <a href="https://ftp.cs.ucla.edu/pub/stat_ser/r493.pdf">A Crash Course in Good and Bad Controls</a>.</p>
</div>
<div id="adstock-and-saturation-order" class="section level3">
<h3>Adstock and Saturation Order</h3>
<p>Note that the adstock and saturation transformations <strong>do not commute</strong> (i.e. the order matters). So which one to apply first? I recommend you look into Section <em>2.3 Combining the Carryover and the Shape Effect</em> in <a href="https://research.google/pubs/pub46001/">Jin, Yuxue, et al. “Bayesian methods for media mix modeling with carryover and shape effects.” (2017)</a>. The rule of thumb is to apply:</p>
<blockquote>
<ul>
<li><em>Adstock transformation first if:</em> the media spend in each time period is relatively small compared to the cumulative spend across multiple time periods.</li>
</ul>
</blockquote>
<blockquote>
<ul>
<li><em>Saturation transformation first if:</em> the media spend is heavily concentrated in some single time periods with an on-and-off pattern.</li>
</ul>
</blockquote>
</div>
<div id="roas-and-mroas" class="section level3">
<h3>ROAS and mROAS</h3>
<p>One of the most important outputs of the model are the Return on Ad Spend (ROAS) and and the
marginal ROAS (mROAS), see Section 4.1 in <a href="https://research.google/pubs/pub46001/">Jin, Yuxue, et al. “Bayesian methods for media mix modeling with carryover and shape effects.” (2017)</a>. In this post we describe how to extract them from the bayesian model in PyMC. Moreover, we compare the inferred with the true values which can be easily computed in an additive model.</p>
<p>In practice one can actually calibrate the model through the priors (this is one of the key benefits of going fully bayesian) using geo-lift tests, see for example the article <a href="https://research.google/pubs/pub45950/">Estimating Ad Effectiveness using Geo Experiments in a Time-Based Regression Framework</a> (and for alternative methodologies see for example the blog post <a href="https://juanitorduz.github.io/synthetic_control_pymc/">Synthetic Control in PyMC</a>).</p>
</div>
<div id="it-is-all-about-the-data" class="section level3">
<h3>It is <strong>All</strong> about the Data</h3>
<p>As any other data science project the data is the key factor of the success of an MMM project. As the main idea of an MMM is to estimate effects across many digital and offline channels, the data collection can be challenging (it usually is). Hence, the recommendation is to spend a significant par of the project collecting and understanding the data: Where is the data coming from? How is it transformed and stored? Outliers? Missing values?</p>
<p><strong>Remark:</strong> The <a href="https://facebookexperimental.github.io/Robyn/docs/analysts-guide-to-MMM">Analysts guide to MMM</a> from <a href="https://facebookexperimental.github.io/Robyn/">Facebook’s Robyn</a> gives nice tips about planing and executing an MMM project (steps, data collection, timeline, etc.).</p>
<hr />
</div>
</div>
<div id="prepare-notebook" class="section level2">
<h2>Prepare Notebook</h2>
<pre class="python"><code>import pytensor.tensor as pt
import arviz as az
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import pymc as pm
import pymc.sampling_jax
import seaborn as sns
from scipy.stats import pearsonr
from sklearn.preprocessing import MaxAbsScaler
import xarray as xr

plt.style.use(&quot;bmh&quot;)
plt.rcParams[&quot;figure.figsize&quot;] = [10, 6]
plt.rcParams[&quot;figure.dpi&quot;] = 100

%load_ext rich
%load_ext autoreload
%autoreload 2
%config InlineBackend.figure_format = &quot;retina&quot;</code></pre>
</div>
<div id="read-data" class="section level2">
<h2>Read Data</h2>
<p>We start by reading the data. This <code>csv</code> was generated in the post <a href="https://juanitorduz.github.io/orbit_mmm/">Media Effect Estimation with Orbit’s KTR Model</a>, please refer to it for details. Here we give a quick overview of the data.</p>
<pre class="python"><code>data_path = &quot;https://raw.githubusercontent.com/juanitorduz/website_projects/master/data/ktr_data.csv&quot;

data_df = pd.read_csv(data_path, parse_dates=[&quot;date&quot;])

data_df.info()</code></pre>
<pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
RangeIndex: 179 entries, 0 to 178
Data columns (total 20 columns):
 #   Column                Non-Null Count  Dtype         
---  ------                --------------  -----         
 0   index                 179 non-null    int64         
 1   date                  179 non-null    datetime64[ns]
 2   year                  179 non-null    int64         
 3   month                 179 non-null    int64         
 4   dayofyear             179 non-null    int64         
 5   z                     179 non-null    float64       
 6   z_adstock             179 non-null    float64       
 7   z_adstock_saturated   179 non-null    float64       
 8   beta                  179 non-null    float64       
 9   z_effect              179 non-null    float64       
 10  effect_ratio          179 non-null    float64       
 11  effect_ratio_smooth   179 non-null    float64       
 12  trend                 179 non-null    float64       
 13  cs                    179 non-null    float64       
 14  cc                    179 non-null    float64       
 15  seasonality           179 non-null    float64       
 16  intercept             179 non-null    float64       
 17  trend_plus_intercept  179 non-null    float64       
 18  epsilon               179 non-null    float64       
 19  y                     179 non-null    float64       
dtypes: datetime64[ns](1), float64(15), int64(4)
memory usage: 28.1 KB</code></pre>
<p>Let us now plot the most relevant variables for the analysis:</p>
<pre class="python"><code>fig, ax = plt.subplots(
    nrows=3,
    ncols=1,
    figsize=(12, 9),
    sharex=True,
    sharey=False,
    layout=&quot;constrained&quot;
)
sns.lineplot(x=&quot;date&quot;, y=&quot;y&quot;, color=&quot;black&quot;, data=data_df, ax=ax[0])
ax[0].set(title=&quot;Sales (Target Variable)&quot;)
sns.lineplot(x=&quot;date&quot;, y=&quot;z_effect&quot;, color=&quot;C3&quot;, data=data_df, ax=ax[1])
ax[1].set(title=&quot;Media Cost Effect on Sales&quot;)
sns.lineplot(x=&quot;date&quot;, y=&quot;z&quot;, data=data_df, ax=ax[2])
ax[2].set(title=&quot;Raw Media Cost Data&quot;);</code></pre>
<center>
<img src="../images/pymc_mmm_files/pymc_mmm_8_0.png" style="width: 1000px;"/>
</center>
<ol style="list-style-type: decimal">
<li>The first plot is our target variable <code>y</code>, which can represent sales data, for example.</li>
<li>The second plot is the <code>z_effect</code> variable, which is the simulated effect of the media spent variable <code>z</code> on the variable <code>y</code>. <strong>In practice we do not know <code>z_effect</code>. We would like to infer it from the data!</strong></li>
<li>The last plot represent the input data <code>z</code> which is something we have control of.</li>
</ol>
<p>Note that the variable <code>y</code> has a trend and strong (additive) yearly seasonality components.</p>
</div>
<div id="features" class="section level2">
<h2>Features</h2>
<p>We of course do not want to use the trend os seasonal components from the <code>data_df</code> dataframe, as the whole point id to learn them tom the data. Hence, let us keep the variables we would actually have in when developing the model.</p>
<pre class="python"><code>columns_to_keep = [&quot;index&quot;, &quot;date&quot;, &quot;year&quot;, &quot;month&quot;, &quot;dayofyear&quot;, &quot;z&quot;, &quot;y&quot;]

df = data_df[columns_to_keep].copy()</code></pre>
<p>Next, we generate input features to model the trend and seasonal components. We follow the strategy presented in the very comprehensive post <a href="https://docs.pymc.io/en/stable/pymc-examples/examples/time_series/Air_passengers-Prophet_with_Bayesian_workflow.html">Air passengers - Prophet-like model</a> from the <a href="https://docs.pymc.io/en/stable/pymc-examples/README.html"><code>pymc-examples</code></a> repository (please check it out!).</p>
<div id="trend" class="section level3">
<h3>Trend</h3>
<p>For the trend component we simply use a linear feature (which we scale between <span class="math inline">\(0\)</span> and <span class="math inline">\(1\)</span>).</p>
<pre class="python"><code>t = (df.index - df.index.min()) / (df.index.max() - df.index.min())</code></pre>
</div>
<div id="seasonality" class="section level3">
<h3>Seasonality</h3>
<p>To model the seasonality, we use <a href="https://en.wikipedia.org/wiki/Fourier_series">Fourier modes</a> (similarly as in Prophet or Orbit).</p>
<pre class="python"><code>n_order = 7
periods = df[&quot;dayofyear&quot;] / 365.25
fourier_features = pd.DataFrame(
    {
        f&quot;{func}_order_{order}&quot;: getattr(np, func)(2 * np.pi * periods * order)
        for order in range(1, n_order + 1)
        for func in (&quot;sin&quot;, &quot;cos&quot;)
    }
)</code></pre>
<p>We can see how these cyclic features look like:</p>
<pre class="python"><code>fig, ax = plt.subplots(nrows=2, sharex=True, layout=&quot;constrained&quot;)
fourier_features.filter(like=&quot;sin&quot;).plot(color=&quot;C0&quot;, alpha=0.15, ax=ax[0])
ax[0].get_legend().remove()
ax[0].set(title=&quot;Fourier Modes (Sin)&quot;, xlabel=&quot;index (week)&quot;)
fourier_features.filter(like=&quot;cos&quot;).plot(color=&quot;C1&quot;, alpha=0.15, ax=ax[1])
ax[1].get_legend().remove()
ax[1].set(title=&quot;Fourier Modes (Cos)&quot;, xlabel=&quot;index (week)&quot;);</code></pre>
<center>
<img src="../images/pymc_mmm_files/pymc_mmm_18_0.png" style="width: 1000px;"/>
</center>
<p>Finally, we extract the target and features as <code>numpy</code> arrays.</p>
<pre class="python"><code>date = df[&quot;date&quot;].to_numpy()
date_index = df.index
y = df[&quot;y&quot;].to_numpy()
z = df[&quot;z&quot;].to_numpy()
t = t.values
n_obs = y.size</code></pre>
</div>
<div id="scaling" class="section level3">
<h3>Scaling</h3>
<p>We scale both the target variable <code>y</code> and the channel input <code>z</code> using a <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html"><code>MaxAbsScaler</code></a>. During the whole analysis we carefully study the effect of these scalers and describe how to recover back the predictions and effects in the original scale.</p>
<p><strong>Remark:</strong> One would be tempted to use a <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MinMaxScaler.html"><code>MinMaxScaler</code></a> here. However, this would lead to a problem when computing the
(m)ROAS (see more details below). The reason is that for the ROAS computation we want to set the media cost to <span class="math inline">\(0\)</span> (in the raw data
scale!) and generate in-sample predictions. The problem comes when the media data never reaches <span class="math inline">\(0\)</span>. In this case,
when using a <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html"><code>MaxAbsScaler</code></a>, the zero value of the transformed variable represents the minimum value of the
original variable. Hence, when setting the media cost to <span class="math inline">\(0\)</span>, the transformed variable would be less than zero.
This itself not a problem, but since most og the times we are restricting the regression coefficients of the
media variables to be positive (e.g. via a <a href="https://www.pymc.io/projects/docs/en/latest/api/distributions/generated/pymc.HalfNormal.html"><code>pm.HalfNormal</code></a> prior) then the media contribution of in the ROAS computation would be negative! This of course does not make sense. Hence, we use a <a href="https://scikit-learn.org/stable/modules/generated/sklearn.preprocessing.MaxAbsScaler.html"><code>MaxAbsScaler</code></a> here, which
ensures that the zero in the transformed variable is the zero in the original variable.</p>
<pre class="python"><code>endog_scaler = MaxAbsScaler()
endog_scaler.fit(y.reshape(-1, 1))
y_scaled = endog_scaler.transform(y.reshape(-1, 1)).flatten()

channel_scaler = MaxAbsScaler()
channel_scaler.fit(z.reshape(-1, 1))
z_scaled = channel_scaler.transform(z.reshape(-1, 1)).flatten()</code></pre>
</div>
</div>
<div id="models" class="section level2">
<h2>Models</h2>
<p>In this section we fit <span class="math inline">\(3\)</span> models, from simpler to complex:
1. <strong>Base Model:</strong> We fit a linear regression model with a single regressor <code>z</code> and controlling from trend ans seasonality.</p>
<ol start="2" style="list-style-type: decimal">
<li><p><strong>Adstock-Saturation Model:</strong> We use the same model structure as the base model but we now apply the (geometric) adstock and saturation transformations to the <code>z</code> variable. We <strong>do not</strong> set a value for <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\lambda\)</span> as we learn them from the data. We do fix the variable <span class="math inline">\(\ell=12\)</span> of the adstock transformation.</p></li>
<li><p><strong>Adstock-Saturation-Diminishing Returns Model:</strong> We use the same model structure as the Adstock-Saturation model but we allow a time-varying coefficient for (the transformed) <code>z</code> by modeling it as a gaussian random walk.</p></li>
</ol>
<p>Here are some comments on the models:</p>
<ul>
<li>We use a <a href="https://www.pymc.io/projects/docs/en/latest/api/distributions/generated/pymc.HalfNormal.html"><code>pm.HalfNormal</code></a> distribution for the media coefficients to ensure they are positive.</li>
<li>We use a <a href="https://docs.pymc.io/en/latest/api/distributions/generated/pymc.Laplace.html"><code>pm.Laplace</code></a> distribution fot the fourier coefficient to add certain regularization (these features can easily lead to an overfit).</li>
<li>For the likelihood function we use a <a href="https://www.pymc.io/projects/docs/en/latest/api/distributions/generated/pymc.StudentT.html"><code>pm.StudentT</code></a> distribution which is most robust against outliers.</li>
</ul>
<div id="base-model" class="section level3">
<h3>Base Model</h3>
<p>Let us start by defining the structure of the base model, which is the be the core of the models to come.</p>
<ul>
<li>Model Specification</li>
</ul>
<pre class="python"><code>coords = {&quot;date&quot;: date, &quot;fourier_mode&quot;: np.arange(2 * n_order)}

with pm.Model(coords=coords) as base_model:
    # --- coords ---
    base_model.add_coord(name=&quot;dat&quot;, values=date, mutable=True)
    base_model.add_coord(name=&quot;fourier_mode&quot;, values=np.arange(2 * n_order), mutable=False)

    # --- data containers ---
    z_scaled_ = pm.MutableData(name=&quot;z_scaled&quot;, value=z_scaled, dims=&quot;date&quot;)

    # --- priors ---
    ## intercept
    a = pm.Normal(name=&quot;a&quot;, mu=0, sigma=4)
    ## trend
    b_trend = pm.Normal(name=&quot;b_trend&quot;, mu=0, sigma=2)
    ## seasonality
    b_fourier = pm.Laplace(name=&quot;b_fourier&quot;, mu=0, b=2, dims=&quot;fourier_mode&quot;)
    ## regressor
    b_z = pm.HalfNormal(name=&quot;b_z&quot;, sigma=2)
    ## standard deviation of the normal likelihood
    sigma = pm.HalfNormal(name=&quot;sigma&quot;, sigma=0.5)
    # degrees of freedom of the t distribution
    nu = pm.Gamma(name=&quot;nu&quot;, alpha=25, beta=2)

    # --- model parametrization ---
    trend = pm.Deterministic(name=&quot;trend&quot;, var=a + b_trend * t, dims=&quot;date&quot;)
    seasonality = pm.Deterministic(
        name=&quot;seasonality&quot;, var=pm.math.dot(fourier_features, b_fourier), dims=&quot;date&quot;
    )
    z_effect = pm.Deterministic(name=&quot;z_effect&quot;, var=b_z * z_scaled_, dims=&quot;date&quot;)
    mu = pm.Deterministic(name=&quot;mu&quot;, var=trend + seasonality + z_effect, dims=&quot;date&quot;)

    # --- likelihood ---
    pm.StudentT(name=&quot;likelihood&quot;, nu=nu, mu=mu, sigma=sigma, observed=y_scaled, dims=&quot;date&quot;)

    # --- prior samples ---
    base_model_prior_predictive = pm.sample_prior_predictive()

pm.model_to_graphviz(model=base_model)</code></pre>
<center>
<img src="../images/pymc_mmm_files/pymc_mmm_26_1.svg" style="width: 1000px;"/>
</center>
<ul>
<li>Prior Predictive Samples</li>
</ul>
<p>Let us start by sampling from the model before looking into the data:</p>
<pre class="python"><code># useful way to color the distribution
palette = &quot;viridis_r&quot;
cmap = plt.get_cmap(palette)
percs = np.linspace(51, 99, 100)
colors = (percs - np.min(percs)) / (np.max(percs) - np.min(percs))


fig, ax = plt.subplots()

for i, p in enumerate(percs[::-1]):
    upper = np.percentile(base_model_prior_predictive.prior_predictive[&quot;likelihood&quot;], p, axis=1)
    lower = np.percentile(
        base_model_prior_predictive.prior_predictive[&quot;likelihood&quot;], 100 - p, axis=1
    )
    color_val = colors[i]
    ax.fill_between(
        x=date,
        y1=upper.flatten(),
        y2=lower.flatten(),
        color=cmap(color_val),
        alpha=0.1,
    )

sns.lineplot(x=date, y=y_scaled, color=&quot;black&quot;, label=&quot;target (scaled)&quot;, ax=ax)
ax.legend()
ax.set(title=&quot;Base Model - Prior Predictive Samples&quot;);</code></pre>
<center>
<img src="../images/pymc_mmm_files/pymc_mmm_28_0.png" style="width: 1000px;"/>
</center>
<p>The priors do constrain the range of the generated time series. Nevertheless, they are not too restrictive.</p>
<p><strong>Remark:</strong> Note that the prior predictive shows the possibility of having <em>negative sales</em> which makes no sense. A common approach is to model the logarithm of the sales instead. We do not do this here for the sake of simplicity (specially when interpreting the results).</p>
<ul>
<li>Fit Model</li>
</ul>
<pre class="python"><code>with base_model:
    base_model_trace = pm.sampling_jax.sample_numpyro_nuts(
        draws=6000,
        chains=4,
        idata_kwargs={&quot;log_likelihood&quot;: True},
    )
    base_model_posterior_predictive = pm.sample_posterior_predictive(
        trace=base_model_trace
    )</code></pre>
<p><strong>Remark:</strong> We add the argument <code>idata_kwargs={"log_likelihood": True}</code> since we want to compare various models using <a href="https://python.arviz.org/en/latest/api/generated/arviz.compare.html"><code>az.compare</code></a>. See <a href="https://github.com/arviz-devs/arviz/issues/2155">this issue</a> for more details.</p>
<ul>
<li>Model Diagnostics</li>
</ul>
<pre class="python"><code>az.summary(
    data=base_model_trace,
    var_names=[&quot;a&quot;, &quot;b_trend&quot;, &quot;b_z&quot;, &quot;sigma&quot;, &quot;nu&quot;],
)</code></pre>
<center>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
mean
</th>
<th>
sd
</th>
<th>
hdi_3%
</th>
<th>
hdi_97%
</th>
<th>
mcse_mean
</th>
<th>
mcse_sd
</th>
<th>
ess_bulk
</th>
<th>
ess_tail
</th>
<th>
r_hat
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
a
</th>
<td>
0.788
</td>
<td>
0.003
</td>
<td>
0.782
</td>
<td>
0.794
</td>
<td>
0.000
</td>
<td>
0.000
</td>
<td>
10981.0
</td>
<td>
14592.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
b_trend
</th>
<td>
0.119
</td>
<td>
0.005
</td>
<td>
0.110
</td>
<td>
0.127
</td>
<td>
0.000
</td>
<td>
0.000
</td>
<td>
12503.0
</td>
<td>
14556.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
b_z
</th>
<td>
0.049
</td>
<td>
0.004
</td>
<td>
0.042
</td>
<td>
0.056
</td>
<td>
0.000
</td>
<td>
0.000
</td>
<td>
12070.0
</td>
<td>
13574.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
sigma
</th>
<td>
0.016
</td>
<td>
0.001
</td>
<td>
0.014
</td>
<td>
0.018
</td>
<td>
0.000
</td>
<td>
0.000
</td>
<td>
11428.0
</td>
<td>
12666.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
nu
</th>
<td>
12.470
</td>
<td>
2.395
</td>
<td>
8.091
</td>
<td>
16.988
</td>
<td>
0.022
</td>
<td>
0.016
</td>
<td>
11155.0
</td>
<td>
13065.0
</td>
<td>
1.0
</td>
</tr>
</tbody>
</table>
</div>
</center>
<pre class="python"><code>axes = az.plot_trace(
    data=base_model_trace,
    var_names=[&quot;a&quot;, &quot;b_trend&quot;, &quot;b_fourier&quot;, &quot;b_z&quot;, &quot;sigma&quot;, &quot;nu&quot;],
    compact=True,
    backend_kwargs={
        &quot;figsize&quot;: (12, 9),
        &quot;layout&quot;: &quot;constrained&quot;
    },
)
fig = axes[0][0].get_figure()
fig.suptitle(&quot;Base Model - Trace&quot;);</code></pre>
<center>
<img src="../images/pymc_mmm_files/pymc_mmm_35_0.png" style="width: 1000px;"/>
</center>
<pre class="python"><code>fig, ax = plt.subplots(figsize=(6, 4))
az.plot_forest(
    data=base_model_trace,
    var_names=[&quot;a&quot;, &quot;b_trend&quot;, &quot;b_z&quot;, &quot;sigma&quot;],
    combined=True,
    ax=ax
)
ax.set(
    title=&quot;Base Model: 94.0% HDI&quot;,
    xscale=&quot;log&quot;
);</code></pre>
<center>
<img src="../images/pymc_mmm_files/pymc_mmm_36_0.png" style="width: 700px;"/>
</center>
<p>Overall, the model looks ok!</p>
<ul>
<li>Posterior Predictive Samples</li>
</ul>
<pre class="python"><code>posterior_predictive_likelihood = az.extract(
    data=base_model_posterior_predictive,
    group=&quot;posterior_predictive&quot;,
    var_names=&quot;likelihood&quot;,
)

posterior_predictive_likelihood_inv = endog_scaler.inverse_transform(
    X=posterior_predictive_likelihood
)

fig, ax = plt.subplots()

for i, p in enumerate(percs[::-1]):
    upper = np.percentile(posterior_predictive_likelihood_inv, p, axis=1)
    lower = np.percentile(posterior_predictive_likelihood_inv, 100 - p, axis=1)
    color_val = colors[i]
    ax.fill_between(
        x=date,
        y1=upper,
        y2=lower,
        color=cmap(color_val),
        alpha=0.1,
    )

sns.lineplot(
    x=date,
    y=posterior_predictive_likelihood_inv.mean(axis=1),
    color=&quot;C2&quot;,
    label=&quot;posterior predictive mean&quot;,
    ax=ax,
)
sns.lineplot(
    x=date,
    y=y,
    color=&quot;black&quot;,
    label=&quot;target (scaled)&quot;,
    ax=ax,
)
ax.legend(loc=&quot;upper left&quot;)
ax.set(title=&quot;Base Model - Posterior Predictive Samples&quot;);</code></pre>
<center>
<img src="../images/pymc_mmm_files/pymc_mmm_39_0.png" style="width: 1000px;"/>
</center>
<p>The base model does capture the trend and seasonality of the data. We can now plot the model components:</p>
<pre class="python"><code># compute HDI for all the model parameters
model_hdi = az.hdi(ary=base_model_trace)

fig, ax = plt.subplots()

for i, var_effect in enumerate([&quot;z_effect&quot;, &quot;trend&quot;, &quot;seasonality&quot;]):
    ax.fill_between(
        x=date,
        y1=model_hdi[var_effect][:, 0],
        y2=model_hdi[var_effect][:, 1],
        color=f&quot;C{i}&quot;,
        alpha=0.3,
        label=f&quot;$94\%$ HDI ({var_effect})&quot;,
    )
    sns.lineplot(
        x=date,
        y=base_model_trace.posterior[var_effect]
        .stack(sample=(&quot;chain&quot;, &quot;draw&quot;))
        .mean(axis=1),
        color=f&quot;C{i}&quot;,
    )

sns.lineplot(x=date, y=y_scaled, color=&quot;black&quot;, alpha=1.0, label=&quot;target (scaled)&quot;, ax=ax)
ax.legend(title=&quot;components&quot;, loc=&quot;center left&quot;, bbox_to_anchor=(1, 0.5))
ax.set(title=&quot;Base Model Components&quot;, ylabel=&quot;target (scaled)&quot;);</code></pre>
<center>
<img src="../images/pymc_mmm_files/pymc_mmm_41_0.png" style="width: 1000px;"/>
</center>
<ul>
<li>Estimated <code>z_effect</code></li>
</ul>
<p>Finally, let us look at the estimated effect of <code>z</code> on <code>y</code>. Let’s start by looking into the development over time.</p>
<pre class="python"><code>z_effect_posterior_samples = xr.apply_ufunc(
    lambda x: endog_scaler.inverse_transform(X=x.reshape(1, -1)),
    base_model_trace.posterior[&quot;z_effect&quot;],
    input_core_dims=[[&quot;date&quot;]],
    output_core_dims=[[&quot;date&quot;]],
    vectorize=True,
)

z_effect_hdi = az.hdi(ary=z_effect_posterior_samples)[&quot;z_effect&quot;]

fig, ax = plt.subplots()
ax.fill_between(
    x=date,
    y1=z_effect_hdi[:, 0],
    y2=z_effect_hdi[:, 1],
    color=&quot;C0&quot;,
    alpha=0.5,
    label=&quot;z_effect 94% HDI&quot;,
)
ax.axhline(
    y=z_effect_posterior_samples.mean(),
    color=&quot;C0&quot;,
    linestyle=&quot;--&quot;,
    label=f&quot;posterior mean {z_effect_posterior_samples.mean().values: 0.3f}&quot;,
)
sns.lineplot(x=&quot;date&quot;, y=&quot;z_effect&quot;, color=&quot;C3&quot;, data=data_df, label=&quot;z_effect&quot;, ax=ax)
ax.legend(loc=&quot;upper right&quot;)
ax.set(title=&quot;Media Cost Effect on Sales Estimation - Base Model&quot;);</code></pre>
<center>
<img src="../images/pymc_mmm_files/pymc_mmm_43_0.png" style="width: 1000px;"/>
</center>
<p>We clearly see that the effect of <code>z</code> is a linear function of <code>z</code> and does not depend on the time, as expected from the model specification. It is interesting to see that the variance of the estimated effect is similar to the real effect of the latest observations.</p>
<p>Next, we simply plot the estimated against the true values.</p>
<pre class="python"><code>fig, ax = plt.subplots()

az.plot_hdi(
    x=z,
    y=z_effect_posterior_samples,
    color=&quot;C0&quot;,
    fill_kwargs={&quot;alpha&quot;: 0.2, &quot;label&quot;: &quot;z_effect 94% HDI&quot;},
    ax=ax,
)
sns.scatterplot(
    x=&quot;z&quot;,
    y=&quot;z_effect_pred_mean&quot;,
    color=&quot;C0&quot;,
    size=&quot;index&quot;,
    label=&quot;z_effect (pred mean)&quot;,
    data=data_df.assign(
        z_effect_pred_mean=z_effect_posterior_samples.mean(dim=(&quot;chain&quot;, &quot;draw&quot;))
    ),
    ax=ax,
)
sns.scatterplot(
    x=&quot;z&quot;,
    y=&quot;z_effect&quot;,
    color=&quot;C3&quot;,
    size=&quot;index&quot;,
    label=&quot;z_effect (true)&quot;,
    data=data_df,
    ax=ax,
)
h, l = ax.get_legend_handles_labels()
ax.legend(handles=h[:9], labels=l[:9], loc=&quot;center left&quot;, bbox_to_anchor=(1, 0.5))
ax.set(title=&quot;Base  Model - Estimated Effect&quot;);</code></pre>
<center>
<img src="../images/pymc_mmm_files/pymc_mmm_45_0.png" style="width: 1000px;"/>
</center>
<p>To encode the time component we map the size of the points to the <code>index</code>, which is a global time-component (number of weeks since the first observation). Note that the fitted values do not seem to match the data. This model is too simple to capture the non-linear interactions.</p>
<div id="roas-and-mroas-1" class="section level4">
<h4>ROAS and mROAS</h4>
<p>Estimating the effect as above is very important. Nevertheless, one is mainly interested in certain return on investment metrics. Here we describe how to compute two common ones: ROAS (Return on Ad Spend) and mROAS (marginal ROAS) as described in Section 4.1 in the paper <a href="https://static.googleusercontent.com/media/research.google.com/en//pubs/archive/46001.pdf">Bayesian Methods for Media Mix Modeling with Carryover and
Shape Effects</a>.</p>
<ul>
<li><strong>ROAS</strong></li>
</ul>
<p>From the reference above:</p>
<blockquote>
<p><em>ROAS is the change in revenue (or sales) per dollar spent on the medium; it is usually calculated by setting spend of the medium to zero in the selected time period and comparing the predicted revenue against that of the current media spend.</em></p>
</blockquote>
<p>We can express this mathematically as:</p>
<p><span class="math display">\[
ROAS = \frac{\sum^{n}_{t=0}\hat{f}(z_{t}, \xi_{t}) - \hat{f}(0, \xi_{t})}{\sum^{n}_{t=0} z_{t}}
\]</span></p>
<p>where <span class="math inline">\(\hat{f}(z_{t}, \xi_{t})\)</span> represents the posterior predictive distribution (and <span class="math inline">\(\xi_t\)</span> additional control variables) and <span class="math inline">\(\hat{f}(0, \xi_{t})\)</span> represents the posterior predictive distribution when setting <span class="math inline">\(z=0\)</span>. There is a caveat with this formula however. One has to be careful about the pre/during/post computation periods because of the carryover effects. Here, for simplicity will do it for the whole time-range. For more details, please check the reference above.</p>
<p>In this specific example where we have an additive model we can compute the ROAS’ denominator as:</p>
<p><span class="math display">\[y -  (y - z_{\text{effect}}) = z_{\text{effect}}\]</span></p>
<p>Note that this is not true for a multiplicative model (when for example you are applying log transform to the target variable).</p>
<p><strong>Remark:</strong> Actually, the ROAS denominator is not precisely <span class="math inline">\(y - (y - z_{\text{effect}}) = z_{\text{effect}}\)</span> since we still need to pass this through the normal likelihood function to properly compute the predictions of the whole model. Still, this will serve as a way to verify the results as we will be comparing the expected values.</p>
<pre class="python"><code># true roas for z
roas_true = data_df[&quot;z_effect&quot;].sum() / data_df[&quot;z&quot;].sum()

roas_true</code></pre>
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.06620312360383751</span>
</pre>
<p>Now we estimate it from the data and the model.</p>
<pre class="python"><code>base_model_trace_roas = base_model_trace.copy()

with base_model:
    pm.set_data(new_data={&quot;z_scaled&quot;: np.zeros_like(a=z_scaled)})
    base_model_trace_roas.extend(
        other=pm.sample_posterior_predictive(trace=base_model_trace_roas, var_names=[&quot;likelihood&quot;])
    )</code></pre>
<pre class="python"><code>base_roas_numerator = (
    endog_scaler.inverse_transform(
        X=az.extract(
            data=base_model_posterior_predictive,
            group=&quot;posterior_predictive&quot;,
            var_names=[&quot;likelihood&quot;],
        )
    )
    - endog_scaler.inverse_transform(
        X=az.extract(
            data=base_model_trace_roas,
            group=&quot;posterior_predictive&quot;,
            var_names=[&quot;likelihood&quot;],
        )
    )
).sum(axis=0)

roas_denominator = z.sum()

base_roas = base_roas_numerator / roas_denominator</code></pre>
<pre class="python"><code>base_roas_mean = base_roas.mean()
base_roas_hdi = az.hdi(ary=base_roas)

g = sns.displot(x=base_roas, kde=True, height=5, aspect=1.5)
ax = g.axes.flatten()[0]
ax.axvline(
    x=base_roas_mean, color=&quot;C0&quot;, linestyle=&quot;--&quot;, label=f&quot;mean = {base_roas_mean: 0.3f}&quot;
)
ax.axvline(
    x=base_roas_hdi[0],
    color=&quot;C1&quot;,
    linestyle=&quot;--&quot;,
    label=f&quot;HDI_lwr = {base_roas_hdi[0]: 0.3f}&quot;,
)
ax.axvline(
    x=base_roas_hdi[1],
    color=&quot;C2&quot;,
    linestyle=&quot;--&quot;,
    label=f&quot;HDI_upr = {base_roas_hdi[1]: 0.3f}&quot;,
)
ax.axvline(x=roas_true, color=&quot;black&quot;, linestyle=&quot;--&quot;, label=f&quot;true = {roas_true: 0.3f}&quot;)
ax.legend(loc=&quot;center left&quot;, bbox_to_anchor=(1, 0.5))
ax.set(title=&quot;Base Model ROAS&quot;);</code></pre>
<center>
<img src="../images/pymc_mmm_files/pymc_mmm_54_0.png" style="width: 1000px;"/>
</center>
<p>Observe that, as for the base model is <em>under estimating the effect</em> of <span class="math inline">\(z\)</span> on <span class="math inline">\(y\)</span>, this translates into smaller predicted ROAS as compared to the true ROAS. This is consistent with the plot above.</p>
<ul>
<li><strong>mROAS</strong></li>
</ul>
<blockquote>
<p><em>mROAS for the m-th medium is the additional revenue generated by one-unit increase in spend, usually from the current spent level.</em></p>
</blockquote>
<p>Let’s compute t by evaluating the a <span class="math inline">\(10\%\)</span> increase in media spend <span class="math inline">\(z\)</span> (we will leave the computation of the true mROAS for the next example).</p>
<pre class="python"><code>eta: float = 0.10

base_model_trace_mroas = base_model_trace.copy()

with base_model:
    pm.set_data(new_data={&quot;z_scaled&quot;: (1 + eta) * z_scaled})
    base_model_trace_mroas.extend(
        other=pm.sample_posterior_predictive(trace=base_model_trace_mroas, var_names=[&quot;likelihood&quot;])
    )</code></pre>
<pre class="python"><code>base_mroas_numerator = (
    endog_scaler.inverse_transform(
        X=az.extract(
            data=base_model_trace_mroas,
            group=&quot;posterior_predictive&quot;,
            var_names=[&quot;likelihood&quot;],
        )
    )
    - endog_scaler.inverse_transform(
        X=az.extract(
            data=base_model_posterior_predictive,
            group=&quot;posterior_predictive&quot;,
            var_names=[&quot;likelihood&quot;],
        )
    )
).sum(axis=0)

mroas_denominator = eta * z.sum()

base_mroas = base_mroas_numerator / mroas_denominator
</code></pre>
<pre class="python"><code>base_mroas_mean = base_mroas.mean()
base_mroas_hdi = az.hdi(ary=base_mroas)

g = sns.displot(x=base_mroas, kde=True, height=5, aspect=1.5)
ax = g.axes.flatten()[0]
ax.axvline(
    x=base_mroas_mean, color=&quot;C0&quot;, linestyle=&quot;--&quot;, label=f&quot;mean = {base_mroas_mean: 0.3f}&quot;
)
ax.axvline(
    x=base_mroas_hdi[0],
    color=&quot;C1&quot;,
    linestyle=&quot;--&quot;,
    label=f&quot;HDI_lwr = {base_mroas_hdi[0]: 0.3f}&quot;,
)
ax.axvline(
    x=base_mroas_hdi[1],
    color=&quot;C2&quot;,
    linestyle=&quot;--&quot;,
    label=f&quot;HDI_upr = {base_roas_hdi[1]: 0.3f}&quot;,
)
ax.axvline(x=0.0, color=&quot;gray&quot;, linestyle=&quot;--&quot;, label=&quot;zero&quot;)
ax.legend(loc=&quot;center left&quot;, bbox_to_anchor=(1, 0.5))
ax.set(title=f&quot;Base Model MROAS ({eta:.0%} increase)&quot;);</code></pre>
<center>
<img src="../images/pymc_mmm_files/pymc_mmm_59_0.png" style="width: 1000px;"/>
</center>
<hr />
</div>
</div>
<div id="adstock-saturation-model" class="section level3">
<h3>Adstock-Saturation Model</h3>
<ul>
<li>Features</li>
</ul>
<p>For the second model we need to express the (geometric) adstock and saturation transformations as tensor operations:</p>
<pre class="python"><code>def geometric_adstock(x, alpha: float = 0.0, l_max: int = 12):
    &quot;&quot;&quot;Geometric adstock transformation.&quot;&quot;&quot;
    cycles = [
        pt.concatenate(
            [pt.zeros(i), x[: x.shape[0] - i]]
        )
        for i in range(l_max)
    ]
    x_cycle = pt.stack(cycles)
    w = pt.as_tensor_variable([pt.power(alpha, i) for i in range(l_max)])
    return pt.dot(w, x_cycle)


def logistic_saturation(x, lam: float = 0.5):
    &quot;&quot;&quot;Logistic saturation transformation.&quot;&quot;&quot;
    return (1 - pt.exp(-lam * x)) / (1 + pt.exp(-lam * x))</code></pre>
<ul>
<li>Model Specification</li>
</ul>
<p>As we want <span class="math inline">\(\alpha\)</span> to be in the interval <span class="math inline">\((0, 1)\)</span> we use a Beta distribution. Moreover, we choose <span class="math inline">\(\alpha \sim \text{Beta}(1, 1) = \text{Uniform}(0, 1)\)</span> as prior. For <span class="math inline">\(\lambda\)</span> we use a <span class="math inline">\(\text{Gamma}\)</span> distribution as this parameters has to be positive.</p>
<pre class="python"><code>with pm.Model(coords=coords) as adstock_saturation_model:
    # --- data containers ---
    z_scaled_ = pm.MutableData(name=&quot;z_scaled&quot;, value=z_scaled, dims=&quot;date&quot;)
    
    # --- priors ---
    ## intercept
    a = pm.Normal(name=&quot;a&quot;, mu=0, sigma=4)
    ## trend
    b_trend = pm.Normal(name=&quot;b_trend&quot;, mu=0, sigma=2)
    ## seasonality
    b_fourier = pm.Laplace(name=&quot;b_fourier&quot;, mu=0, b=2, dims=&quot;fourier_mode&quot;)
    ## adstock effect
    alpha = pm.Beta(name=&quot;alpha&quot;, alpha=1, beta=1)
    ## saturation effect
    lam = pm.Gamma(name=&quot;lam&quot;, alpha=3, beta=1)
    ## regressor
    b_z = pm.HalfNormal(name=&quot;b_z&quot;, sigma=2)
    ## standard deviation of the normal likelihood
    sigma = pm.HalfNormal(name=&quot;sigma&quot;, sigma=0.5)
    # degrees of freedom of the t distribution
    nu = pm.Gamma(name=&quot;nu&quot;, alpha=25, beta=2)

    # --- model parametrization ---
    trend = pm.Deterministic(&quot;trend&quot;, a + b_trend * t, dims=&quot;date&quot;)
    seasonality = pm.Deterministic(
        name=&quot;seasonality&quot;, var=pm.math.dot(fourier_features, b_fourier), dims=&quot;date&quot;
    )
    z_adstock = pm.Deterministic(
        name=&quot;z_adstock&quot;, var=geometric_adstock(x=z_scaled_, alpha=alpha, l_max=12), dims=&quot;date&quot;
    )
    z_adstock_saturated = pm.Deterministic(
        name=&quot;z_adstock_saturated&quot;,
        var=logistic_saturation(x=z_adstock, lam=lam),
        dims=&quot;date&quot;,
    )
    z_effect = pm.Deterministic(
        name=&quot;z_effect&quot;, var=b_z * z_adstock_saturated, dims=&quot;date&quot;
    )
    mu = pm.Deterministic(name=&quot;mu&quot;, var=trend + seasonality + z_effect, dims=&quot;date&quot;)

    # --- likelihood ---
    pm.StudentT(name=&quot;likelihood&quot;, nu=nu, mu=mu, sigma=sigma, observed=y_scaled, dims=&quot;date&quot;)

    # --- prior samples
    adstock_saturation_model_prior_predictive = pm.sample_prior_predictive()

pm.model_to_graphviz(model=adstock_saturation_model)</code></pre>
<center>
<img src="../images/pymc_mmm_files/pymc_mmm_63_1.svg" style="width: 1000px;"/>
</center>
<ul>
<li>Prior Predictive Samples</li>
</ul>
<pre class="python"><code>fig, ax = plt.subplots()

for i, p in enumerate(percs[::-1]):
    upper = np.percentile(
        adstock_saturation_model_prior_predictive.prior_predictive[&quot;likelihood&quot;],
        p,
        axis=1,
    )
    lower = np.percentile(
        adstock_saturation_model_prior_predictive.prior_predictive[&quot;likelihood&quot;], 100 - p, axis=1
    )
    color_val = colors[i]
    ax.fill_between(
        x=date,
        y1=upper.flatten(),
        y2=lower.flatten(),
        color=cmap(color_val),
        alpha=0.1,
    )

sns.lineplot(x=date, y=y_scaled, color=&quot;black&quot;, label=&quot;target (scaled)&quot;, ax=ax)
ax.legend()
ax.set(title=&quot;Adstock Saturation Model - Prior Predictive&quot;);</code></pre>
<center>
<img src="../images/pymc_mmm_files/pymc_mmm_65_0.png" style="width: 1000px;"/>
</center>
<ul>
<li>Fit Model</li>
</ul>
<pre class="python"><code>with adstock_saturation_model:
    adstock_saturation_model_trace = pm.sampling_jax.sample_numpyro_nuts(
        draws=6000,
        chains=4,
        idata_kwargs={&quot;log_likelihood&quot;: True},
    )
    adstock_saturation_model_posterior_predictive = pm.sample_posterior_predictive(
        trace=adstock_saturation_model_trace,
    )</code></pre>
<ul>
<li>Model Diagnostics</li>
</ul>
<pre class="python"><code>az.summary(
    data=adstock_saturation_model_trace,
    var_names=[&quot;a&quot;, &quot;b_trend&quot;, &quot;b_z&quot;, &quot;alpha&quot;, &quot;lam&quot;, &quot;sigma&quot;, &quot;nu&quot;]
)</code></pre>
<center>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
mean
</th>
<th>
sd
</th>
<th>
hdi_3%
</th>
<th>
hdi_97%
</th>
<th>
mcse_mean
</th>
<th>
mcse_sd
</th>
<th>
ess_bulk
</th>
<th>
ess_tail
</th>
<th>
r_hat
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
a
</th>
<td>
0.752
</td>
<td>
0.007
</td>
<td>
0.740
</td>
<td>
0.764
</td>
<td>
0.000
</td>
<td>
0.000
</td>
<td>
6114.0
</td>
<td>
9188.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
b_trend
</th>
<td>
0.117
</td>
<td>
0.003
</td>
<td>
0.111
</td>
<td>
0.124
</td>
<td>
0.000
</td>
<td>
0.000
</td>
<td>
16428.0
</td>
<td>
16438.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
b_z
</th>
<td>
0.119
</td>
<td>
0.025
</td>
<td>
0.091
</td>
<td>
0.146
</td>
<td>
0.001
</td>
<td>
0.000
</td>
<td>
4275.0
</td>
<td>
3289.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
alpha
</th>
<td>
0.531
</td>
<td>
0.035
</td>
<td>
0.465
</td>
<td>
0.595
</td>
<td>
0.000
</td>
<td>
0.000
</td>
<td>
7054.0
</td>
<td>
10414.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
lam
</th>
<td>
1.332
</td>
<td>
0.286
</td>
<td>
0.787
</td>
<td>
1.866
</td>
<td>
0.005
</td>
<td>
0.003
</td>
<td>
4289.0
</td>
<td>
3146.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
sigma
</th>
<td>
0.012
</td>
<td>
0.001
</td>
<td>
0.010
</td>
<td>
0.013
</td>
<td>
0.000
</td>
<td>
0.000
</td>
<td>
8559.0
</td>
<td>
12215.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
nu
</th>
<td>
13.300
</td>
<td>
2.461
</td>
<td>
8.934
</td>
<td>
18.128
</td>
<td>
0.026
</td>
<td>
0.018
</td>
<td>
8892.0
</td>
<td>
10523.0
</td>
<td>
1.0
</td>
</tr>
</tbody>
</table>
</div>
</center>
<p>We will see later that the values of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\lambda\)</span> are very close to the true ones (<span class="math inline">\(0.5\)</span> and <span class="math inline">\(0.15\)</span> respectively) up to a scale defined by the channel scaler. Moreover, the true values (up to this scale) are included in the posterior distributions <span class="math inline">\(94\%\)</span> HDI.</p>
<pre class="python"><code>axes = az.plot_trace(
    data=adstock_saturation_model_trace,
    var_names=[&quot;a&quot;, &quot;b_trend&quot;, &quot;b_fourier&quot;, &quot;b_z&quot;, &quot;alpha&quot;, &quot;lam&quot;, &quot;sigma&quot;, &quot;nu&quot;],
    compact=True,
    backend_kwargs={
        &quot;figsize&quot;: (12, 12),
        &quot;layout&quot;: &quot;constrained&quot;
    },
)
fig = axes[0][0].get_figure()
fig.suptitle(&quot;Adstock-Saturation Model - Trace&quot;);</code></pre>
<center>
<img src="../images/pymc_mmm_files/pymc_mmm_71_0.png" style="width: 1000px;"/>
</center>
<pre class="python"><code>fig, ax = plt.subplots(figsize=(6, 4))
az.plot_forest(
    data=adstock_saturation_model_trace,
    var_names=[&quot;a&quot;, &quot;b_trend&quot;,  &quot;b_z&quot;, &quot;alpha&quot;, &quot;lam&quot;, &quot;sigma&quot;, &quot;nu&quot;],
    combined=True,
    ax=ax
)
ax.set(
    title=&quot;Adstock-Saturation Model: 94.0% HDI&quot;,
    xscale=&quot;log&quot;
);</code></pre>
<center>
<img src="../images/pymc_mmm_files/pymc_mmm_72_0.png" style="width: 700px;"/>
</center>
<ul>
<li>Posterior Predictive Samples</li>
</ul>
<pre class="python"><code>posterior_predictive_likelihood = az.extract(
    data=adstock_saturation_model_posterior_predictive,
    group=&quot;posterior_predictive&quot;,
    var_names=&quot;likelihood&quot;,
)

posterior_predictive_likelihood_inv = endog_scaler.inverse_transform(
    X=posterior_predictive_likelihood
)

fig, ax = plt.subplots()

for i, p in enumerate(percs[::-1]):
    upper = np.percentile(posterior_predictive_likelihood_inv, p, axis=1)
    lower = np.percentile(posterior_predictive_likelihood_inv, 100 - p, axis=1)
    color_val = colors[i]
    ax.fill_between(
        x=date,
        y1=upper,
        y2=lower,
        color=cmap(color_val),
        alpha=0.1,
    )

sns.lineplot(
    x=date,
    y=posterior_predictive_likelihood_inv.mean(axis=1),
    color=&quot;C2&quot;,
    label=&quot;posterior predictive mean&quot;,
    ax=ax,
)
sns.lineplot(
    x=date,
    y=y,
    color=&quot;black&quot;,
    label=&quot;target&quot;,
    ax=ax,
)
ax.legend(loc=&quot;upper left&quot;)
ax.set(title=&quot;Adstock-Saturation Model - Posterior Predictive Samples&quot;);</code></pre>
<center>
<img src="../images/pymc_mmm_files/pymc_mmm_74_0.png" style="width: 1000px;"/>
</center>
<pre class="python"><code># compute HDI for all the model parameters
model_hdi = az.hdi(ary=adstock_saturation_model_trace)

fig, ax = plt.subplots()

for i, var_effect in enumerate([&quot;z_effect&quot;, &quot;trend&quot;, &quot;seasonality&quot;]):
    ax.fill_between(
        x=date,
        y1=model_hdi[var_effect][:, 0],
        y2=model_hdi[var_effect][:, 1],
        color=f&quot;C{i}&quot;,
        alpha=0.3,
        label=f&quot;$94\%$ HDI ({var_effect})&quot;,
    )
    sns.lineplot(
        x=date,
        y=adstock_saturation_model_trace.posterior[var_effect]
        .stack(sample=(&quot;chain&quot;, &quot;draw&quot;))
        .mean(axis=1),
        color=f&quot;C{i}&quot;,
    )

sns.lineplot(x=date, y=y_scaled, color=&quot;black&quot;, alpha=1.0, label=&quot;target (scaled)&quot;, ax=ax)
ax.legend(title=&quot;components&quot;, loc=&quot;center left&quot;, bbox_to_anchor=(1, 0.5))
ax.set(title=&quot;Adstock-Saturation Model Components&quot;, ylabel=&quot;target (scaled)&quot;);</code></pre>
<center>
<img src="../images/pymc_mmm_files/pymc_mmm_75_0.png" style="width: 1000px;"/>
</center>
<ul>
<li>Estimated <code>z_effect</code></li>
</ul>
<pre class="python"><code>z_effect_posterior_samples = xr.apply_ufunc(
    lambda x: endog_scaler.inverse_transform(X=x.reshape(1, -1)),
    adstock_saturation_model_trace.posterior[&quot;z_effect&quot;],
    input_core_dims=[[&quot;date&quot;]],
    output_core_dims=[[&quot;date&quot;]],
    vectorize=True,
)

z_effect_hdi = az.hdi(ary=z_effect_posterior_samples)[&quot;z_effect&quot;]
fig, ax = plt.subplots()
ax.fill_between(
    x=date,
    y1=z_effect_hdi[:, 0],
    y2=z_effect_hdi[:, 1],
    color=&quot;C0&quot;,
    alpha=0.5,
    label=&quot;z_effect 94% HDI&quot;,
)
ax.axhline(
    y=z_effect_posterior_samples.mean(),
    color=&quot;C0&quot;,
    linestyle=&quot;--&quot;,
    label=f&quot;posterior mean {z_effect_posterior_samples.mean().values: 0.3f}&quot;,
)
sns.lineplot(x=&quot;date&quot;, y=&quot;z_effect&quot;, color=&quot;C3&quot;, data=data_df, label=&quot;z_effect&quot;, ax=ax)
ax.legend(loc=&quot;upper right&quot;)
ax.set(title=&quot;Media Cost Effect on Sales Estimation - Adstock-Saturation&quot;);</code></pre>
<center>
<img src="../images/pymc_mmm_files/pymc_mmm_77_0.png" style="width: 1000px;"/>
</center>
<p>This model captures more variance in the effect of <code>z</code> than the base model. This shows that the adstock and saturation transformations do make the difference. Note however that the diminishing returns effect is not present in this model, as the regression coefficient is not time-varying.</p>
<p>We continue by looking into the estimated against the true values for this adstock-saturation model. We would expect to find a non-linear patter because of the composition of these two transformations.</p>
<pre class="python"><code>fig, ax = plt.subplots()

az.plot_hdi(
    x=z,
    y=z_effect_posterior_samples,
    color=&quot;C0&quot;,
    fill_kwargs={&quot;alpha&quot;: 0.2, &quot;label&quot;: &quot;z_effect 94% HDI&quot;},
    ax=ax,
)
sns.scatterplot(
    x=&quot;z&quot;,
    y=&quot;z_effect_pred_mean&quot;,
    color=&quot;C0&quot;,
    size=&quot;index&quot;,
    label=&quot;z_effect (pred mean)&quot;,
    data=data_df.assign(
        z_effect_pred_mean=z_effect_posterior_samples.mean(dim=(&quot;chain&quot;, &quot;draw&quot;))
    ),
    ax=ax,
)
sns.scatterplot(
    x=&quot;z&quot;,
    y=&quot;z_effect&quot;,
    color=&quot;C3&quot;,
    size=&quot;index&quot;,
    label=&quot;z_effect (true)&quot;,
    data=data_df,
    ax=ax,
)
h, l = ax.get_legend_handles_labels()
ax.legend(handles=h[:9], labels=l[:9], loc=&quot;center left&quot;, bbox_to_anchor=(1, 0.5))
ax.set(title=&quot;Adstock-Saturation  Model - Estimated Effect&quot;);</code></pre>
<center>
<img src="../images/pymc_mmm_files/pymc_mmm_79_0.png" style="width: 1000px;"/>
</center>
<p>As for the base-model, we encode the time component as the size of the points. Note that we indeed see a better fit and a non-linear pattern. For low values of <code>z</code> the effect seems constant whereas for larger values we see a non-linear pattern which saturates as expected.</p>
<p>We will deep-dive in the adstock and saturation transformations in the next example.</p>
<p><strong>Remark:</strong> One can easily vectorize the adstock transformation above to include various channels without using a for loop. Namely,</p>
<pre class="python"><code>def geometric_adstock_vectorized(x, alpha, l_max: int = 12):
    &quot;&quot;&quot;Vectorized geometric adstock transformation.&quot;&quot;&quot;
    cycles = [
        pt.concatenate(tensor_list=[pt.zeros(shape=x.shape)[:i], x[: x.shape[0] - i]])
        for i in range(l_max)
    ]
    x_cycle = pt.stack(cycles)
    x_cycle = pt.transpose(x=x_cycle, axes=[1, 2, 0])
    w = pt.as_tensor_variable([pt.power(alpha, i) for i in range(l_max)])
    w = pt.transpose(w)[None, ...]
    return pt.sum(pt.mul(x_cycle, w), axis=2)</code></pre>
<p>Here alpha is a tensor where each dimension corresponds to a channel. Moreover, one can normalize the weights as</p>
<pre class="python"><code>w / pt.sum(w, axis=2, keepdims=True)</code></pre>
<div id="roas-and-mroas-2" class="section level4">
<h4>ROAS and mROAS</h4>
<ul>
<li><strong>ROAS</strong></li>
</ul>
<pre class="python"><code>adstock_saturation_model_trace_roas = adstock_saturation_model_trace.copy()

with adstock_saturation_model:
    pm.set_data(new_data={&quot;z_scaled&quot;: np.zeros_like(a=z_scaled)})
    adstock_saturation_model_trace_roas.extend(
        other=pm.sample_posterior_predictive(trace=adstock_saturation_model_trace_roas, var_names=[&quot;likelihood&quot;])
    )</code></pre>
<pre class="python"><code>adstock_saturation_roas_numerator = (
    endog_scaler.inverse_transform(
        X=az.extract(
            data=adstock_saturation_model_posterior_predictive,
            group=&quot;posterior_predictive&quot;,
            var_names=[&quot;likelihood&quot;],
        )
    )
    - endog_scaler.inverse_transform(
        X=az.extract(
            data=adstock_saturation_model_trace_roas,
            group=&quot;posterior_predictive&quot;,
            var_names=[&quot;likelihood&quot;],
        )
    )
).sum(axis=0)

roas_denominator = z.sum()

adstock_saturation_roas = adstock_saturation_roas_numerator / roas_denominator</code></pre>
<pre class="python"><code>adstock_saturation_roas_mean = adstock_saturation_roas.mean()
adstock_saturation_roas_hdi = az.hdi(ary=adstock_saturation_roas)

g = sns.displot(x=adstock_saturation_roas, kde=True, height=5, aspect=1.5)
ax = g.axes.flatten()[0]
ax.axvline(
    x=adstock_saturation_roas_mean, color=&quot;C0&quot;, linestyle=&quot;--&quot;, label=f&quot;mean = {adstock_saturation_roas_mean: 0.3f}&quot;
)
ax.axvline(
    x=adstock_saturation_roas_hdi[0],
    color=&quot;C1&quot;,
    linestyle=&quot;--&quot;,
    label=f&quot;HDI_lwr = {adstock_saturation_roas_hdi[0]: 0.3f}&quot;,
)
ax.axvline(
    x=adstock_saturation_roas_hdi[1],
    color=&quot;C2&quot;,
    linestyle=&quot;--&quot;,
    label=f&quot;HDI_upr = {adstock_saturation_roas_hdi[1]: 0.3f}&quot;,
)
ax.axvline(x=roas_true, color=&quot;black&quot;, linestyle=&quot;--&quot;, label=f&quot;true = {roas_true: 0.3f}&quot;)
ax.legend(loc=&quot;center left&quot;, bbox_to_anchor=(1, 0.5))
ax.set(title=&quot;Adstock Saturation Model ROAS&quot;);</code></pre>
<center>
<img src="../images/pymc_mmm_files/pymc_mmm_88_0.png" style="width: 1000px;"/>
</center>
<p>In this case, as for the adstock saturation model is <em>over estimating the effect</em> of <span class="math inline">\(z\)</span> on <span class="math inline">\(y\)</span>, this translates into a <em>smaller</em> predicted ROAS as compared to the true ROAS.</p>
<ul>
<li><strong>mROAS</strong></li>
</ul>
<p>Let’s compute the true mROAS. We first need to estimate the effect of <span class="math inline">\(z\)</span> on <span class="math inline">\(y\)</span> by increasing it by <span class="math inline">\(10%\)</span> and then pushing it through the data transformations using the true values of the parameters (see the blog post (Media Effect Estimation with Orbit’s KTR Model)[<a href="https://juanitorduz.github.io/orbit_mmm/" class="uri">https://juanitorduz.github.io/orbit_mmm/</a>] to see the data generation process). Then we subtract the initial effect (again, note that this just works for additive models!).</p>
<pre class="python"><code>b_z_true = (np.arange(start=0.0, stop=1.0, step=1/n_obs) + 1) ** (-1.8)

z_effect_eta = b_z_true * logistic_saturation(
    x=geometric_adstock(x=(1 + eta) * z, alpha=0.5, l_max=12),
    lam=0.15
).eval()

mroas_true = (z_effect_eta - data_df[&quot;z_effect&quot;]).sum() / ( eta * z.sum())

mroas_true</code></pre>
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">0.045259246530653344</span>
</pre>
<p>Now, let use compute it from the model.</p>
<pre class="python"><code>eta: float = 0.10

adstock_saturation_model_trace_mroas = adstock_saturation_model_trace.copy()

with adstock_saturation_model:
    pm.set_data(new_data={&quot;z_scaled&quot;: (1 + eta) * z_scaled})
    adstock_saturation_model_trace_mroas.extend(
        other=pm.sample_posterior_predictive(trace=adstock_saturation_model_trace_mroas, var_names=[&quot;likelihood&quot;])
    )</code></pre>
<pre class="python"><code>adstock_saturation_mroas_numerator = (
    endog_scaler.inverse_transform(
        X=az.extract(
            data=adstock_saturation_model_trace_mroas,
            group=&quot;posterior_predictive&quot;,
            var_names=[&quot;likelihood&quot;],
        )
    )
    - endog_scaler.inverse_transform(
        X=az.extract(
            data=adstock_saturation_model_posterior_predictive,
            group=&quot;posterior_predictive&quot;,
            var_names=[&quot;likelihood&quot;],
        )
    )
).sum(axis=0)

mroas_denominator = eta * z.sum()

adstock_saturation_mroas = adstock_saturation_mroas_numerator / mroas_denominator</code></pre>
<pre class="python"><code>adstock_saturation_mroas_mean = adstock_saturation_mroas.mean()
adstock_saturation_mroas_hdi = az.hdi(ary=adstock_saturation_mroas)

g = sns.displot(x=adstock_saturation_mroas, kde=True, height=5, aspect=1.5)
ax = g.axes.flatten()[0]
ax.axvline(
    x=adstock_saturation_mroas_mean, color=&quot;C0&quot;, linestyle=&quot;--&quot;, label=f&quot;mean = {adstock_saturation_mroas_mean: 0.3f}&quot;
)
ax.axvline(
    x=adstock_saturation_mroas_hdi[0],
    color=&quot;C1&quot;,
    linestyle=&quot;--&quot;,
    label=f&quot;HDI_lwr = {adstock_saturation_mroas_hdi[0]: 0.3f}&quot;,
)
ax.axvline(
    x=adstock_saturation_mroas_hdi[1],
    color=&quot;C2&quot;,
    linestyle=&quot;--&quot;,
    label=f&quot;HDI_upr = {adstock_saturation_roas_hdi[1]: 0.3f}&quot;,
)
ax.axvline(x=mroas_true, color=&quot;black&quot;, linestyle=&quot;--&quot;, label=f&quot;true = {roas_true: 0.3f}&quot;)
ax.axvline(x=0.0, color=&quot;gray&quot;, linestyle=&quot;--&quot;, label=&quot;zero&quot;)
ax.legend(loc=&quot;center left&quot;, bbox_to_anchor=(1, 0.5))
ax.set(title=f&quot;Adstock Saturation Model MROAS ({eta:.0%} increase)&quot;);</code></pre>
<center>
<img src="../images/pymc_mmm_files/pymc_mmm_95_0.png" style="width: 1000px;"/>
</center>
<hr />
</div>
</div>
<div id="adstock-saturation-diminishing-returns-asdr-model" class="section level3">
<h3>Adstock-Saturation-Diminishing-Returns (ASDR) Model</h3>
<p>In this final model we add a time-varying coefficient for the adstock and saturation transformations (plus controlling for the trend and seasonality). Note that, we ensure the time varying coefficients are all positive by adding an <code>exp</code> transformation to the output of the Gaussian random walk.</p>
<ul>
<li>Model Specification</li>
</ul>
<pre class="python"><code>with pm.Model(coords=coords) as asdr_model:
    # --- data containers ---
    z_scaled_ = pm.MutableData(name=&quot;z_scaled&quot;, value=z_scaled, dims=&quot;date&quot;)

    # --- priors ---
    ## intercept
    a = pm.Normal(name=&quot;a&quot;, mu=0, sigma=4)
    ## trend
    b_trend = pm.Normal(name=&quot;b_trend&quot;, mu=0, sigma=2)
    ## seasonality
    b_fourier = pm.Laplace(name=&quot;b_fourier&quot;, mu=0, b=2, dims=&quot;fourier_mode&quot;)
    ## adstock effect
    alpha = pm.Beta(name=&quot;alpha&quot;, alpha=1, beta=1)
    ## saturation effect
    lam = pm.Gamma(name=&quot;lam&quot;, alpha=1, beta=1)
    ## gaussian random walk standard deviation
    sigma_slope = pm.HalfNormal(name=&quot;sigma_slope&quot;, sigma=0.05)
    ## standard deviation of the normal likelihood
    sigma = pm.HalfNormal(name=&quot;sigma&quot;, sigma=0.5)
    # degrees of freedom of the t distribution
    nu = pm.Gamma(name=&quot;nu&quot;, alpha=10, beta=1)

    # --- model parametrization ---
    trend = pm.Deterministic(name=&quot;trend&quot;, var=a + b_trend * t, dims=&quot;date&quot;)
    seasonality = pm.Deterministic(
        name=&quot;seasonality&quot;, var=pm.math.dot(fourier_features, b_fourier), dims=&quot;date&quot;
    )
    slopes = pm.GaussianRandomWalk(
        name=&quot;slopes&quot;,
        sigma=sigma_slope,
        init_dist=pymc.distributions.continuous.Normal.dist(
            name=&quot;init_dist&quot;, mu=0, sigma=2
        ),
        dims=&quot;date&quot;,
    )
    z_adstock = pm.Deterministic(
        name=&quot;z_adstock&quot;, var=geometric_adstock(x=z_scaled_, alpha=alpha, l_max=12), dims=&quot;date&quot;
    )
    z_adstock_saturated = pm.Deterministic(
        name=&quot;z_adstock_saturated&quot;,
        var=logistic_saturation(x=z_adstock, lam=lam),
        dims=&quot;date&quot;,
    )
    z_effect = pm.Deterministic(
        name=&quot;z_effect&quot;, var=pm.math.exp(slopes) * z_adstock_saturated, dims=&quot;date&quot;
    )
    mu = pm.Deterministic(name=&quot;mu&quot;, var=trend + seasonality + z_effect, dims=&quot;date&quot;)

    # --- likelihood ---
    pm.StudentT(name=&quot;likelihood&quot;, nu=nu, mu=mu, sigma=sigma, observed=y_scaled, dims=&quot;date&quot;)

    # --- prior samples ---
    asdr_model_prior_predictive = pm.sample_prior_predictive()

pm.model_to_graphviz(model=asdr_model)</code></pre>
<center>
<img src="../images/pymc_mmm_files/pymc_mmm_98_1.svg" style="width: 1000px;"/>
</center>
<pre class="python"><code>fig, ax = plt.subplots()

for i, p in enumerate(percs[::-1]):
    upper = np.percentile(
        asdr_model_prior_predictive.prior_predictive[&quot;likelihood&quot;],
        p,
        axis=1,
    )
    lower = np.percentile(
        asdr_model_prior_predictive.prior_predictive[&quot;likelihood&quot;], 100 - p, axis=1
    )
    color_val = colors[i]
    ax.fill_between(
        x=date,
        y1=upper.flatten(),
        y2=lower.flatten(),
        color=cmap(color_val),
        alpha=0.1,
    )

sns.lineplot(x=date, y=y_scaled, color=&quot;black&quot;, label=&quot;target (scaled)&quot;, ax=ax)
ax.legend()
ax.set(title=&quot;Adstock-Saturation-Diminishing-Returns Model - Prior Predictive Samples&quot;);</code></pre>
<center>
<img src="../images/pymc_mmm_files/pymc_mmm_99_0.png" style="width: 1000px;"/>
</center>
<ul>
<li>Model Fit</li>
</ul>
<pre class="python"><code>with asdr_model:
    asdr_model_trace = pm.sample(
        draws=6_000,
        chains=4,
        idata_kwargs={&quot;log_likelihood&quot;: True},
    )
    asdr_model_posterior_predictive = pm.sample_posterior_predictive(
        trace=asdr_model_trace
    )</code></pre>
<p><strong>Remark:</strong> In <code>pymc &gt;= 5.0</code> it seems we can not use the JAX samplers with a <a href="https://docs.pymc.io/en/stable/api/distributions/generated/pymc.GaussianRandomWalk.html"><code>pm.GaussianRandomWalk</code></a>, see <a href="https://github.com/pymc-devs/pymc/issues/6467">this issue</a> for more details.</p>
<ul>
<li>Model Diagnostics</li>
</ul>
<pre class="python"><code>az.summary(
    data=asdr_model_trace,
    var_names=[&quot;a&quot;, &quot;b_trend&quot;, &quot;sigma_slope&quot;, &quot;alpha&quot;, &quot;lam&quot;, &quot;sigma&quot;, &quot;nu&quot;]
)</code></pre>
<center>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
mean
</th>
<th>
sd
</th>
<th>
hdi_3%
</th>
<th>
hdi_97%
</th>
<th>
mcse_mean
</th>
<th>
mcse_sd
</th>
<th>
ess_bulk
</th>
<th>
ess_tail
</th>
<th>
r_hat
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
a
</th>
<td>
0.720
</td>
<td>
0.006
</td>
<td>
0.708
</td>
<td>
0.732
</td>
<td>
0.000
</td>
<td>
0.000
</td>
<td>
11466.0
</td>
<td>
15495.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
b_trend
</th>
<td>
0.184
</td>
<td>
0.008
</td>
<td>
0.168
</td>
<td>
0.198
</td>
<td>
0.000
</td>
<td>
0.000
</td>
<td>
3348.0
</td>
<td>
6134.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
sigma_slope
</th>
<td>
0.040
</td>
<td>
0.009
</td>
<td>
0.025
</td>
<td>
0.058
</td>
<td>
0.000
</td>
<td>
0.000
</td>
<td>
717.0
</td>
<td>
1619.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
alpha
</th>
<td>
0.509
</td>
<td>
0.028
</td>
<td>
0.455
</td>
<td>
0.560
</td>
<td>
0.000
</td>
<td>
0.000
</td>
<td>
4477.0
</td>
<td>
10449.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
lam
</th>
<td>
1.589
</td>
<td>
0.248
</td>
<td>
1.129
</td>
<td>
2.048
</td>
<td>
0.005
</td>
<td>
0.004
</td>
<td>
2450.0
</td>
<td>
3531.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
sigma
</th>
<td>
0.008
</td>
<td>
0.001
</td>
<td>
0.007
</td>
<td>
0.010
</td>
<td>
0.000
</td>
<td>
0.000
</td>
<td>
13002.0
</td>
<td>
18058.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
nu
</th>
<td>
11.355
</td>
<td>
3.099
</td>
<td>
5.825
</td>
<td>
17.072
</td>
<td>
0.017
</td>
<td>
0.012
</td>
<td>
33564.0
</td>
<td>
18162.0
</td>
<td>
1.0
</td>
</tr>
</tbody>
</table>
</div>
</center>
<p>As in the second model, the true values of <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\lambda\)</span> are included in the posterior distributions <span class="math inline">\(94\%\)</span> hdi (up to a scale as we will see below).</p>
<pre class="python"><code>axes = az.plot_trace(
    data=asdr_model_trace,
    var_names=[
        &quot;a&quot;,
        &quot;b_trend&quot;,
        &quot;sigma_slope&quot;,
        &quot;b_fourier&quot;,
        &quot;alpha&quot;,
        &quot;lam&quot;,
        &quot;sigma&quot;,
        &quot;nu&quot;,
    ],
    compact=True,
    backend_kwargs={&quot;figsize&quot;: (12, 12), &quot;layout&quot;: &quot;constrained&quot;},
)
fig = axes[0][0].get_figure()
fig.suptitle(&quot;Adstock-Saturation-Diminishing-Returns Model - Trace&quot;);</code></pre>
<center>
<img src="../images/pymc_mmm_files/pymc_mmm_106_0.png" style="width: 1000px;"/>
</center>
<pre class="python"><code>fig, ax = plt.subplots(figsize=(6, 4))
az.plot_forest(
    data=asdr_model_trace,
    var_names=[&quot;a&quot;, &quot;b_trend&quot;, &quot;sigma_slope&quot;, &quot;alpha&quot;, &quot;lam&quot;, &quot;sigma&quot;, &quot;nu&quot;],
    combined=True,
    ax=ax
)
ax.set(
    title=&quot;Adstock-Saturation-Diminishing-Returns Model Model: 94.0% HDI&quot;,
    xscale=&quot;log&quot;
);</code></pre>
<center>
<img src="../images/pymc_mmm_files/pymc_mmm_107_0.png" style="width: 700px;"/>
</center>
<ul>
<li>Posterior Predictive Samples</li>
</ul>
<pre class="python"><code>posterior_predictive_likelihood = az.extract(
    data=asdr_model_posterior_predictive,
    group=&quot;posterior_predictive&quot;,
    var_names=&quot;likelihood&quot;,
)

posterior_predictive_likelihood_inv = endog_scaler.inverse_transform(
    X=posterior_predictive_likelihood
)

fig, ax = plt.subplots()

for i, p in enumerate(percs[::-1]):
    upper = np.percentile(posterior_predictive_likelihood_inv, p, axis=1)
    lower = np.percentile(posterior_predictive_likelihood_inv, 100 - p, axis=1)
    color_val = colors[i]
    ax.fill_between(
        x=date,
        y1=upper,
        y2=lower,
        color=cmap(color_val),
        alpha=0.1,
    )

sns.lineplot(
    x=date,
    y=posterior_predictive_likelihood_inv.mean(axis=1),
    color=&quot;C2&quot;,
    label=&quot;posterior predictive mean&quot;,
    ax=ax,
)
sns.lineplot(
    x=date,
    y=y,
    color=&quot;black&quot;,
    label=&quot;target&quot;,
    ax=ax,
)
ax.legend(loc=&quot;upper left&quot;)
ax.set(title=&quot;Adstock-Saturation-Diminishing-Returns Model - Posterior Predictive&quot;);</code></pre>
<center>
<img src="../images/pymc_mmm_files/pymc_mmm_109_0.png" style="width: 1000px;"/>
</center>
<pre class="python"><code># compute HDI for all the model parameters
model_hdi = az.hdi(ary=asdr_model_trace)

fig, ax = plt.subplots()

for i, var_effect in enumerate([&quot;z_effect&quot;, &quot;trend&quot;, &quot;seasonality&quot;]):
    ax.fill_between(
        x=date,
        y1=model_hdi[var_effect][:, 0],
        y2=model_hdi[var_effect][:, 1],
        color=f&quot;C{i}&quot;,
        alpha=0.3,
        label=f&quot;$94\%$ HDI ({var_effect})&quot;,
    )
    sns.lineplot(
        x=date,
        y=asdr_model_trace.posterior[var_effect]
        .stack(sample=(&quot;chain&quot;, &quot;draw&quot;))
        .mean(axis=1),
        color=f&quot;C{i}&quot;,
    )

sns.lineplot(
    x=date, y=y_scaled, color=&quot;black&quot;, alpha=1.0, label=&quot;target (scaled)&quot;, ax=ax
)
ax.legend(title=&quot;components&quot;, loc=&quot;center left&quot;, bbox_to_anchor=(1, 0.5))
ax.set(
    title=&quot;Adstock-Saturation-Diminishing-Return Model Components&quot;,
    ylabel=&quot;target (scaled)&quot;,
);</code></pre>
<center>
<img src="../images/pymc_mmm_files/pymc_mmm_110_0.png" style="width: 1000px;"/>
</center>
<p>Now we want to deep dive into the parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\lambda\)</span> of the adstock and saturation transformations respectively. First, let us look into their joint posterior distributions.</p>
<pre class="python"><code>alpha_true = 0.5
lam_true = 0.15
lam_true_scaled = 0.15 * channel_scaler.scale_.item()

fig, ax = plt.subplots(figsize=(6, 5))
az.plot_pair(
    data=asdr_model_trace,
    var_names=[&quot;alpha&quot;, &quot;lam&quot;],
    kind=&quot;kde&quot;,
    divergences=True,
    ax=ax
)
ax.axhline(lam_true_scaled, color=&quot;C1&quot;, linestyle=&quot;--&quot;, label=&quot;$\lambda_{true} (scaled)$&quot;)
ax.axvline(alpha_true, color=&quot;C4&quot;, linestyle=&quot;--&quot;, label=&quot;$\\alpha_{true}$&quot;)
ax.legend(title=&quot;true params&quot;, loc=&quot;upper right&quot;)
ax.set(
    title=&quot;Adstock-Saturation-Diminishing-Returns Model&quot;,
    xlabel=&quot;$\\alpha$&quot;,
    ylabel=&quot;$\lambda$&quot;
);</code></pre>
<center>
<img src="../images/pymc_mmm_files/pymc_mmm_112_0.png" style="width: 700px;"/>
</center>
<p>The true values are quite close to the posterior mode. Note that there seems to be a weak negative correlation between these two parameters.</p>
<pre class="python"><code>corr, _ = pearsonr(
    x=asdr_model_trace.posterior[&quot;alpha&quot;].stack(sample=(&quot;chain&quot;, &quot;draw&quot;)).to_numpy(),
    y=asdr_model_trace.posterior[&quot;lam&quot;].stack(sample=(&quot;chain&quot;, &quot;draw&quot;)).to_numpy()
)

print(f&quot;Correlation between alpha and lambda {corr: 0.3f}&quot;);</code></pre>
<pre><code>Correlation between alpha and lambda -0.481</code></pre>
<ul>
<li><span class="math inline">\(\alpha\)</span> deep dive</li>
</ul>
<p>Now, we can look into the posterior distribution of the of <code>z</code> when applying the <code>geometric_adstock</code> transformation for all the <span class="math inline">\(\alpha\)</span> posterior samples.</p>
<pre class="python"><code>alpha_posterior = az.extract(data=asdr_model_trace, group=&quot;posterior&quot;, var_names=&quot;alpha&quot;)

alpha_posterior_samples = alpha_posterior.to_numpy()[:100]

# pass z through the adstock transformation
geometric_adstock_posterior_samples = np.array([
    geometric_adstock(x=z, alpha=x).eval()
    for x in alpha_posterior_samples
])</code></pre>
<p>Let us compare the estimates against the true values.</p>
<pre class="python"><code>geometric_adstock_hdi = az.hdi(ary=geometric_adstock_posterior_samples)

yerr = geometric_adstock_hdi[:, 1] - geometric_adstock_hdi[:, 0]

fig, ax = plt.subplots(figsize=(8, 7))

markers, caps, bars = ax.errorbar(
    x=data_df[&quot;z_adstock&quot;], 
    y=geometric_adstock_posterior_samples.mean(axis=0), 
    yerr=yerr/2,
    color=&quot;C0&quot;,
    fmt=&#39;o&#39;,
    ms=1,
    capsize=5,
    label=&quot;$94\%$ HDI&quot;,
)
[bar.set_alpha(0.3) for bar in bars]
ax.axline(
    xy1=(10, 10),
    slope=1.0,
    color=&quot;black&quot;,
    linestyle=&quot;--&quot;,
    label=&quot;diagonal&quot;
)
ax.legend()
ax.set(
    title=&quot;Adstock-Saturation-Diminishing-Returns Model - $\\alpha$ Estimation&quot;,
    xlabel=&quot;z_adstock (true)&quot;,
    ylabel=&quot;z_adstock (pred)&quot;,
);</code></pre>
<center>
<img src="../images/pymc_mmm_files/pymc_mmm_118_1.png" style="width: 700px;"/>
</center>
<p>We see that the model is capturing the adstock transformation (within the model uncertainty limits). Note how the hdi intervals increase as a function of <code>z_adstock</code> (true).</p>
<ul>
<li><span class="math inline">\(\lambda\)</span> deep dive</li>
</ul>
<p>Next we look into the <span class="math inline">\(\lambda\)</span> parameter. We follow the a similar strategy as above.</p>
<pre class="python"><code>lam_posterior = (
    az.extract(data=asdr_model_trace, group=&quot;posterior&quot;, var_names=&quot;lam&quot;)
    / channel_scaler.scale_.item()
)

lam_posterior_samples = lam_posterior.to_numpy()[:100]

logistic_saturation_posterior_samples = np.array(
    [
        logistic_saturation(x=x, lam=lam_posterior_samples).eval()
        for x in data_df[&quot;z_adstock&quot;].values
    ]
)</code></pre>
<p>We can now plot the estimated saturation curve against the true one.</p>
<pre class="python"><code>logistic_saturation_hdi = az.hdi(ary=logistic_saturation_posterior_samples.T)

yerr = logistic_saturation_hdi[:, 1] - logistic_saturation_hdi[:, 0]


fig, ax = plt.subplots(figsize=(7, 6))

latex_function = r&quot;$x\longmapsto \frac{1 - e^{-\lambda x}}{1 + e^{-\lambda x}}$&quot;

markers, caps, bars = ax.errorbar(
    x=data_df[&quot;z_adstock&quot;], 
    y=logistic_saturation_posterior_samples.mean(axis=1), 
    yerr=yerr/2,
    color=&quot;C0&quot;,
    fmt=&#39;o&#39;,
    ms=3,
    capsize=5,
    label=&quot;$94\%$ HDI&quot;,
)
[bar.set_alpha(0.3) for bar in bars]
sns.lineplot(
    x=&quot;z_adstock&quot;,
    y=&quot;z_adstock_saturated&quot;,
    color=&quot;C2&quot;,
    label=latex_function,
    data=data_df,
    ax=ax
)
ax.legend(loc=&quot;lower right&quot;, prop={&quot;size&quot;: 15})
ax.set(
    title=&quot;Adstock-Saturation-Diminishing-Returns Model - $\lambda$ Estimation&quot;,
    xlabel=&quot;z_adstock (true)&quot;,
    ylabel=&quot;z_adstock_saturaded (pred)&quot;,
);</code></pre>
<center>
<img src="../images/pymc_mmm_files/pymc_mmm_123_1.png" style="width: 700px;"/>
</center>
<p>The true saturation curve lies within the <span class="math inline">\(94\%\)</span> hdi estimated by the model.</p>
<ul>
<li>Transformation Deep-Dive</li>
</ul>
<pre class="python"><code>model_hdi_inv = az.hdi(ary=asdr_model_trace)

fig, axes = plt.subplots(
    nrows=4, ncols=1, figsize=(12, 9), sharex=True, sharey=False, layout=&quot;constrained&quot;
)

sns.lineplot(
    x=date,
    y=z,
    color=&quot;black&quot;,
    ax=axes[0],
)
axes[0].set(title=&quot;z&quot;)

for i, var_name in enumerate([&quot;z_adstock&quot;, &quot;z_adstock_saturated&quot;, &quot;z_effect&quot;]):

    var_name_posterior = endog_scaler.inverse_transform(
            X=az.extract(data=asdr_model_trace, group=&quot;posterior&quot;, var_names=var_name)
        )
    var_name_hdi =az.hdi(ary=var_name_posterior.T)

    ax = axes[i + 1]
    sns.lineplot(
        x=date,
        y=var_name_posterior.mean(axis=1),
        color=f&quot;C{i}&quot;,
        ax=ax,
    )
    ax.fill_between(
        x=date,
        y1=var_name_hdi[:, 0],
        y2=var_name_hdi[:, 1],
        color=f&quot;C{i}&quot;,
        alpha=0.5,
    )
    ax.set(title=var_name)
</code></pre>
<center>
<img src="../images/pymc_mmm_files/pymc_mmm_126_1.png" style="width: 1000px;"/>
</center>
<ul>
<li>Estimated <code>z_effect</code></li>
</ul>
<p>Let us look into the estimated effect of <code>z</code> on <code>y</code> inferred by the model against the true one from the data generation process.</p>
<pre class="python"><code>z_effect_posterior_samples = xr.apply_ufunc(
    lambda x: endog_scaler.inverse_transform(X=x.reshape(1, -1)),
    asdr_model_trace.posterior[&quot;z_effect&quot;],
    input_core_dims=[[&quot;date&quot;]],
    output_core_dims=[[&quot;date&quot;]],
    vectorize=True,
)

z_effect_hdi = az.hdi(ary=z_effect_posterior_samples)[&quot;z_effect&quot;]

fig, ax = plt.subplots()
ax.fill_between(
    x=date,
    y1=z_effect_hdi[:, 0],
    y2=z_effect_hdi[:, 1],
    color=&quot;C0&quot;,
    alpha=0.5,
    label=&quot;z_effect 94% HDI&quot;,
)
ax.axhline(
    y=z_effect_posterior_samples.mean(),
    color=&quot;C0&quot;,
    linestyle=&quot;--&quot;,
    label=f&quot;posterior mean {z_effect_posterior_samples.mean().values: 0.3f}&quot;,
)
sns.lineplot(x=&quot;date&quot;, y=&quot;z_effect&quot;, color=&quot;C3&quot;, data=data_df, label=&quot;z_effect&quot;, ax=ax)
ax.legend(loc=&quot;upper right&quot;)
ax.set(
    title=&quot;Media Cost Effect Estimation - Adstock-Saturation-Diminishing-Returns Model&quot;
);</code></pre>
<center>
<img src="../images/pymc_mmm_files/pymc_mmm_128_0.png" style="width: 1000px;"/>
</center>
<p>As expected, we get a very good fit. In particular, the model is capturing the time-varying effect as a result of the gaussian random walk component. As above, we can now look into the estimated vs true scatter plot.</p>
<pre class="python"><code>fig, ax = plt.subplots()

az.plot_hdi(
    x=z,
    y=z_effect_posterior_samples,
    color=&quot;C0&quot;,
    fill_kwargs={&quot;alpha&quot;: 0.2, &quot;label&quot;: &quot;z_effect 94% HDI&quot;},
    ax=ax,
)
sns.scatterplot(
    x=&quot;z&quot;,
    y=&quot;z_effect_pred_mean&quot;,
    color=&quot;C0&quot;,
    size=&quot;index&quot;,
    label=&quot;z_effect (pred mean)&quot;,
    data=data_df.assign(
        z_effect_pred_mean=z_effect_posterior_samples.mean(dim=(&quot;chain&quot;, &quot;draw&quot;))
    ),
    ax=ax,
)
sns.scatterplot(
    x=&quot;z&quot;, y=&quot;z_effect&quot;, color=&quot;C3&quot;, size=&quot;index&quot;, label=&quot;z_effect (true)&quot;, data=data_df
)
h, l = ax.get_legend_handles_labels()
ax.legend(handles=h[:9], labels=l[:9], loc=&quot;center left&quot;, bbox_to_anchor=(1, 0.5))
ax.set(title=&quot;Adstock-Saturation-Diminishing-Returns Model - Estimated Effect&quot;);</code></pre>
<center>
<img src="../images/pymc_mmm_files/pymc_mmm_130_0.png" style="width: 1000px;"/>
</center>
<p>It is interesting to see that the non-linear pattern does not look precisely as a logistic saturation. One of the main reasons for this is the diminishing returns in the time component. We can better see the logistic-like saturation if we factor the time component, for example by splitting by year:</p>
<pre class="python"><code>z_effect_hdi = az.hdi(ary=z_effect_posterior_samples)[&quot;z_effect&quot;]

data_df = data_df.assign(
    z_effect_pred_mean=z_effect_posterior_samples.mean(dim=(&quot;chain&quot;, &quot;draw&quot;)),
    z_effect_hdi_lower=z_effect_hdi[:, 0],
    z_effect_hdi_upper=z_effect_hdi[:, 1],
)</code></pre>
<pre class="python"><code>fig, axes = plt.subplots(
    nrows=2, ncols=2, figsize=(10, 9), sharex=True, sharey=True, layout=&quot;constrained&quot;
)

axes = axes.flatten()

for i, year in enumerate(data_df[&quot;year&quot;].sort_values().unique()):
    ax = axes[i]
    mask = f&quot;year == {year}&quot;

    yerr = (
        data_df.query(mask)[&quot;z_effect_hdi_upper&quot;]
        - data_df.query(mask)[&quot;z_effect_hdi_lower&quot;]
    )

    markers, caps, bars = ax.errorbar(
        x=data_df.query(mask)[&quot;z&quot;],
        y=data_df.query(mask)[&quot;z_effect_pred_mean&quot;],
        yerr=yerr / 2,
        color=&quot;C0&quot;,
        fmt=&quot;o&quot;,
        ms=0,
        capsize=5,
        label=&quot;estimated effect&quot;,
    )
    [bar.set_alpha(0.3) for bar in bars]
    sns.regplot(
        x=&quot;z&quot;,
        y=&quot;z_effect_pred_mean&quot;,
        order=2,
        color=&quot;C0&quot;,
        label=&quot;pred mean effect&quot;,
        data=data_df.query(mask),
        ax=ax,
    )
    sns.regplot(
        x=&quot;z&quot;,
        y=&quot;z_effect&quot;,
        order=2,
        color=&quot;C3&quot;,
        label=&quot;true effect&quot;,
        data=data_df.query(mask),
        ax=ax,
    )
    if i == 0:
        ax.legend(loc=&quot;upper center&quot;, bbox_to_anchor=(0.5, 1.2), ncol=3)
    else:
        ax.legend().remove()
    ax.set(title=f&quot;{year}&quot;)

fig.suptitle(&quot;Media Cost Effect Estimation - ASDR Model&quot;, y=1.05);</code></pre>
<center>
<img src="../images/pymc_mmm_files/pymc_mmm_133_0.png" style="width: 1000px;"/>
</center>
<p>The results look very good 🚀! These are exactly the curves we would need for decision making and budget allocation.</p>
<p><strong>Remark:</strong> Note that this plot looks very similar to the one obtained in the post <a href="https://juanitorduz.github.io/orbit_mmm/">Media Effect Estimation with Orbit’s KTR Model</a>. The main difference is that in the previous post we could only estimated the effect of <code>z_adstock</code> on <code>y</code> while in this one we estimate the effect of <code>z</code> on <code>y</code> directly by learning the adstock effect from the data.</p>
<div id="roas-and-mroas-3" class="section level4">
<h4>ROAS and mROAS</h4>
<ul>
<li><strong>ROAS</strong></li>
</ul>
<pre class="python"><code>asdr_model_trace_roas = asdr_model_trace.copy()

with asdr_model:
    pm.set_data(new_data={&quot;z_scaled&quot;: np.zeros_like(a=z_scaled)})
    asdr_model_trace_roas.extend(
        other=pm.sample_posterior_predictive(trace=asdr_model_trace_roas, var_names=[&quot;likelihood&quot;])
    )</code></pre>
<pre class="python"><code>asdr_roas_numerator = (
    endog_scaler.inverse_transform(
        X=az.extract(
            data=asdr_model_posterior_predictive,
            group=&quot;posterior_predictive&quot;,
            var_names=[&quot;likelihood&quot;],
        )

    )
    - endog_scaler.inverse_transform(
        X=az.extract(
            data=asdr_model_trace_roas,
            group=&quot;posterior_predictive&quot;,
            var_names=[&quot;likelihood&quot;],
        )
    )
).sum(axis=0)

roas_denominator = z.sum()

asdr_roas = asdr_roas_numerator / roas_denominator
</code></pre>
<pre class="python"><code>asdr_roas_mean = np.median(asdr_roas)
asdr_roas_hdi = az.hdi(ary=asdr_roas)

g = sns.displot(x=asdr_roas, kde=True, height=5, aspect=1.5)
ax = g.axes.flatten()[0]
ax.axvline(
    x=asdr_roas_mean, color=&quot;C0&quot;, linestyle=&quot;--&quot;, label=f&quot;mean = {asdr_roas_mean: 0.3f}&quot;
)
ax.axvline(
    x=asdr_roas_hdi[0],
    color=&quot;C1&quot;,
    linestyle=&quot;--&quot;,
    label=f&quot;HDI_lwr = {asdr_roas_hdi[0]: 0.3f}&quot;,
)
ax.axvline(
    x=asdr_roas_hdi[1],
    color=&quot;C2&quot;,
    linestyle=&quot;--&quot;,
    label=f&quot;HDI_upr = {asdr_roas_hdi[1]: 0.3f}&quot;,
)
ax.axvline(x=roas_true, color=&quot;black&quot;, linestyle=&quot;--&quot;, label=f&quot;true = {roas_true: 0.3f}&quot;)
ax.legend(loc=&quot;center left&quot;, bbox_to_anchor=(1, 0.5))
ax.set(title=&quot;Adstock-Saturation-Diminishing-Returns Model ROAS&quot;);</code></pre>
<center>
<img src="../images/pymc_mmm_files/pymc_mmm_138_0.png" style="width: 1000px;"/>
</center>
<p>As expected, the true and predicted values are very close! Also, the HDI is much smaller than the adstock-saturation model.</p>
<ul>
<li><strong>mROAS</strong></li>
</ul>
<pre class="python"><code>eta: float = 0.10

asdr_model_trace_mroas = asdr_model_trace.copy()

with asdr_model:
    pm.set_data(new_data={&quot;z_scaled&quot;: (1 + eta) * z_scaled})
    asdr_model_trace_mroas.extend(
        other=pm.sample_posterior_predictive(trace=asdr_model_trace_mroas, var_names=[&quot;likelihood&quot;])
    )</code></pre>
<pre class="python"><code>asdr_mroas_numerator = (
    endog_scaler.inverse_transform(
        X=az.extract(
            data=asdr_model_trace_mroas,
            group=&quot;posterior_predictive&quot;,
            var_names=[&quot;likelihood&quot;],
        )
    )
    - endog_scaler.inverse_transform(
        X=az.extract(
            data=asdr_model_posterior_predictive,
            group=&quot;posterior_predictive&quot;,
            var_names=[&quot;likelihood&quot;],
        )
    )
).sum(axis=0)

mroas_denominator = eta * z.sum()

asdr_mroas = asdr_mroas_numerator / mroas_denominator</code></pre>
<pre class="python"><code>asdr_mroas_mean = asdr_mroas.mean()
asdr_mroas_hdi = az.hdi(ary=asdr_mroas)

g = sns.displot(x=asdr_mroas, kde=True, height=5, aspect=1.5)
ax = g.axes.flatten()[0]
ax.axvline(
    x=asdr_mroas_mean, color=&quot;C0&quot;, linestyle=&quot;--&quot;, label=f&quot;mean = {asdr_mroas_mean: 0.3f}&quot;
)
ax.axvline(
    x=asdr_mroas_hdi[0],
    color=&quot;C1&quot;,
    linestyle=&quot;--&quot;,
    label=f&quot;HDI_lwr = {asdr_mroas_hdi[0]: 0.3f}&quot;,
)
ax.axvline(
    x=asdr_mroas_hdi[1],
    color=&quot;C2&quot;,
    linestyle=&quot;--&quot;,
    label=f&quot;HDI_upr = {asdr_roas_hdi[1]: 0.3f}&quot;,
)
ax.axvline(x=0.0, color=&quot;gray&quot;, linestyle=&quot;--&quot;, label=&quot;zero&quot;)
ax.axvline(x=mroas_true, color=&quot;black&quot;, linestyle=&quot;--&quot;, label=f&quot;true = {mroas_true: 0.3f}&quot;)
ax.legend(loc=&quot;center left&quot;, bbox_to_anchor=(1, 0.5))
ax.set(title=f&quot;Adstock-Saturation-Diminishing-Returns Model MROAS ({eta:.0%} increase)&quot;);</code></pre>
<center>
<img src="../images/pymc_mmm_files/pymc_mmm_143_0.png" style="width: 1000px;"/>
</center>
<p>The estimated mROAS mean and the true values aver almost the same!</p>
</div>
</div>
</div>
<div id="model-comparison" class="section level2">
<h2>Model Comparison</h2>
<p>To end this notebook, let us compare the three models.</p>
<pre class="python"><code>dataset_dict = {
    &quot;base_model&quot;: base_model_trace,
    &quot;adstock_saturation_model&quot;: adstock_saturation_model_trace,
    &quot;asdr_model&quot;: asdr_model_trace,
}

az.compare(compare_dict=dataset_dict, ic=&quot;loo&quot;, method=&quot;stacking&quot;, scale=&quot;log&quot;)</code></pre>
<center>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe thead th {
        text-align: left;
        font-size: 16px;
    }

    .dataframe tbody tr th {
        vertical-align: top;
        font-size: 16px;
    }
    
    .dataframe tbody tr td {
        vertical-align: top;
        font-size: 16px;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
rank
</th>
<th>
elpd_loo
</th>
<th>
p_loo
</th>
<th>
elpd_diff
</th>
<th>
weight
</th>
<th>
se
</th>
<th>
dse
</th>
<th>
warning
</th>
<th>
scale
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
asdr_model
</th>
<td>
0
</td>
<td>
565.150979
</td>
<td>
36.346577
</td>
<td>
0.000000
</td>
<td>
0.984025
</td>
<td>
9.062507
</td>
<td>
0.000000
</td>
<td>
False
</td>
<td>
log
</td>
</tr>
<tr>
<th>
adstock_saturation_model
</th>
<td>
1
</td>
<td>
518.390205
</td>
<td>
20.722418
</td>
<td>
46.760774
</td>
<td>
0.015975
</td>
<td>
8.762302
</td>
<td>
8.842153
</td>
<td>
False
</td>
<td>
log
</td>
</tr>
<tr>
<th>
base_model
</th>
<td>
2
</td>
<td>
464.316965
</td>
<td>
18.135413
</td>
<td>
100.834014
</td>
<td>
0.000000
</td>
<td>
10.852874
</td>
<td>
12.764730
</td>
<td>
False
</td>
<td>
log
</td>
</tr>
</tbody>
</table>
</div>
</center>
<p>We clearly see that the <code>asdr_model</code> is the best one (no surprise here). Let’s finally compare the posterior distributions for the three models.</p>
<pre class="python"><code>axes = az.plot_forest(
    data=[base_model_trace, adstock_saturation_model_trace, asdr_model_trace],
    model_names=[&quot;base_model&quot;, &quot;adstock_saturation_model&quot;, &quot;asdr_model&quot;],
    var_names=[&quot;a&quot;, &quot;b_trend&quot;, &quot;b_z&quot;, &quot;sigma_slope&quot;, &quot;alpha&quot;, &quot;lam&quot;, &quot;sigma&quot;],
    combined=True,
    figsize=(10, 7),
);</code></pre>
<center>
<img src="../images/pymc_mmm_files/pymc_mmm_148_0.png" style="width: 1000px;"/>
</center>
<pre class="python"><code>axes = az.plot_forest(
    data=[base_model_trace, adstock_saturation_model_trace, asdr_model_trace],
    model_names=[&quot;base_model&quot;, &quot;adstock_saturation_model&quot;, &quot;asdr_model&quot;],
    var_names=[&quot;nu&quot;],
    combined=True,
    figsize=(8, 3),
);</code></pre>
<center>
<img src="../images/pymc_mmm_files/pymc_mmm_149_0.png" style="width: 700px;"/>
</center>
<p>Finally, let’s compare the estimated ROAS and mROAS of all models. First we collect the samples:</p>
<pre class="python"><code>roas_samples_df = (
    pd.DataFrame(
        data={
            &quot;base&quot;: base_roas,
            &quot;adstock-saturation&quot;: adstock_saturation_roas,
            &quot;asdr&quot;: asdr_mroas,
        }
    )
    .melt()
    .assign(metric=&quot;ROAS&quot;)
)

mroas_samples_df = (
    pd.DataFrame(
        data={
            &quot;base&quot;: base_mroas,
            &quot;adstock-saturation&quot;: adstock_saturation_mroas,
            &quot;asdr&quot;: asdr_mroas,
        }
    )
    .melt()
    .assign(metric=&quot;mROAS&quot;)
)</code></pre>
<p>Now we generate the plots:</p>
<pre class="python"><code>fig, axes = plt.subplots(
    nrows=2, ncols=1, sharex=True, sharey=False, figsize=(10, 7), layout=&quot;constrained&quot;
)
sns.violinplot(x=&quot;variable&quot;, y=&quot;value&quot;, color=&quot;C0&quot;, data=roas_samples_df, ax=axes[0])
axes[0].axhline(
    y=roas_true, color=&quot;black&quot;, linestyle=&quot;--&quot;, label=f&quot;true value = {roas_true: 0.3f}&quot;
)
axes[0].legend(loc=&quot;upper left&quot;)
axes[0].set(title=&quot;ROAS Samples - Model Comparison&quot;, xlabel=&quot;model&quot;, ylabel=&quot;ROAS&quot;)
sns.violinplot(x=&quot;variable&quot;, y=&quot;value&quot;, color=&quot;C1&quot;, data=mroas_samples_df, ax=axes[1])
axes[1].axhline(
    y=mroas_true,
    color=&quot;black&quot;,
    linestyle=&quot;--&quot;,
    label=f&quot;true value = {mroas_true: 0.3f}&quot;,
)
axes[1].legend(loc=&quot;upper left&quot;)
axes[1].set(
    title=f&quot;mROAS Samples({eta:.0%} increase) - Model Comparison&quot;,
    xlabel=&quot;model&quot;,
    ylabel=&quot;mROAS&quot;,
);</code></pre>
<center>
<img src="../images/pymc_mmm_files/pymc_mmm_153_0.png" style="width: 1000px;"/>
</center>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="../index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="../images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/dockerfile.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/bash.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="../js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'G-5NM5EDH834', 'auto');
	
	ga('send', 'pageview');
}
</script>

  </body>
</html>

