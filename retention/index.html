<!DOCTYPE html>
<html lang="en-us">
  <head>
    <script defer src="https://use.fontawesome.com/releases/v5.13.0/js/all.js"></script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-5NM5EDH834"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-5NM5EDH834');
</script>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.79.1" />


<title>A Simple Cohort Retention Analysis in PyMC - Dr. Juan Camilo Orduz</title>
<meta property="og:title" content="A Simple Cohort Retention Analysis in PyMC - Dr. Juan Camilo Orduz">


  <link href='../images/favicon.ico' rel='icon' type='image/x-icon'/>



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="../css/fonts.css" media="all">
<link rel="stylesheet" href="../css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="../" class="nav-logo">
    <img src="../images/sphere2.gif"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="../about/"> About</a></li>
    
    <li><a href="https://github.com/juanitorduz"><i class='fab fa-github fa-2x'></i>  </a></li>
    
    <li><a href="https://www.linkedin.com/in/juanitorduz/"><i class='fab fa-linkedin fa-2x' style='color:#0a66c2;'></i>  </a></li>
    
    <li><a href="https://bayes.club/@juanitorduz"><i class='fab fa-mastodon fa-2x' style='color:#6364FF;'></i>  </a></li>
    
    <li><a href="https://twitter.com/juanitorduz"><i class='fab fa-twitter fa-2x' style='color:#1DA1F2;'></i>  </a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">12 min read</span>
    

    <h1 class="article-title">A Simple Cohort Retention Analysis in PyMC</h1>

    
    <span class="article-date">2022-12-20</span>
    

    <div class="article-content">
      


<p>In this notebook we present a simple approach to study cohort retention analysis through a simulated data set. The aim is to understand how retention rates change over time and provide a simple model to predict them (with uncertainty estimates!). We do not expect this technique to be a silver bullet for all retention problems, but rather a simple approach to get started with the problem.</p>
<p><strong>Remark:</strong> A motivation for this notebook was the great post <em><a href="https://www.austinrochford.com/posts/apc-pymc.html">Bayesian Age/Period/Cohort Models in Python with PyMC</a></em> by <a href="https://www.austinrochford.com/about.html">Austin Rochford</a>.</p>
<div id="prepare-notebook" class="section level2">
<h2>Prepare Notebook</h2>
<pre class="python"><code>import arviz as az
import matplotlib.pyplot as plt
import matplotlib.ticker as mtick
import numpy as np
import numpy.typing as npt
import pandas as pd
import pymc as pm
import pymc.sampling_jax
import seaborn as sns
from scipy.special import expit
from sklearn.preprocessing import LabelEncoder, StandardScaler


plt.style.use(&quot;bmh&quot;)
plt.rcParams[&quot;figure.figsize&quot;] = [12, 7]
plt.rcParams[&quot;figure.dpi&quot;] = 100
plt.rcParams[&quot;figure.facecolor&quot;] = &quot;white&quot;

%load_ext autoreload
%autoreload 2
%config InlineBackend.figure_format = &quot;retina&quot;</code></pre>
</div>
<div id="generate-data" class="section level2">
<h2>Generate Data</h2>
<p>In this section we generate some synthetic retention data. We assume we are interested in monthly retention rates. Here is the logic behind the data generation process: For a given month, say <code>2020-01</code>, we acquired <span class="math inline">\(100\)</span> users from which <span class="math inline">\(20\)</span> returned back (say, did a purchase) in <code>2020-02</code>. The retention rate for <code>2020-02</code> for the <code>2020-01</code> cohort is then <span class="math inline">\(20/100 = 0.2\)</span>. We simulate cohort retention rates based on the following assumptions:</p>
<ul>
<li>The retention rate is a function of the <em>cohort age</em> (i.e. how many months the cohort has been alive) and the <em>absolute age</em> (i.e. how many months have passed since the first month of the cohort since today).</li>
<li>The retention rate has a yearly seasonality component and a trend component.</li>
</ul>
<p>We do not simulate retention rates directly but rather the number of users that returned back in a given month. We use a <a href="https://en.wikipedia.org/wiki/Binomial_distribution">Binomial distribution</a> for this purpose.</p>
<p>The following <code>CohortDataGenerator</code> class implements the data generation process.</p>
<pre class="python"><code>class CohortDataGenerator:
    def __init__(
        self,
        rng: np.random.Generator,
        start_cohort: str,
        n_cohorts,
        user_base: int = 10_000,
    ) -&gt; None:
        self.rng = rng
        self.start_cohort = start_cohort
        self.n_cohorts = n_cohorts
        self.user_base = user_base

    def _generate_cohort_labels(self) -&gt; pd.DatetimeIndex:
        return pd.period_range(
            start=&quot;2020-01-01&quot;, periods=self.n_cohorts, freq=&quot;M&quot;
        ).to_timestamp()

    def _generate_cohort_sizes(self) -&gt; npt.NDArray[np.int_]:
        ones = np.ones(shape=self.n_cohorts)
        trend = ones.cumsum() / ones.sum()
        return (
            (
                self.user_base
                * trend
                * self.rng.gamma(shape=1, scale=1, size=self.n_cohorts)
            )
            .round()
            .astype(int)
        )

    def _generate_dataset_base(self) -&gt; pd.DataFrame:
        cohorts = self._generate_cohort_labels()
        n_users = self._generate_cohort_sizes()
        data_df = pd.merge(
            left=pd.DataFrame(data={&quot;cohort&quot;: cohorts, &quot;n_users&quot;: n_users}),
            right=pd.DataFrame(data={&quot;period&quot;: cohorts}),
            how=&quot;cross&quot;,
        )
        data_df[&quot;age&quot;] = (data_df[&quot;period&quot;].max() - data_df[&quot;cohort&quot;]).dt.days
        data_df[&quot;cohort_age&quot;] = (data_df[&quot;period&quot;] - data_df[&quot;cohort&quot;]).dt.days
        data_df = data_df.query(&quot;cohort_age &gt;= 0&quot;)
        return data_df

    def _generate_retention_rates(self, data_df: pd.DataFrame) -&gt; pd.DataFrame:
        data_df[&quot;retention_true_mu&quot;] = (
            -data_df[&quot;cohort_age&quot;] / (data_df[&quot;age&quot;] + 1)
            + 0.8 * np.cos(2 * np.pi * data_df[&quot;period&quot;].dt.dayofyear / 365)
            + 0.5 * np.sin(2 * 3 * np.pi * data_df[&quot;period&quot;].dt.dayofyear / 365)
            - 0.5 * np.log1p(data_df[&quot;age&quot;])
            + 1.0
        )
        data_df[&quot;retention_true&quot;] = expit(data_df[&quot;retention_true_mu&quot;])
        return data_df

    def _generate_user_history(self, data_df: pd.DataFrame) -&gt; pd.DataFrame:
        data_df[&quot;n_active_users&quot;] = self.rng.binomial(
            n=data_df[&quot;n_users&quot;], p=data_df[&quot;retention_true&quot;]
        )
        data_df[&quot;n_active_users&quot;] = np.where(
            data_df[&quot;cohort_age&quot;] == 0, data_df[&quot;n_users&quot;], data_df[&quot;n_active_users&quot;]
        )
        return data_df

    def run(
        self,
    ) -&gt; pd.DataFrame:
        return (
            self._generate_dataset_base()
            .pipe(self._generate_retention_rates)
            .pipe(self._generate_user_history)
        )</code></pre>
<p>Let’s generate the data for <span class="math inline">\(48\)</span> cohorts.</p>
<pre class="python"><code>seed: int = sum(map(ord, &quot;retention&quot;))
rng: np.random.Generator = np.random.default_rng(seed=seed)

start_cohort: str = &quot;2020-01-01&quot;
n_cohorts: int = 48


cohort_generator = CohortDataGenerator(rng=rng, start_cohort=start_cohort, n_cohorts=n_cohorts)
data_df = cohort_generator.run()

# calculate retention rates
data_df[&quot;retention&quot;] = data_df[&quot;n_active_users&quot;] / data_df[&quot;n_users&quot;]

data_df.head()</code></pre>
<center>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe thead th {
        text-align: left;
        font-size: 16px;
    }

    .dataframe tbody tr th {
        vertical-align: top;
        font-size: 16px;
    }
    
    .dataframe tbody tr td {
        vertical-align: top;
        font-size: 16px;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
cohort
</th>
<th>
n_users
</th>
<th>
period
</th>
<th>
age
</th>
<th>
cohort_age
</th>
<th>
retention_true_mu
</th>
<th>
retention_true
</th>
<th>
n_active_users
</th>
<th>
retention
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
2020-01-01
</td>
<td>
150
</td>
<td>
2020-01-01
</td>
<td>
1430
</td>
<td>
0
</td>
<td>
-1.807373
</td>
<td>
0.140956
</td>
<td>
150
</td>
<td>
1.000000
</td>
</tr>
<tr>
<th>
1
</th>
<td>
2020-01-01
</td>
<td>
150
</td>
<td>
2020-02-01
</td>
<td>
1430
</td>
<td>
31
</td>
<td>
-1.474736
</td>
<td>
0.186224
</td>
<td>
25
</td>
<td>
0.166667
</td>
</tr>
<tr>
<th>
2
</th>
<td>
2020-01-01
</td>
<td>
150
</td>
<td>
2020-03-01
</td>
<td>
1430
</td>
<td>
60
</td>
<td>
-2.281286
</td>
<td>
0.092685
</td>
<td>
13
</td>
<td>
0.086667
</td>
</tr>
<tr>
<th>
3
</th>
<td>
2020-01-01
</td>
<td>
150
</td>
<td>
2020-04-01
</td>
<td>
1430
</td>
<td>
91
</td>
<td>
-3.206610
</td>
<td>
0.038918
</td>
<td>
6
</td>
<td>
0.040000
</td>
</tr>
<tr>
<th>
4
</th>
<td>
2020-01-01
</td>
<td>
150
</td>
<td>
2020-05-01
</td>
<td>
1430
</td>
<td>
121
</td>
<td>
-3.112983
</td>
<td>
0.042575
</td>
<td>
2
</td>
<td>
0.013333
</td>
</tr>
</tbody>
</table>
</div>
</center>
<p>Now we do a train test split to evaluate our model out-of-sample predictions.</p>
<pre class="python"><code>period_train_test_split = &quot;2022-11-01&quot;

train_data_df = data_df.query(&quot;period &lt;= @period_train_test_split&quot;)
test_data_df = data_df.query(&quot;period &gt; @period_train_test_split&quot;)
test_data_df = test_data_df[test_data_df[&quot;cohort&quot;].isin(train_data_df[&#39;cohort&#39;].unique())]</code></pre>
</div>
<div id="eda" class="section level2">
<h2>EDA</h2>
<p>Now, let’s have a look at the data. We restrict ourselves to the training data as we really want to use the test data as a hold-out-set. First, we plot the number of users per cohort and the number of users that returned back per cohort.</p>
<pre class="python"><code>fig, ax = plt.subplots(
    nrows=2, ncols=1, sharex=True, sharey=False, figsize=(12, 7), layout=&quot;constrained&quot;
)
(
    train_data_df[[&quot;cohort&quot;, &quot;n_users&quot;]]
    .drop_duplicates()
    .set_index(&quot;cohort&quot;)
    .plot(color=&quot;C0&quot;, marker=&quot;o&quot;, ax=ax[0])
)
ax[0].set(title=&quot;Number of users per cohort&quot;, xlabel=&quot;cohort&quot;, ylabel=&quot;number of users&quot;)

for cohort in train_data_df[&quot;cohort&quot;].unique()[:-1]:
    train_data_df.query(&quot;cohort == @cohort and cohort_age &gt; 0&quot;).set_index(&quot;period&quot;)[
        &quot;n_active_users&quot;
    ].plot(color=&quot;black&quot;, alpha=0.5, ax=ax[1])

ax[1].set(
    title=&quot;Number of active users per cohort (log scale)&quot;,
    xlabel=&quot;period&quot;,
    ylabel=&quot;number of active users&quot;,
    yscale=&quot;log&quot;,
);</code></pre>
<center>
<img src="../images/retention_files/retention_11_1.png" style="width: 1000px;"/>
</center>
<ul>
<li>Number of users: We see a mild positive trend with some peaks which do not seem driven by seasonality.</li>
<li>Number of returning users: We see a clear seasonality component with a peak in the winter months.</li>
</ul>
<p>Now we plot the retention rates per cohort.</p>
<pre class="python"><code>fig, ax = plt.subplots(figsize=(18, 7))

fmt = lambda y, _: f&quot;{y :0.0%}&quot;

(
    train_data_df.assign(
        cohort=lambda df: df[&quot;cohort&quot;].dt.strftime(&quot;%Y-%m&quot;),
        period=lambda df: df[&quot;period&quot;].dt.strftime(&quot;%Y-%m&quot;),
    )
    .query(&quot;cohort_age != 0&quot;)
    .filter([&quot;cohort&quot;, &quot;period&quot;, &quot;retention&quot;])
    .pivot(index=&quot;cohort&quot;, columns=&quot;period&quot;, values=&quot;retention&quot;)
    .pipe(
        (sns.heatmap, &quot;data&quot;),
        cmap=&quot;viridis_r&quot;,
        linewidths=0.2,
        linecolor=&quot;black&quot;,
        annot=True,
        fmt=&quot;0.0%&quot;,
        cbar_kws={&quot;format&quot;: mtick.FuncFormatter(fmt)},
        ax=ax,
    )
)

ax.set_title(&quot;Retention by Cohort and Period&quot;);</code></pre>
<center>
<img src="../images/retention_files/retention_14_1.png" style="width: 1000px;"/>
</center>
<pre class="python"><code>fig, ax = plt.subplots(figsize=(12, 7))
sns.lineplot(
    x=&quot;period&quot;,
    y=&quot;retention&quot;,
    hue=&quot;cohort&quot;,
    palette=&quot;viridis_r&quot;,
    alpha=0.8,
    data=train_data_df.query(&quot;cohort_age &gt; 0&quot;).assign(
        cohort=lambda df: df[&quot;cohort&quot;].dt.strftime(&quot;%Y-%m&quot;)
    ),
    ax=ax,
)
ax.legend(title=&quot;cohort&quot;, loc=&quot;center left&quot;, bbox_to_anchor=(1, 0.5), fontsize=7.5)
ax.set(title=&quot;Retention by Cohort and Period&quot;);</code></pre>
<center>
<img src="../images/retention_files/retention_15_1.png" style="width: 1000px;"/>
</center>
<ul>
<li>It seems that for a given period, the retention rates of the new cohorts are higher than the retention rates of the older cohorts. This is a clear indication that the retention rate is a function of the absolute cohort age.</li>
<li>We also see a clear seasonality component in the retention rates.</li>
<li>For a given cohort, seasonality peaks are decreasing as a function of time (period).</li>
</ul>
<p>Let’s have a look at this last point in more detail. We plot the retention rates per month:</p>
<pre class="python"><code>g = sns.relplot(
    x=&quot;period&quot;,
    y=&quot;retention&quot;,
    hue=&quot;cohort&quot;,
    palette=&quot;viridis_r&quot;,
    col=&quot;month&quot;,
    col_wrap=3,
    kind=&quot;line&quot;,
    marker=&quot;o&quot;,
    alpha=0.8,
    data=train_data_df.query(&quot;cohort_age &gt; 0&quot;).assign(
        cohort=lambda df: df[&quot;cohort&quot;].dt.strftime(&quot;%Y-%m&quot;),
        month=lambda df: df[&quot;period&quot;].dt.strftime(&quot;%b&quot;)
    ),
    height=2,
    aspect=3,
    facet_kws={&quot;sharex&quot;: True, &quot;sharey&quot;: True},
)</code></pre>
<center>
<img src="../images/retention_files/retention_17_1.png" style="width: 1000px;"/>
</center>
<p>Finally, we plot the retention rates for each period (observation time) as a function of the cohort age (i.e. how long is the cohort):</p>
<pre class="python"><code>fig, ax = plt.subplots(figsize=(12, 7))
sns.lineplot(
    x=&quot;cohort_age&quot;,
    y=&quot;retention&quot;,
    hue=&quot;period&quot;,
    palette=&quot;viridis_r&quot;,
    alpha=0.8,
    data=train_data_df.query(&quot;cohort_age &gt; 0&quot;).assign(
        period=lambda df: df[&quot;period&quot;].dt.strftime(&quot;%Y-%m&quot;)
    ),
    ax=ax,
)
ax.legend(title=&quot;period&quot;, loc=&quot;center left&quot;, bbox_to_anchor=(1, 0.5), fontsize=7.5)
ax.set(title=&quot;Retention by Cohort and Period&quot;);</code></pre>
<center>
<img src="../images/retention_files/retention_19_1.png" style="width: 1000px;"/>
</center>
<p>Most of them have a negative trend. This is just representing the vertical columns from the retention matrix above.</p>
<p><strong>One of the main takeaways from this EDA is that the retention rate is a function of (after controlling for seasonality) the absolute cohort age and the period, with a potential interaction effect.</strong></p>
</div>
<div id="model" class="section level2">
<h2>Model</h2>
<p>We now want to model the retention rates based in the EDA above. We use a generalized linear model with binomial likelihood and a logit link function. Concretely,</p>
<p><span class="math display">\[\begin{align*}
N_{\text{active}} \sim &amp;\text{Binomial}(N_{\text{total}}, p) \\
\textrm{logit}(p) = (&amp; \text{intercept} \\
    &amp; + \beta_{\text{cohort age}} \text{cohort age} \\
    &amp; + \beta_{\text{age}} \text{age} \\
    &amp; + \beta_{\text{cohort age} \times \text{age}} \text{cohort age} \times \text{age} \\
    &amp; + \beta_{\text{seasonality}} \text{seasonality} )
\end{align*}\]</span></p>
<p>We also choose normal priors which are not very informative.</p>
<p><strong>Remarks</strong></p>
<ul>
<li><p>For the seasonality we use dummy monthly indicators. In addition, we use a <a href="https://docs.pymc.io/en/v4.2.2/api/distributions/generated/pymc.ZeroSumNormal.html"><code>ZeroSumNormal</code></a> distribution to model these terms as we re interested in the relative effect (note that we have an intercept in the model to account for the baseline).</p></li>
<li><p>For the numerical variables (<code>cohort age</code> and <code>age</code>) we scale the data.</p></li>
</ul>
<p>Before we jump into the data transformations and model fitting, let’s have a look at the data structure:</p>
<pre class="python"><code>train_data_df.head()</code></pre>
<center>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe thead th {
        text-align: left;
        font-size: 16px;
    }

    .dataframe tbody tr th {
        vertical-align: top;
        font-size: 16px;
    }
    
    .dataframe tbody tr td {
        vertical-align: top;
        font-size: 16px;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
cohort
</th>
<th>
n_users
</th>
<th>
period
</th>
<th>
age
</th>
<th>
cohort_age
</th>
<th>
retention_true_mu
</th>
<th>
retention_true
</th>
<th>
n_active_users
</th>
<th>
retention
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
2020-01-01
</td>
<td>
150
</td>
<td>
2020-01-01
</td>
<td>
1430
</td>
<td>
0
</td>
<td>
-1.807373
</td>
<td>
0.140956
</td>
<td>
150
</td>
<td>
1.000000
</td>
</tr>
<tr>
<th>
1
</th>
<td>
2020-01-01
</td>
<td>
150
</td>
<td>
2020-02-01
</td>
<td>
1430
</td>
<td>
31
</td>
<td>
-1.474736
</td>
<td>
0.186224
</td>
<td>
25
</td>
<td>
0.166667
</td>
</tr>
<tr>
<th>
2
</th>
<td>
2020-01-01
</td>
<td>
150
</td>
<td>
2020-03-01
</td>
<td>
1430
</td>
<td>
60
</td>
<td>
-2.281286
</td>
<td>
0.092685
</td>
<td>
13
</td>
<td>
0.086667
</td>
</tr>
<tr>
<th>
3
</th>
<td>
2020-01-01
</td>
<td>
150
</td>
<td>
2020-04-01
</td>
<td>
1430
</td>
<td>
91
</td>
<td>
-3.206610
</td>
<td>
0.038918
</td>
<td>
6
</td>
<td>
0.040000
</td>
</tr>
<tr>
<th>
4
</th>
<td>
2020-01-01
</td>
<td>
150
</td>
<td>
2020-05-01
</td>
<td>
1430
</td>
<td>
121
</td>
<td>
-3.112983
</td>
<td>
0.042575
</td>
<td>
2
</td>
<td>
0.013333
</td>
</tr>
</tbody>
</table>
</div>
</center>
<div id="data-transformations" class="section level3">
<h3>Data Transformations</h3>
<pre class="python"><code>train_data_red_df = train_data_df.query(&quot;cohort_age &gt; 0&quot;).reset_index(drop=True)
train_obs_idx = train_data_red_df.index.to_numpy()
train_n_users = train_data_red_df[&quot;n_users&quot;].to_numpy()
train_n_active_users = train_data_red_df[&quot;n_active_users&quot;].to_numpy()
train_retention = train_data_red_df[&quot;retention&quot;].to_numpy()

# Continuous features
train_age = train_data_red_df[&quot;age&quot;].to_numpy()
train_age_scaler = StandardScaler(with_mean=False)
train_age_scaled = train_age_scaler.fit_transform(train_age.reshape(-1, 1)).flatten()
train_cohort_age = train_data_red_df[&quot;cohort_age&quot;].to_numpy()
train_cohort_age_scaler = StandardScaler(with_mean=False)
train_cohort_age_scaled = train_cohort_age_scaler.fit_transform(
    train_cohort_age.reshape(-1, 1)
).flatten()

# Categorical features
train_cohort = train_data_red_df[&quot;cohort&quot;].to_numpy()
train_cohort_encoder = LabelEncoder()
train_cohort_idx = train_cohort_encoder.fit_transform(train_cohort).flatten()
train_period = train_data_red_df[&quot;period&quot;].to_numpy()
train_period_encoder = LabelEncoder()
train_period_idx = train_period_encoder.fit_transform(train_period).flatten()
train_period_month = train_data_red_df[&quot;period&quot;].dt.month.to_numpy()
train_period_month_encoder = LabelEncoder()
train_period_month_idx = train_period_month_encoder.fit_transform(
    train_period_month
).flatten()

coords: dict[str, npt.NDArray] = {
    &quot;period_month&quot;: train_period_month_encoder.classes_,
}
</code></pre>
<p><strong>Remark:</strong> Note that we know all features at prediction time!</p>
</div>
<div id="model-specification" class="section level3">
<h3>Model Specification</h3>
<p>The following PyMC model implements the model specification above. We use <a href="https://www.pymc.io/projects/docs/en/latest/api/generated/pymc.MutableData.html"><code>pm.MutableData</code></a> wrappers so that we can update the model with new data when generating predictions.</p>
<pre class="python"><code>with pm.Model(coords=coords) as model:
    # --- Data ---
    model.add_coord(name=&quot;obs&quot;, values=train_obs_idx, mutable=True)
    age_scaled = pm.MutableData(name=&quot;age_scaled&quot;, value=train_age_scaled, dims=&quot;obs&quot;)
    cohort_age_scaled = pm.MutableData(
        name=&quot;cohort_age_scaled&quot;, value=train_cohort_age_scaled, dims=&quot;obs&quot;
    )
    period_month_idx = pm.MutableData(
        name=&quot;period_month_idx&quot;, value=train_period_month_idx, dims=&quot;obs&quot;
    )
    n_users = pm.MutableData(name=&quot;n_users&quot;, value=train_n_users, dims=&quot;obs&quot;)
    n_active_users = pm.MutableData(
        name=&quot;n_active_users&quot;, value=train_n_active_users, dims=&quot;obs&quot;
    )

    # --- Priors ---
    intercept = pm.Normal(name=&quot;intercept&quot;, mu=0, sigma=1)
    b_age_scaled = pm.Normal(name=&quot;b_age_scaled&quot;, mu=0, sigma=1)
    b_cohort_age_scaled = pm.Normal(name=&quot;b_cohort_age_scaled&quot;, mu=0, sigma=1)
    b_period_month = pm.ZeroSumNormal(
        name=&quot;b_period_month&quot;, sigma=1, dims=&quot;period_month&quot;
    )
    b_age_cohort_age_interaction = pm.Normal(
        name=&quot;b_age_cohort_age_interaction&quot;, mu=0, sigma=1
    )

    # --- Parametrization ---
    mu = pm.Deterministic(
        name=&quot;mu&quot;,
        var=intercept
        + b_age_scaled * age_scaled
        + b_cohort_age_scaled * cohort_age_scaled
        + b_age_cohort_age_interaction * age_scaled * cohort_age_scaled
        + b_period_month[period_month_idx],
        dims=&quot;obs&quot;,
    )

    p = pm.Deterministic(name=&quot;p&quot;, var=pm.math.invlogit(mu), dims=&quot;obs&quot;)

    # --- Likelihood ---
    pm.Binomial(
        name=&quot;likelihood&quot;,
        n=n_users,
        p=p,
        observed=n_active_users,
        dims=&quot;obs&quot;,
    )

pm.model_to_graphviz(model=model)
</code></pre>
<center>
<img src="../images/retention_files/retention_27_0.svg" style="width: 1000px;"/>
</center>
</div>
<div id="model-fitting" class="section level3">
<h3>Model Fitting</h3>
<pre class="python"><code>with model:
    idata = pm.sampling_jax.sample_numpyro_nuts(
        target_accept=0.8, draws=2_000, chains=4
    )
    posterior_predictive = pm.sample_posterior_predictive(trace=idata)</code></pre>
</div>
<div id="model-diagnostics" class="section level3">
<h3>Model Diagnostics</h3>
<p>The model samples nicely and diagnostics look good:</p>
<pre class="python"><code>az.summary(data=idata, var_names=[&quot;~mu&quot;, &quot;~p&quot;])</code></pre>
<center>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe thead th {
        text-align: left;
        font-size: 16px;
    }

    .dataframe tbody tr th {
        vertical-align: top;
        font-size: 16px;
    }
    
    .dataframe tbody tr td {
        vertical-align: top;
        font-size: 16px;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
mean
</th>
<th>
sd
</th>
<th>
hdi_3%
</th>
<th>
hdi_97%
</th>
<th>
mcse_mean
</th>
<th>
mcse_sd
</th>
<th>
ess_bulk
</th>
<th>
ess_tail
</th>
<th>
r_hat
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
intercept
</th>
<td>
-1.856
</td>
<td>
0.023
</td>
<td>
-1.901
</td>
<td>
-1.814
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
5196.0
</td>
<td>
5384.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
b_age_scaled
</th>
<td>
-0.141
</td>
<td>
0.007
</td>
<td>
-0.154
</td>
<td>
-0.128
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
5786.0
</td>
<td>
5706.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
b_cohort_age_scaled
</th>
<td>
-0.568
</td>
<td>
0.026
</td>
<td>
-0.615
</td>
<td>
-0.517
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
5691.0
</td>
<td>
5961.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
b_age_cohort_age_interaction
</th>
<td>
0.075
</td>
<td>
0.006
</td>
<td>
0.064
</td>
<td>
0.086
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
5258.0
</td>
<td>
5233.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
b_period_month[1]
</th>
<td>
0.806
</td>
<td>
0.012
</td>
<td>
0.783
</td>
<td>
0.828
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
13899.0
</td>
<td>
5449.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
b_period_month[2]
</th>
<td>
1.178
</td>
<td>
0.011
</td>
<td>
1.157
</td>
<td>
1.198
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
13509.0
</td>
<td>
5804.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
b_period_month[3]
</th>
<td>
0.406
</td>
<td>
0.013
</td>
<td>
0.382
</td>
<td>
0.431
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
12560.0
</td>
<td>
6055.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
b_period_month[4]
</th>
<td>
-0.507
</td>
<td>
0.018
</td>
<td>
-0.541
</td>
<td>
-0.473
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
11474.0
</td>
<td>
5408.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
b_period_month[5]
</th>
<td>
-0.461
</td>
<td>
0.018
</td>
<td>
-0.494
</td>
<td>
-0.428
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
11166.0
</td>
<td>
5476.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
b_period_month[6]
</th>
<td>
-0.182
</td>
<td>
0.014
</td>
<td>
-0.207
</td>
<td>
-0.156
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
13638.0
</td>
<td>
5294.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
b_period_month[7]
</th>
<td>
-0.808
</td>
<td>
0.016
</td>
<td>
-0.837
</td>
<td>
-0.776
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
12110.0
</td>
<td>
5431.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
b_period_month[8]
</th>
<td>
-1.182
</td>
<td>
0.019
</td>
<td>
-1.217
</td>
<td>
-1.148
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
11723.0
</td>
<td>
5748.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
b_period_month[9]
</th>
<td>
-0.351
</td>
<td>
0.013
</td>
<td>
-0.376
</td>
<td>
-0.327
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
11346.0
</td>
<td>
5962.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
b_period_month[10]
</th>
<td>
0.511
</td>
<td>
0.009
</td>
<td>
0.493
</td>
<td>
0.529
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
12123.0
</td>
<td>
6523.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
b_period_month[11]
</th>
<td>
0.394
</td>
<td>
0.010
</td>
<td>
0.376
</td>
<td>
0.414
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
10509.0
</td>
<td>
5545.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
b_period_month[12]
</th>
<td>
0.195
</td>
<td>
0.017
</td>
<td>
0.163
</td>
<td>
0.228
</td>
<td>
0.0
</td>
<td>
0.0
</td>
<td>
8672.0
</td>
<td>
6512.0
</td>
<td>
1.0
</td>
</tr>
</tbody>
</table>
</div>
</center>
<pre class="python"><code>_ = az.plot_trace(
    data=idata,
    var_names=[&quot;~mu&quot;, &quot;~p&quot;],
    compact=True,
    backend_kwargs={&quot;figsize&quot;: (12, 10), &quot;layout&quot;: &quot;constrained&quot;},
)
plt.gcf().suptitle(&quot;Model Trace&quot;, fontsize=16);</code></pre>
<center>
<img src="../images/retention_files/retention_32_1.png" style="width: 1000px;"/>
</center>
<pre class="python"><code>ax, *_ = az.plot_forest(data=idata, var_names=[&quot;~mu&quot;, &quot;~p&quot;], combined=True)
ax.axvline(x=0, color=&quot;black&quot;, linestyle=&quot;--&quot;)
ax.set(title=&quot;Model Posterior Predictive&quot;, xlabel=&quot;Retention&quot;);</code></pre>
<center>
<img src="../images/retention_files/retention_33_1.png" style="width: 700px;"/>
</center>
<p>It is nice to see how the seasonality coefficients resemble seasonality patterns from the EDA.</p>
<p>We can now look into the posterior predictive check:</p>
<pre class="python"><code>ax = az.plot_ppc(data=posterior_predictive, kind=&quot;cumulative&quot;, observed_rug=True)
ax.set(
    title=&quot;Posterior Predictive Check&quot;,
    xscale=&quot;log&quot;,
    xlabel=&quot;likelihood (n_active_users) - log scale&quot;,
);</code></pre>
<center>
<img src="../images/retention_files/retention_36_1.png" style="width: 1000px;"/>
</center>
</div>
<div id="retention-rate-in-sample-predictions" class="section level3">
<h3>Retention Rate In-Sample Predictions</h3>
<p>Now we can use the model to predict the retention rates for the training data. We plot the observed retention rates and the predicted (posterior mean) retention rates.</p>
<pre class="python"><code>train_posterior_retention = (
    posterior_predictive.posterior_predictive / train_n_users[np.newaxis, None]
)
train_posterior_retention_mean = az.extract(
    data=train_posterior_retention, var_names=[&quot;likelihood&quot;]
).mean(&quot;sample&quot;)

fig, ax = plt.subplots(figsize=(10, 9))
sns.scatterplot(
    x=&quot;retention&quot;,
    y=&quot;posterior_retention_mean&quot;,
    data=train_data_red_df.assign(
        posterior_retention_mean=train_posterior_retention_mean
    ),
    hue=&quot;age&quot;,
    palette=&quot;viridis_r&quot;,
    size=&quot;n_users&quot;,
    ax=ax,
)
ax.axline(xy1=(0, 0), slope=1, color=&quot;black&quot;, linestyle=&quot;--&quot;, label=&quot;diagonal&quot;)
ax.legend()
ax.set(title=&quot;Posterior Predictive - Retention Mean&quot;);</code></pre>
<center>
<img src="../images/retention_files/retention_38_1.png" style="width: 900px;"/>
</center>
<p>The results look quite good 🚀 !</p>
<p>Now, we can deep dive into specific cohorts to see the predictions and uncertainty of the estimates.</p>
<pre class="python"><code>train_retention_hdi = az.hdi(ary=train_posterior_retention)[&quot;likelihood&quot;]


def plot_train_retention_hdi_cohort(cohort_index: int, ax: plt.Axes) -&gt; plt.Axes:

    mask = train_cohort_idx == cohort_index

    ax.fill_between(
        x=train_period[train_period_idx[mask]],
        y1=train_retention_hdi[mask, :][:, 0],
        y2=train_retention_hdi[mask, :][:, 1],
        alpha=0.3,
        color=&quot;C0&quot;,
        label=&quot;94% HDI (train)&quot;,
    )
    sns.lineplot(
        x=train_period[train_period_idx[mask]],
        y=train_retention[mask],
        color=&quot;C0&quot;,
        marker=&quot;o&quot;,
        label=&quot;observed retention (train)&quot;,
        ax=ax,
    )
    cohort_name = (
        pd.to_datetime(train_cohort_encoder.classes_[cohort_index]).date().isoformat()
    )
    ax.legend(loc=&quot;upper left&quot;)
    ax.set(title=f&quot;Retention HDI - Cohort {cohort_name}&quot;)
    return ax


cohort_index_to_plot = [0, 1, 5, 10, 15, 20, 25, 30]

fig, axes = plt.subplots(
    nrows=np.ceil(len(cohort_index_to_plot) / 2).astype(int),
    ncols=2,
    figsize=(15, 10),
    sharex=True,
    sharey=True,
    layout=&quot;constrained&quot;,
)

for cohort_index, ax in zip(cohort_index_to_plot, axes.flatten()):
    plot_train_retention_hdi_cohort(cohort_index=cohort_index, ax=ax)</code></pre>
<center>
<img src="../images/retention_files/retention_40_1.png" style="width: 1000px;"/>
</center>
<p>Here we also see the model is capturing the retention rate development over time. In addition we see how older cohorts have a higher uncertainty estimation as the number of base users is lower than the new cohorts.</p>
</div>
</div>
<div id="predictions" class="section level2">
<h2>Predictions</h2>
<p>Now we transform the test data to the same format as the training data and use the model to predict the retention rates. Note that we are using the scalers and encoders from the training data.</p>
<div id="data-transformations-1" class="section level3">
<h3>Data Transformations</h3>
<pre class="python"><code>test_data_red_df = test_data_df.query(&quot;cohort_age &gt; 0&quot;)
test_data_red_df = test_data_red_df[
    test_data_red_df[&quot;cohort&quot;].isin(train_data_red_df[&quot;cohort&quot;].unique())
].reset_index(drop=True)
test_obs_idx = test_data_red_df.index.to_numpy()
test_n_users = test_data_red_df[&quot;n_users&quot;].to_numpy()
test_n_active_users = test_data_red_df[&quot;n_active_users&quot;].to_numpy()
test_retention = test_data_red_df[&quot;retention&quot;].to_numpy()

# Continuous features
test_cohort = test_data_red_df[&quot;cohort&quot;].to_numpy()
test_cohort_idx = train_cohort_encoder.transform(test_cohort).flatten()
test_age = test_data_red_df[&quot;age&quot;].to_numpy()
test_age_scaled = train_age_scaler.transform(test_age.reshape(-1, 1)).flatten()
test_cohort_age = test_data_red_df[&quot;cohort_age&quot;].to_numpy()
test_cohort_age_scaled = train_cohort_age_scaler.transform(
    test_cohort_age.reshape(-1, 1)
).flatten()

# Categorical features
test_period_month = test_data_red_df[&quot;period&quot;].dt.month.to_numpy()
test_period_month_idx = train_period_month_encoder.fit_transform(
    test_period_month
).flatten()
</code></pre>
</div>
<div id="out-of-sample-posterior-predictions" class="section level3">
<h3>Out-of-Sample Posterior Predictions</h3>
<p>We now replace the model data with the test data and generate posterior predictions.</p>
<pre class="python"><code>with model:
    pm.set_data(
        new_data={
            &quot;age_scaled&quot;: test_age_scaled,
            &quot;cohort_age_scaled&quot;: test_cohort_age_scaled,
            &quot;period_month_idx&quot;: test_period_month_idx,
            &quot;n_users&quot;: test_n_users,
            &quot;n_active_users&quot;: np.ones_like(
                test_n_active_users
            ),  # Dummy data to make coords work! We are not using this at prediction time!
        },
        coords={&quot;obs&quot;: test_obs_idx},
    )
    idata.extend(
        pm.sample_posterior_predictive(
            trace=idata,
            var_names=[&quot;likelihood&quot;, &quot;p&quot;, &quot;mu&quot;],
            idata_kwargs={&quot;coords&quot;: {&quot;obs&quot;: test_obs_idx}},
        )
    )</code></pre>
</div>
<div id="retention-rate-out-of-sample-predictions" class="section level3">
<h3>Retention Rate Out-of-Sample Predictions</h3>
<p>Finally we compute the posterior retention rate distributions for the test data and visualize the results.</p>
<pre class="python"><code>test_posterior_retention = (
    idata.posterior_predictive[&quot;likelihood&quot;] / test_n_users[np.newaxis, None]
)

test_retention_hdi = az.hdi(ary=test_posterior_retention)[&quot;likelihood&quot;]</code></pre>
<pre class="python"><code>def plot_test_retention_hdi_cohort(cohort_index: int, ax: plt.Axes) -&gt; plt.Axes:
    mask = test_cohort_idx == cohort_index

    test_period_range = test_data_red_df.query(
        f&quot;cohort == &#39;{train_cohort_encoder.classes_[cohort_index]}&#39;&quot;
    )[&quot;period&quot;]

    ax.fill_between(
        x=test_period_range,
        y1=test_retention_hdi[mask, :][:, 0],
        y2=test_retention_hdi[mask, :][:, 1],
        alpha=0.3,
        color=&quot;C1&quot;,
        label=&quot;94% HDI (test)&quot;,
    )
    sns.lineplot(
        x=test_period_range,
        y=test_retention[mask],
        color=&quot;C1&quot;,
        marker=&quot;o&quot;,
        label=&quot;observed retention (test)&quot;,
        ax=ax,
    )
    return ax</code></pre>
<pre class="python"><code>cohort_index_to_plot = [0, 1, 5, 10, 15, 20, 25, 30]

fig, axes = plt.subplots(
    nrows=len(cohort_index_to_plot),
    ncols=1,
    figsize=(12, 15),
    sharex=True,
    sharey=True,
    layout=&quot;constrained&quot;,
)

for cohort_index, ax in zip(cohort_index_to_plot, axes.flatten()):
    plot_train_retention_hdi_cohort(cohort_index=cohort_index, ax=ax)
    plot_test_retention_hdi_cohort(cohort_index=cohort_index, ax=ax)
    ax.axvline(
        x=pd.to_datetime(period_train_test_split),
        color=&quot;black&quot;,
        linestyle=&quot;--&quot;,
        label=&quot;train/test split&quot;,
    )
    ax.legend(loc=&quot;center left&quot;, bbox_to_anchor=(1, 0.5))
fig.suptitle(&quot;Retention Predictions&quot;, y=1.03, fontsize=16);</code></pre>
<center>
<img src="../images/retention_files/retention_50_1.png" style="width: 1000px;"/>
</center>
<p>The out-of-sample predictions look quite good as well 🙌 !</p>
<p>For real data sets we expect this model to be a baseline for further improvements. For example, we could add more features (e.g. country, user segment, etc.) or we could use a more complex model (e.g. a hierarchical model).</p>
</div>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="../index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="../images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/dockerfile.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/bash.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="../js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'G-5NM5EDH834', 'auto');
	
	ga('send', 'pageview');
}
</script>

  </body>
</html>

