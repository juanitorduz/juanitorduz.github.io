<!DOCTYPE html>
<html lang="en-us">
  <head>
    <script defer src="https://use.fontawesome.com/releases/v5.13.0/js/all.js"></script>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.79.1" />


<title>BG/NBD Model in PyMC - Dr. Juan Camilo Orduz</title>
<meta property="og:title" content="BG/NBD Model in PyMC - Dr. Juan Camilo Orduz">


  <link href='../images/favicon.ico' rel='icon' type='image/x-icon'/>



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="../css/fonts.css" media="all">
<link rel="stylesheet" href="../css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="../" class="nav-logo">
    <img src="../images/sphere2.gif"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="../about/"> About</a></li>
    
    <li><a href="https://github.com/juanitorduz"><i class='fab fa-github fa-2x'></i>  </a></li>
    
    <li><a href="https://www.linkedin.com/in/juanitorduz/"><i class='fab fa-linkedin fa-2x' style='color:#0077B5;'></i>  </a></li>
    
    <li><a href="https://twitter.com/juanitorduz"><i class='fab fa-twitter fa-2x' style='color:#1DA1F2;'></i>  </a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">20 min read</span>
    

    <h1 class="article-title">BG/NBD Model in PyMC</h1>

    
    <span class="article-date">2022-03-03</span>
    

    <div class="article-content">
      


<p>In this notebook we show how to port the BG/NBD model from the the <a href="https://github.com/CamDavidsonPilon/lifetimes"><code>lifetimes</code></a> (developed mainly by <a href="https://github.com/CamDavidsonPilon">Cameron Davidson-Pilon</a>) package to <a href="https://github.com/pymc-devs/pymc"><code>pymc</code></a>. The BG/NBD model, introduced in the seminal paper <a href="http://brucehardie.com/papers/018/fader_et_al_mksc_05.pdf">“Counting Your Customers” the Easy Way: An Alternative to the Pareto/NBD Model</a> by Peter S. Fader, Bruce G. S. Hardie and Ka Lok Lee in 2005, is used to</p>
<blockquote>
<p><em>predict future purchasing patterns, which can then serve as an input into “lifetime value” calculations, in the “non-contractual” setting (i.e., where the opportunities for transactions are continuous and the time at which customers become inactive is unobserved).</em></p>
</blockquote>
<p>Why to port the BG/NBD model to <a href="https://github.com/pymc-devs/pymc"><code>pymc</code></a>?</p>
<ol style="list-style-type: decimal">
<li>The <a href="https://github.com/CamDavidsonPilon/lifetimes"><code>lifetimes</code></a> package is currently in “maintenance-mode”, so we do not expect it to be further developed. In addition, I am not aware of other python implementation (the R community has more options available)</li>
<li>This current implementation does not allow for time-invariant external covariates even though is an easy extension an shown in the paper <a href="http://brucehardie.com/notes/019/time_invariant_covariates.pdf">Incorporating Time-Invariant Covariates into the Pareto/NBD and BG/NBD Models</a> by Peter S. Fader and Bruce G. S. Hardie. There is actually an <a href="https://github.com/CamDavidsonPilon/lifetimes/pull/342">open PR</a> to att his feature into the <a href="https://github.com/CamDavidsonPilon/lifetimes"><code>lifetimes</code></a> package, but it is highly unlikely to be merged. Hence, as in practice time-invariant covariates could further segment user purchase patterns, writing this model explicitly can allow for more flexibility.</li>
<li>We can take advantage of the powerful bayesian machinery to have better uncertainty estimation and the possibility to use extend the base BG/NBD model to more complex ones, e.g. hierarchical models.</li>
</ol>
<p><strong>Remark:</strong> The content of this post was presented at the <a href="https://www.meetup.com/berlinbayesians/">Berlin Bayesians MeetUp</a> in <a href="https://www.meetup.com/berlinbayesians/events/288023821/">September 2022</a>. <a href="../html/berlin_bayesians_btyd.html">Here</a> you can find the slides.</p>
<div id="prepare-notebook" class="section level2">
<h2>Prepare Notebook</h2>
<pre class="python"><code>import arviz as az
import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
import seaborn as sns
from lifetimes.datasets import load_cdnow_summary
from lifetimes import BetaGeoFitter
import pymc as pm
import pymc.sampling_jax
from scipy.special import expit, hyp2f1
import aesara.tensor as at

plt.style.use(&quot;bmh&quot;)
plt.rcParams[&quot;figure.figsize&quot;] = [10, 6]
plt.rcParams[&quot;figure.dpi&quot;] = 100
plt.rcParams[&quot;figure.facecolor&quot;] = &quot;white&quot;

seed = sum(map(ord, &quot;juanitorduz&quot;))
rng = np.random.default_rng(seed)

%load_ext rich
%load_ext autoreload
%autoreload 2
%config InlineBackend.figure_format = &quot;retina&quot;</code></pre>
</div>
<div id="load-data" class="section level2">
<h2>Load Data</h2>
<p>We are going to use an existing data set from the <a href="https://github.com/CamDavidsonPilon/lifetimes"><code>lifetimes</code></a> package documentation, see <a href="https://lifetimes.readthedocs.io/en/latest/Quickstart.html">here</a>.</p>
<pre class="python"><code>data_df = load_cdnow_summary(index_col=[0])

n_obs = data_df.shape[0]

data_df.info()</code></pre>
<pre><code>&lt;class &#39;pandas.core.frame.DataFrame&#39;&gt;
Int64Index: 2357 entries, 1 to 2357
Data columns (total 3 columns):
 #   Column     Non-Null Count  Dtype  
---  ------     --------------  -----  
 0   frequency  2357 non-null   int64  
 1   recency    2357 non-null   float64
 2   T          2357 non-null   float64
dtypes: float64(2), int64(1)
memory usage: 73.7 KB</code></pre>
<p>From the package’s documentation:</p>
<blockquote>
<ul>
<li><code>frequency</code>: Number of repeat purchases the customer has made. More precisely, It’s the count of time periods the customer had a purchase in.</li>
<li><code>T</code>: Age of the customer in whatever time units chosen (weekly, in the above dataset). This is equal to the duration between a customer’s first purchase and the end of the period under study.</li>
<li><code>recency</code>: Age of the customer when they made their most recent purchases. This is equal to the duration between a customer’s first purchase and their latest purchase.</li>
</ul>
</blockquote>
<pre class="python"><code>data_df.head(10)</code></pre>
<center>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
frequency
</th>
<th>
recency
</th>
<th>
T
</th>
</tr>
<tr>
<th>
ID
</th>
<th>
</th>
<th>
</th>
<th>
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
1
</th>
<td>
2
</td>
<td>
30.43
</td>
<td>
38.86
</td>
</tr>
<tr>
<th>
2
</th>
<td>
1
</td>
<td>
1.71
</td>
<td>
38.86
</td>
</tr>
<tr>
<th>
3
</th>
<td>
0
</td>
<td>
0.00
</td>
<td>
38.86
</td>
</tr>
<tr>
<th>
4
</th>
<td>
0
</td>
<td>
0.00
</td>
<td>
38.86
</td>
</tr>
<tr>
<th>
5
</th>
<td>
0
</td>
<td>
0.00
</td>
<td>
38.86
</td>
</tr>
<tr>
<th>
6
</th>
<td>
7
</td>
<td>
29.43
</td>
<td>
38.86
</td>
</tr>
<tr>
<th>
7
</th>
<td>
1
</td>
<td>
5.00
</td>
<td>
38.86
</td>
</tr>
<tr>
<th>
8
</th>
<td>
0
</td>
<td>
0.00
</td>
<td>
38.86
</td>
</tr>
<tr>
<th>
9
</th>
<td>
2
</td>
<td>
35.71
</td>
<td>
38.86
</td>
</tr>
<tr>
<th>
10
</th>
<td>
0
</td>
<td>
0.00
</td>
<td>
38.86
</td>
</tr>
</tbody>
</table>
</div>
</center>
<p>Let us extract the data as arrays and recover the notation from the original papers.</p>
<pre class="python"><code>n = data_df.shape[0]
x = data_df[&quot;frequency&quot;].to_numpy()
t_x = data_df[&quot;recency&quot;].to_numpy()
T = data_df[&quot;T&quot;].to_numpy()

# convenient indicator function
int_vec = np.vectorize(int)
x_zero = int_vec(x &gt; 0)</code></pre>
</div>
<div id="bgnbd-model-lifetimes" class="section level2">
<h2>BG/NBD Model (Lifetimes)</h2>
<p>We do not want to give a detailed description of the model, so we will just show the notation. Please refer to the original papers for more details, e.g. <a href="http://brucehardie.com/papers/018/fader_et_al_mksc_05.pdf">“Counting Your Customers” the Easy Way: An Alternative to the Pareto/NBD Model</a>.</p>
<center>
<img src="../images/bg_nbd_model_description.png" height=800 />
</center>
<p>We now use the <a href="https://github.com/CamDavidsonPilon/lifetimes"><code>lifetimes</code></a> object <a href="https://lifetimes.readthedocs.io/en/latest/lifetimes.fitters.html#module-lifetimes.fitters.beta_geo_fitter"><code>BetaGeoFitter</code></a> to estimate the model parameters via maximum likelihood estimation.</p>
<pre class="python"><code># fit BG/NBD model
bgf = BetaGeoFitter()
bgf.fit(frequency=x, recency=t_x, T=T)

bgf.summary</code></pre>
<center>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
coef
</th>
<th>
se(coef)
</th>
<th>
lower 95% bound
</th>
<th>
upper 95% bound
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
r
</th>
<td>
0.242593
</td>
<td>
0.012557
</td>
<td>
0.217981
</td>
<td>
0.267205
</td>
</tr>
<tr>
<th>
alpha
</th>
<td>
4.413532
</td>
<td>
0.378221
</td>
<td>
3.672218
</td>
<td>
5.154846
</td>
</tr>
<tr>
<th>
a
</th>
<td>
0.792886
</td>
<td>
0.185719
</td>
<td>
0.428877
</td>
<td>
1.156895
</td>
</tr>
<tr>
<th>
b
</th>
<td>
2.425752
</td>
<td>
0.705345
</td>
<td>
1.043276
</td>
<td>
3.808229
</td>
</tr>
</tbody>
</table>
</div>
</center>
<p>Note that the models provide confidence intervals for the estimated parameters. These are estimated using by using the Hessian to calculate the variance matrix, see <a href="https://stats.stackexchange.com/questions/381984/standard-error-from-hessian-matrix-when-likelihood-is-used-rather-than-ln-l">Standard error from Hessian matrix when likelihood is used (rather than Ln L)</a>.</p>
<div id="full-bayesian-model" class="section level3">
<h3>Full Bayesian Model</h3>
<p>A nice <code>pymc</code> implementation of the full bayesian model can be found in the blog post <a href="https://sidravi1.github.io/blog/2018/07/08/fader-hardie-clv">Implementing Fader Hardie (2005) in pymc3</a> by <a href="https://sidravi1.github.io/about.html">Sid Ravinutala</a>. In this post, the author uses the complete likelihood function (Equation (3) in <a href="http://brucehardie.com/papers/018/fader_et_al_mksc_05.pdf">“Counting Your Customers” the Easy Way: An Alternative to the Pareto/NBD Model</a>):</p>
<p><span class="math display">\[
L(\lambda, p | X=x, T) = (1 - p)^{x}\lambda^{x}e^{-\lambda T} + \delta_{X &gt; 0}p(1 - p)^{x - 1}\lambda^x e^{-\lambda t_x}
\]</span></p>
<p>Here is the model in <code>pymc</code>:</p>
<pre class="python"><code>with pm.Model() as model_full:

    # hyper priors for the Gamma params    
    a = pm.HalfNormal(name=&quot;a&quot;, sigma=10)
    b = pm.HalfNormal(name=&quot;b&quot;, sigma=10)

    # hyper priors for the Beta params
    alpha = pm.HalfNormal(name=&quot;alpha&quot;, sigma=10)
    r = pm.HalfNormal(name=&quot;r&quot;, sigma=10)

    lam = pm.Gamma(name=&quot;lam&quot;, alpha=r, beta=alpha, shape=n)
    p = pm.Beta(name=&quot;p&quot;, alpha=a, beta=b, shape=n)

    def logp(x, t_x, T, x_zero):
        log_term_a = x * at.log(1 - p) + x * at.log(lam) - t_x * lam
        term_b_1 = -lam * (T - t_x)
        term_b_2 = at.log(p) - at.log(1 - p)
        log_term_b = pm.math.switch(x_zero, pm.math.logaddexp(term_b_1, term_b_2), term_b_1)

        return at.sum(log_term_a) + at.sum(log_term_b)

    likelihood = pm.Potential(
        name=&quot;likelihood&quot;,
        var=logp(x=x, t_x=t_x, T=T, x_zero=x_zero),
    )

pm.model_to_graphviz(model=model_full)</code></pre>
<center>
<img src="../images/bg_nbd_pymc_files/bg_nbd_pymc_13_0.svg" style="width: 800px;"/>
</center>
<pre class="python"><code>with model_full:
    trace_full = pm.sampling_jax.sample_numpyro_nuts(
        tune=3000,
        draws=6000,
        chains=4,
        target_accept=0.95,
    )</code></pre>
<p>This model takes a while to run and the results coincide with the ones from obtained using <code>lifetimes</code>.</p>
<pre class="python"><code>axes = az.plot_trace(
    data=trace_full,
    var_names=[&quot;a&quot;, &quot;b&quot;, &quot;alpha&quot;, &quot;r&quot;],
    lines=[
        (k, {}, [v.to_numpy()])
        for k, v in bgf.summary[
            [&quot;coef&quot;, &quot;lower 95% bound&quot;, &quot;upper 95% bound&quot;]
        ].T.items()
    ],
    compact=True,
    backend_kwargs={&quot;figsize&quot;: (12, 9), &quot;layout&quot;: &quot;constrained&quot;},
)
fig = axes[0][0].get_figure()
fig.suptitle(&quot;Full BG/NBD Model Trace&quot;, fontsize=16)
</code></pre>
<center>
<img src="../images/bg_nbd_pymc_files/bg_nbd_pymc_16_0.png" style="width: 1000px;"/>
</center>
<pre class="python"><code>axes = az.plot_pair(data=trace_full, var_names=[&quot;a&quot;, &quot;b&quot;, &quot;alpha&quot;, &quot;r&quot;], figsize=(12, 12))
fig = axes[0][0].get_figure()
fig.suptitle(&quot;Full BG/NBD Model Parameters Pairplot&quot;, y=0.95, fontsize=16);</code></pre>
<center>
<img src="../images/bg_nbd_pymc_files/bg_nbd_pymc_17_0.png" style="width: 800px;"/>
</center>
<p>The chains for <code>a</code> and <code>b</code> do not look so good. We actually see from the pairplot that these parameters highly correlated. See the <code>r_hat</code> values below.</p>
<pre class="python"><code>az.summary(data=trace_full, var_names=[&quot;a&quot;, &quot;b&quot;, &quot;alpha&quot;, &quot;r&quot;])</code></pre>
<center>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
mean
</th>
<th>
sd
</th>
<th>
hdi_3%
</th>
<th>
hdi_97%
</th>
<th>
mcse_mean
</th>
<th>
mcse_sd
</th>
<th>
ess_bulk
</th>
<th>
ess_tail
</th>
<th>
r_hat
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
a
</th>
<td>
0.949
</td>
<td>
0.284
</td>
<td>
0.513
</td>
<td>
1.399
</td>
<td>
0.026
</td>
<td>
0.018
</td>
<td>
108.0
</td>
<td>
214.0
</td>
<td>
1.03
</td>
</tr>
<tr>
<th>
b
</th>
<td>
3.095
</td>
<td>
1.202
</td>
<td>
1.437
</td>
<td>
5.053
</td>
<td>
0.103
</td>
<td>
0.073
</td>
<td>
118.0
</td>
<td>
287.0
</td>
<td>
1.02
</td>
</tr>
<tr>
<th>
alpha
</th>
<td>
4.467
</td>
<td>
0.384
</td>
<td>
3.757
</td>
<td>
5.196
</td>
<td>
0.008
</td>
<td>
0.005
</td>
<td>
2608.0
</td>
<td>
6504.0
</td>
<td>
1.00
</td>
</tr>
<tr>
<th>
r
</th>
<td>
0.244
</td>
<td>
0.013
</td>
<td>
0.219
</td>
<td>
0.267
</td>
<td>
0.000
</td>
<td>
0.000
</td>
<td>
1913.0
</td>
<td>
4427.0
</td>
<td>
1.00
</td>
</tr>
</tbody>
</table>
</div>
</center>
</div>
</div>
<div id="bayesian-model-randomly-chosen-individual" class="section level2">
<h2>Bayesian Model: Randomly Chosen Individual</h2>
<p>One drawback (computationally) of the <code>model_full</code> is that we have <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(p\)</span> parameters per user. This does not scale that easily. That is why in practice one usually takes the expectation over <span class="math inline">\(\lambda\)</span> and <span class="math inline">\(p\)</span> to study a randomly chosen individual. One can compute this expectation analytically, see Section <span class="math inline">\(5\)</span> in <a href="http://brucehardie.com/papers/018/fader_et_al_mksc_05.pdf">“Counting Your Customers” the Easy Way: An Alternative to the Pareto/NBD Model</a> for the mathematical details. What is important to remark is this the end expression for the log-likelihood is relatively easy to write. Actually, the authors of the paper have a document which describes how to estimate the corresponding parameters in Excel, see <a href="http://brucehardie.com/notes/004/bgnbd_spreadsheet_note.pdf">Implementing the BG/NBD Model for Customer Base Analysis in Excel</a>. The resulting expression for the likelihood function is:</p>
<p><span class="math display">\[
L(a, b, \alpha, r|X=x, t_x, T) = A_{1}A_{2}(A_{3} + \delta_{x&gt;0}A_4)
\]</span></p>
<p>where</p>
<p><span class="math display">\[\begin{align*}
A_{1} &amp; = \frac{\Gamma(r + x)\alpha^{{r}}}{\Gamma(x)} \\
A_{2} &amp; = \frac{\Gamma(a + b)\Gamma(b + x)}{\Gamma(b)\Gamma(a + b + x)} \\
A_{3} &amp; = \left(\frac{1}{\alpha + T}\right)^{r+x} \\
A_{4} &amp; = \left(\frac{a}{b + x - 1}\right)\left(\frac{1}{\alpha + t_x}\right)^{r + x}
\end{align*}\]</span></p>
<p>Computing the <span class="math inline">\(\log (L(a, b, \alpha, r|X=x, t_x, T))\)</span> is straight forward from these explicit expressions <span class="math inline">\(a_{i} := \log(A_{i})\)</span>. Note however that one has to be careful with the <span class="math inline">\(a_{4} := \log(A_4)\)</span> term since we need to ensure that <span class="math inline">\(b + x - 1 &gt;0\)</span> (otherwise should be zero). Fortunately, we wan simple re-factor the log-likelihood method code of th <code>lifetimes</code>’s <code>BetaGeoFitter</code> class from <code>numpy</code> to <code>theano</code> (see <a href="https://github.com/CamDavidsonPilon/lifetimes/blob/master/lifetimes/fitters/beta_geo_fitter.py#L164"><code>lifetimes.BetaGeoFitter._negative_log_likelihood</code></a>).</p>
<pre class="python"><code>with pm.Model() as model:
   
    a = pm.HalfNormal(name=&quot;a&quot;, sigma=10)
    b = pm.HalfNormal(name=&quot;b&quot;, sigma=10)

    alpha = pm.HalfNormal(name=&quot;alpha&quot;, sigma=10)
    r = pm.HalfNormal(name=&quot;r&quot;, sigma=10)

    def logp(x, t_x, T, x_zero):
        a1 = at.gammaln(r + x) - at.gammaln(r) + r * at.log(alpha)
        a2 = at.gammaln(a + b) + at.gammaln(b + x) - at.gammaln(b) - at.gammaln(a + b + x)
        a3 = -(r + x) * at.log(alpha + T)
        a4 =  at.log(a) - at.log(b + at.maximum(x, 1) - 1) - (r + x) * at.log(t_x + alpha)
        max_a3_a4 = at.maximum(a3, a4)
        ll_1 = a1 + a2 
        ll_2 = at.log(at.exp(a3 - max_a3_a4) + at.exp(a4 - max_a3_a4) * pm.math.switch(x_zero, 1, 0)) + max_a3_a4
        return at.sum(ll_1 + ll_2)

    likelihood = pm.Potential(
        name=&quot;likelihood&quot;,
        var=logp(x=x, t_x=t_x, T=T, x_zero=x_zero),
    )

pm.model_to_graphviz(model=model)</code></pre>
<center>
<img src="../images/bg_nbd_pymc_files/bg_nbd_pymc_21_0.svg" style="width: 800px;"/>
</center>
<p>Let us compute the posterior distributions:</p>
<pre class="python"><code>with model:
    trace = pm.sampling_jax.sample_numpyro_nuts(
        tune=3000,
        draws=6000,
        chains=4,
        target_accept=0.95,
    )</code></pre>
<p>This model is much faster to train! We can now take a look at the traces, pairplots and summary statistics.</p>
<pre class="python"><code>axes = az.plot_trace(
    data=trace,
    lines=[
        (k, {}, [v.to_numpy()])
        for k, v in bgf.summary[
            [&quot;coef&quot;, &quot;lower 95% bound&quot;, &quot;upper 95% bound&quot;]
        ].T.items()
    ],
    compact=True,
    backend_kwargs={&quot;figsize&quot;: (12, 9), &quot;layout&quot;: &quot;constrained&quot;},
)
fig = axes[0][0].get_figure()
fig.suptitle(&quot;BG/NBD Model Trace&quot;, fontsize=16)
</code></pre>
<center>
<img src="../images/bg_nbd_pymc_files/bg_nbd_pymc_25_0.png" style="width: 1000px;"/>
</center>
<pre class="python"><code>axes = az.plot_pair(data=trace, figsize=(12, 12))
fig = axes[0][0].get_figure()
fig.suptitle(&quot;BG/NBD Model Parameters Pairplot&quot;, y=0.95, fontsize=16);</code></pre>
<center>
<img src="../images/bg_nbd_pymc_files/bg_nbd_pymc_26_0.png" style="width: 800px;"/>
</center>
<pre class="python"><code>az.summary(data=trace)</code></pre>
<center>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
mean
</th>
<th>
sd
</th>
<th>
hdi_3%
</th>
<th>
hdi_97%
</th>
<th>
mcse_mean
</th>
<th>
mcse_sd
</th>
<th>
ess_bulk
</th>
<th>
ess_tail
</th>
<th>
r_hat
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
a
</th>
<td>
0.969
</td>
<td>
0.279
</td>
<td>
0.541
</td>
<td>
1.490
</td>
<td>
0.003
</td>
<td>
0.002
</td>
<td>
9063.0
</td>
<td>
9408.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
b
</th>
<td>
3.170
</td>
<td>
1.150
</td>
<td>
1.423
</td>
<td>
5.242
</td>
<td>
0.013
</td>
<td>
0.010
</td>
<td>
9134.0
</td>
<td>
9060.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
alpha
</th>
<td>
4.472
</td>
<td>
0.382
</td>
<td>
3.757
</td>
<td>
5.184
</td>
<td>
0.004
</td>
<td>
0.003
</td>
<td>
9292.0
</td>
<td>
10149.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
r
</th>
<td>
0.244
</td>
<td>
0.012
</td>
<td>
0.220
</td>
<td>
0.267
</td>
<td>
0.000
</td>
<td>
0.000
</td>
<td>
9521.0
</td>
<td>
11044.0
</td>
<td>
1.0
</td>
</tr>
</tbody>
</table>
</div>
</center>
<p>The summary statistics look overall good. In particular the chains values for <code>a</code> and <code>b</code> look better than the <code>model_full</code>.</p>
<div id="probability-of-being-alive" class="section level3">
<h3>Probability of being alive</h3>
<p>Now that we have posterior samples for the model parameters we can easily compute quantities of interest. For example, let us compute the <em>conditional probability of being alive</em>, see the note <a href="https://brucehardie.com/notes/021/palive_for_BGNBD.pdf">Computing P(alive) Using
the BG/NBD Model</a>. Again, we can simply port the corresponding method from <a href="https://lifetimes.readthedocs.io/en/latest/lifetimes.fitters.html#lifetimes.fitters.beta_geo_fitter.BetaGeoFitter.conditional_probability_alive"><code>lifetimes.BetaGeoFitter.conditional_probability_alive</code></a>.</p>
<pre class="python"><code># See https://docs.pymc.io/en/stable/pymc-examples/examples/time_series/Air_passengers-Prophet_with_Bayesian_workflow.html
def _sample(array, n_samples):
    &quot;&quot;&quot;Little utility function, sample n_samples with replacement.&quot;&quot;&quot;
    idx = np.random.choice(np.arange(len(array)), n_samples, replace=True)
    return array[idx]


def conditional_probability_alive(trace, x, t_x, T):
    n_vals = x.shape[0]

    a = _sample(array=trace.posterior[&quot;a&quot;].to_numpy(), n_samples=n_vals)
    b = _sample(array=trace.posterior[&quot;b&quot;].to_numpy(), n_samples=n_vals)
    alpha = _sample(array=trace.posterior[&quot;alpha&quot;].to_numpy(), n_samples=n_vals)
    r = _sample(array=trace.posterior[&quot;r&quot;].to_numpy(), n_samples=n_vals)

    log_div = (
        (r + x[..., None]) * np.log((alpha + T[..., None]) / (alpha + t_x[..., None]))
        + np.log(a / (b + np.maximum(x[..., None], 1) - 1))
    )
    return np.where(x[..., None] == 0, 1.0, expit(-log_div))


p_alive_sample = conditional_probability_alive(trace, x, t_x, T)</code></pre>
<p>We compute these probabilities directly from the <code>BetaGeoFitter</code> object directly so that we could compare the results.</p>
<pre class="python"><code>p_alive = bgf.conditional_probability_alive(x, t_x, T)</code></pre>
<p>We can simply plot some example predictions:</p>
<pre class="python"><code>def plot_conditional_probability_alive(p_alive_sample, p_alive, idx, ax):
    sns.kdeplot(x=p_alive_sample[idx], color=&quot;C0&quot;, fill=True, ax=ax)
    ax.axvline(x=p_alive[idx], color=&quot;C1&quot;, linestyle=&quot;--&quot;)
    ax.set(title=f&quot;idx={idx}&quot;)
    return ax

fig, axes = plt.subplots(
    nrows=3,
    ncols=3,
    figsize=(9, 9),
    layout=&quot;constrained&quot;
)
for idx, ax in enumerate(axes.flatten()):
    plot_conditional_probability_alive(p_alive_sample, p_alive, idx, ax)

fig.suptitle(&quot;Conditional Probability Alive&quot;, fontsize=16);</code></pre>
<center>
<img src="../images/bg_nbd_pymc_files/bg_nbd_pymc_34_1.png" style="width: 800px;"/>
</center>
<p>The plots without density correspond to users with probability of being active of <span class="math inline">\(1.0\)</span>. Note for example that for <code>idx=2</code> we have:</p>
<pre class="python"><code>idx = 2
# probability estimation lifetimes model
p_alive[idx]</code></pre>
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1.0</span>
</pre>
<pre class="python"><code># posterior samples
np.unique(p_alive_sample[idx])</code></pre>
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace"><span style="color: #800080; text-decoration-color: #800080; font-weight: bold">array</span><span style="font-weight: bold">([</span><span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1</span>.<span style="font-weight: bold">])</span>
</pre>
<p>Hence, the bayesian model also captures the case when the user is certainly alive (because did a very recent purchase).</p>
</div>
<div id="predicting-the-number-of-future-transactions" class="section level3">
<h3>Predicting the number of future transactions</h3>
<p>We can do something similar for the expected number of future transactions using the analytical expression implemented in the method <a href="https://lifetimes.readthedocs.io/en/latest/lifetimes.fitters.html#lifetimes.fitters.beta_geo_fitter.BetaGeoFitter.conditional_expected_number_of_purchases_up_to_time"><code>lifetimes.BetaGeoFitter. conditional_expected_number_of_purchases_up_to_time</code></a>.</p>
<pre class="python"><code>def conditional_expected_number_of_purchases_up_to_time(t, trace, x, t_x, T):
    n_vals = x.shape[0]
    a = _sample(array=trace.posterior[&quot;a&quot;].to_numpy(), n_samples=n_vals)
    b = _sample(array=trace.posterior[&quot;b&quot;].to_numpy(), n_samples=n_vals)
    alpha = _sample(array=trace.posterior[&quot;alpha&quot;].to_numpy(), n_samples=n_vals)
    r = _sample(array=trace.posterior[&quot;r&quot;].to_numpy(), n_samples=n_vals)

    _a = r + x[..., None]
    _b = b + x[..., None]
    _c = a + b + x[..., None] - 1
    _z = t / (alpha + T[..., None] + t)
    ln_hyp_term = np.log(hyp2f1(_a, _b, _c, _z))

    ln_hyp_term_alt = np.log(hyp2f1(_c - _a, _c - _b, _c, _z)) + (
        _c - _a - _b
    ) * np.log(1 - _z)
    ln_hyp_term = np.where(np.isinf(ln_hyp_term), ln_hyp_term_alt, ln_hyp_term)
    first_term = (a + b + x[..., None] - 1) / (a - 1)
    second_term = 1 - np.exp(
        ln_hyp_term
        + (r + x[..., None])
        * np.log((alpha + T[..., None]) / (alpha + t + T[..., None]))
    )

    numerator = first_term * second_term
    denominator = 1 + (x &gt; 0)[..., None] * (a / (b + x[..., None] - 1)) * (
        (alpha + T[..., None]) / (alpha + t_x[..., None])
    ) ** (r + x[..., None])

    return numerator / denominator


# generate posterior samples
t = 90
conditional_expected_number_of_purchases_up_to_time_samples = (
    conditional_expected_number_of_purchases_up_to_time(t, trace, x, t_x, T)
)</code></pre>
<p>We can now compare it with the predictions from the maximum likelihood model.</p>
<pre class="python"><code>conditional_expected_number_of_purchases_up_to_time = (
    bgf.conditional_expected_number_of_purchases_up_to_time(t, x, t_x, T)
)

idx_sort = np.argsort(conditional_expected_number_of_purchases_up_to_time)

fig, ax = plt.subplots(figsize=(9, 8))


az.plot_hdi(
    x=conditional_expected_number_of_purchases_up_to_time[idx_sort],
    y=conditional_expected_number_of_purchases_up_to_time_samples[idx_sort, :].T,
    hdi_prob=0.94,
    color=&quot;C0&quot;,
    fill_kwargs={&quot;alpha&quot;: 0.15, &quot;label&quot;: r&quot;$94\%$ HDI&quot;},
    ax=ax,
)

az.plot_hdi(
    x=conditional_expected_number_of_purchases_up_to_time[idx_sort],
    y=conditional_expected_number_of_purchases_up_to_time_samples[idx_sort, :].T,
    hdi_prob=0.5,
    color=&quot;C0&quot;,
    fill_kwargs={&quot;alpha&quot;: 0.3, &quot;label&quot;: r&quot;$50\%$ HDI&quot;},
    ax=ax,
)

sns.lineplot(
    x=conditional_expected_number_of_purchases_up_to_time,
    y=conditional_expected_number_of_purchases_up_to_time_samples.mean(axis=1),
    color=&quot;C0&quot;,
    label=&quot;posterior predictive mean&quot;,
    ax=ax,
)

sns.lineplot(
    x=conditional_expected_number_of_purchases_up_to_time,
    y=conditional_expected_number_of_purchases_up_to_time,
    color=&quot;C1&quot;,
    label=&quot;maximum likelihood estimate&quot;,
    ax=ax,
)

sns.rugplot(
    x=conditional_expected_number_of_purchases_up_to_time,
    color=&quot;C1&quot;,
    lw=1,
    alpha=0.3,
    ax=ax,
)

ax.legend(loc=&quot;upper left&quot;)
ax.set(
    title=f&quot;Conditional Expected Number of Purchases Up to Time: {t} days&quot;,
    xlabel=&quot;Expected Number of Purchases (BetaGeoFitter)&quot;,
    ylabel=&quot;Expected Number of Purchases (BetaGeoFitter - PyMC)&quot;
);</code></pre>
<center>
<img src="../images/bg_nbd_pymc_files/bg_nbd_pymc_42_0.png" style="width: 800px;"/>
</center>
<p><strong>Remark:</strong> We can compare both model’s parameters:</p>
<pre class="python"><code>axes = az.plot_forest(
    data=[trace_full, trace],
    model_names=[&quot;full_model&quot;, &quot;bg_nbd_model&quot;],
    var_names=[&quot;a&quot;, &quot;b&quot;, &quot;alpha&quot;, &quot;r&quot;],
    combined=True,
    r_hat=True,
    ess=True,
    figsize=(10, 9)
)
plt.gcf().suptitle(&quot;Model Comparison&quot;, y=0.95, fontsize=16);</code></pre>
<center>
<img src="../images/bg_nbd_pymc_files/bg_nbd_pymc_44_0.png" style="width: 800px;"/>
</center>
<p>We indeed see that the <code>full_model</code> has a lower <a href="https://arviz-devs.github.io/arviz/api/generated/arviz.ess.html"><code>ess</code></a>.</p>
</div>
</div>
<div id="bayesian-model-randomly-chosen-individual-with-time-independent-covariates" class="section level2">
<h2>Bayesian Model: Randomly Chosen Individual with Time-Independent Covariates</h2>
<p>In the research note <a href="http://brucehardie.com/notes/019/time_invariant_covariates.pdf">Incorporating Time-Invariant Covariates into the Pareto/NBD and BG/NBD Models</a> by Peter S. Fader and Bruce G. S. Hardie, it is shown how easy (after some clever math manipulations of course!) to incorporate time-invariant covariates into the BG/NBD model. The main idea is to allow covariates <span class="math inline">\(z_1\)</span> and <span class="math inline">\(z_2\)</span> to explain the cross-sectional heterogeneity in the purchasing process and cross-sectional heterogeneity in the dropout process respectively (see model description above). The authors show that the likelihood and quantities of interests computed by computing expectations (e.g. quantities which have a close expression so that, for example can be computed in Excel), remain almost the same. One only has to replace:</p>
<p><span class="math display">\[\begin{align*}
\alpha &amp; \longmapsto \alpha_{0}\exp(-\gamma_{1}^{T}z_{1}) \\
a &amp; \longmapsto a_{0}\exp(\gamma_{2}^{T}z_{2}) \\
b &amp; \longmapsto b_{0}\exp(\gamma_{3}^{T}z_{2})
\end{align*}\]</span></p>
<p>where <span class="math inline">\(\gamma_{i}\)</span> are the model coefficients.</p>
<p>Let us modify the frequency of our data set by reducing it as a function of a covariate <span class="math inline">\(z\)</span>.</p>
<pre class="python"><code>np.random.seed(42)

# construct covariate
mu = 0.4
rho = 0.7
z = np.random.binomial(n=1, p=mu, size=x.size)
# change frequency values by reducing it the values where z !=0
x_z = np.floor(x * (1 - (rho * z)))
# make sure the recency is zero whenever the frequency is zero
t_x_z = t_x.copy()
t_x_z[np.argwhere(x_z == 0).flatten()] = 0
# sanity checks
assert x_z.min() == 0
assert np.all(t_x_z[np.argwhere(x_z == 0).flatten()] == 0)
# convenient indicator function
int_vec = np.vectorize(int)
x_zero_z = int_vec(x_z &gt; 0)</code></pre>
<p>First, we fit the <code>lifetimes</code> model with this modified data.</p>
<pre class="python"><code>bgf_cov = BetaGeoFitter()
bgf_cov.fit(frequency=x_z, recency=t_x_z, T=T)

bgf_cov.summary</code></pre>
<center>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
coef
</th>
<th>
se(coef)
</th>
<th>
lower 95% bound
</th>
<th>
upper 95% bound
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
r
</th>
<td>
0.142129
</td>
<td>
0.008739
</td>
<td>
0.125000
</td>
<td>
0.159258
</td>
</tr>
<tr>
<th>
alpha
</th>
<td>
4.252136
</td>
<td>
0.447587
</td>
<td>
3.374866
</td>
<td>
5.129407
</td>
</tr>
<tr>
<th>
a
</th>
<td>
0.750734
</td>
<td>
0.283204
</td>
<td>
0.195654
</td>
<td>
1.305815
</td>
</tr>
<tr>
<th>
b
</th>
<td>
3.157024
</td>
<td>
1.505522
</td>
<td>
0.206201
</td>
<td>
6.107846
</td>
</tr>
</tbody>
</table>
</div>
</center>
<p>Next, we write the bayesian model, which adds term <span class="math inline">\(\alpha \longmapsto \alpha_{0}\exp(-\gamma_{1}^{T}z_{1})\)</span> to model the effect of <span class="math inline">\(z\)</span> on the to explain the cross-sectional heterogeneity in the purchasing process (because we are modifying the frequency as a function of <span class="math inline">\(z\)</span>).</p>
<p><strong>Remark:</strong> I also tried adding the covariate <span class="math inline">\(z_2 = z\)</span>, but the resulting model had trouble sampling. By looking into the pair-plots of the posterior distributions (which had many divergences) I found that the terms <span class="math inline">\(a_0\)</span> and <span class="math inline">\(b_0\)</span> where highly correlated (actually, <span class="math inline">\(g_2\)</span> (<span class="math inline">\(=\gamma_{2}\)</span>)and <span class="math inline">\(g_3\)</span> (<span class="math inline">\(=\gamma_{3}\)</span>) as well, see notation below). This indicates that we might need a different and more appropriate parametrization of the log-likelihood function.</p>
<pre class="python"><code>with pm.Model() as model_cov:

    # a0 = pm.HalfNormal(name=&quot;a0&quot;, sigma=10)
    # g2 = pm.Normal(name=&quot;g2&quot;, mu=0, sigma=10)
    # a = a0 * tt.exp(g2 * z)
    a = pm.HalfNormal(name=&quot;a&quot;, sigma=10)
    
    # b0 = pm.HalfNormal(name=&quot;b0&quot;, sigma=10)
    # g3 = pm.Normal(name=&quot;g3&quot;, mu=0, sigma=10)
    # b = tt.exp(g3 * z)
    b = pm.HalfNormal(name=&quot;b&quot;, sigma=10)

    alpha0 = pm.HalfNormal(name=&quot;alpha0&quot;, sigma=10)
    g1 = pm.Normal(name=&quot;g1&quot;, mu=0, sigma=10)
    alpha = pm.Deterministic(name=&quot;alpha&quot;, var=alpha0 * at.exp(- g1 * z))
    
    r = pm.HalfNormal(name=&quot;r&quot;, sigma=10)

    def logp(x, t_x, T, x_zero):
        a1 = at.gammaln(r + x) - at.gammaln(r) + r * at.log(alpha)
        a2 = at.gammaln(a + b) + at.gammaln(b + x) - at.gammaln(b) - at.gammaln(a + b + x)
        a3 = -(r + x) * at.log(alpha + T)
        a4 =  at.log(a) - at.log(b + at.maximum(x, 1) - 1) - (r + x) * at.log(t_x + alpha)
        max_a3_a4 = at.maximum(a3, a4)
        ll_1 = a1 + a2 
        ll_2 = at.log(at.exp(a3 - max_a3_a4) + at.exp(a4 - max_a3_a4) * pm.math.switch(x_zero, 1, 0)) + max_a3_a4
        return at.sum(ll_1 + ll_2)

    likelihood = pm.Potential(
        name=&quot;likelihood&quot;,
        var=logp(x=x_z, t_x=t_x_z, T=T, x_zero=x_zero_z),
    )

pm.model_to_graphviz(model=model_cov)</code></pre>
<center>
<img src="../images/bg_nbd_pymc_files/bg_nbd_pymc_52_0.svg" style="width: 800px;"/>
</center>
<pre class="python"><code>with model_cov:
    trace_cov = pm.sampling_jax.sample_numpyro_nuts(
        tune=3000,
        draws=6000,
        chains=4,
        target_accept=0.95,
    )</code></pre>
<pre class="python"><code>az.summary(data=trace_cov, var_names=[&quot;~alpha&quot;])</code></pre>
<center>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
mean
</th>
<th>
sd
</th>
<th>
hdi_3%
</th>
<th>
hdi_97%
</th>
<th>
mcse_mean
</th>
<th>
mcse_sd
</th>
<th>
ess_bulk
</th>
<th>
ess_tail
</th>
<th>
r_hat
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
g1
</th>
<td>
-2.860
</td>
<td>
0.141
</td>
<td>
-3.133
</td>
<td>
-2.606
</td>
<td>
0.001
</td>
<td>
0.001
</td>
<td>
15215.0
</td>
<td>
14112.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
a
</th>
<td>
1.175
</td>
<td>
0.571
</td>
<td>
0.380
</td>
<td>
2.192
</td>
<td>
0.006
</td>
<td>
0.004
</td>
<td>
10343.0
</td>
<td>
10398.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
b
</th>
<td>
5.264
</td>
<td>
3.162
</td>
<td>
1.321
</td>
<td>
10.998
</td>
<td>
0.033
</td>
<td>
0.024
</td>
<td>
10325.0
</td>
<td>
10249.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
alpha0
</th>
<td>
4.024
</td>
<td>
0.434
</td>
<td>
3.232
</td>
<td>
4.854
</td>
<td>
0.004
</td>
<td>
0.003
</td>
<td>
11666.0
</td>
<td>
13128.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
r
</th>
<td>
0.226
</td>
<td>
0.015
</td>
<td>
0.198
</td>
<td>
0.253
</td>
<td>
0.000
</td>
<td>
0.000
</td>
<td>
12265.0
</td>
<td>
13506.0
</td>
<td>
1.0
</td>
</tr>
</tbody>
</table>
</div>
</center>
<pre class="python"><code>axes = az.plot_trace(
    data=trace_cov,
    var_names=[&quot;~alpha&quot;],
    lines=[(k, {}, [v]) for k, v in bgf_cov.summary[&quot;coef&quot;].items()],
    compact=True,
    backend_kwargs={
        &quot;figsize&quot;: (12, 15),
        &quot;layout&quot;: &quot;constrained&quot;
    }
)
fig = axes[0][0].get_figure()
fig.suptitle(&quot;BG/NBD Model (Time-Invariant Covariates) Trace&quot;, fontsize=16);</code></pre>
<center>
<img src="../images/bg_nbd_pymc_files/bg_nbd_pymc_55_1.png" style="width: 1000px;"/>
</center>
<p>We indeed see a difference in the model parameters as compared with the model without covariates <code>bgf_cov</code> (note specially the shift in <span class="math inline">\(r\)</span>).</p>
<pre class="python"><code>axes = az.plot_pair(data=trace_cov, var_names=[&quot;~alpha&quot;], figsize=(12, 12))
fig = axes[0][0].get_figure()
fig.suptitle(&quot;BG/NBD Model Parameters (Time-Invariant Covariates) Pairplot&quot;, y=0.95, fontsize=16);</code></pre>
<center>
<img src="../images/bg_nbd_pymc_files/bg_nbd_pymc_57_0.png" style="width: 800px;"/>
</center>
<p>Finally, let us see the two different values (for <span class="math inline">\(z=0\)</span> and <span class="math inline">\(z=1\)</span>) of the <code>alpha</code> parameter.</p>
<pre class="python"><code>fig, ax = plt.subplots()
sns.histplot(
    x=trace_cov.posterior[&quot;alpha&quot;][:, :, z == 0].to_numpy().flatten(),
    color=&quot;C0&quot;,
    label=r&quot;z = 0&quot;,
    ax=ax,
)
sns.histplot(
    x=trace_cov.posterior[&quot;alpha&quot;][:, :, z == 1].to_numpy().flatten(),
    color=&quot;C1&quot;,
    label=r&quot;z = 1&quot;,
    ax=ax,
)
ax.legend(loc=&quot;upper right&quot;)
ax.set(title=r&quot;$\alpha$ posterior&quot;, xlabel=r&quot;$\alpha$&quot;, ylabel=&quot;count&quot;);</code></pre>
<center>
<img src="../images/bg_nbd_pymc_files/bg_nbd_pymc_59_0.png" style="width: 800px;"/>
</center>
<ul>
<li>Recall <span class="math inline">\(\lambda \sim \text{Gamma}(r, \alpha)\)</span>, which has expected value <span class="math inline">\(r/\alpha\)</span>.</li>
<li>Hence, as <span class="math inline">\(g_1 &lt; 0\)</span>, then <span class="math inline">\(\alpha(z=1) &gt; \alpha(z=0)\)</span> which is consistent with the data generation process where <span class="math inline">\(z = 1\)</span> implied a lower frequency.</li>
</ul>
</div>
<div id="bayesian-model-hierarchical-model" class="section level2">
<h2>Bayesian Model: Hierarchical Model</h2>
<p>In this final section we outline how to set up a hierarchical model. This is relevant when you want to model each user cohort separately. For example, you might want to model the purchasing behavior of users who joined the platform in April, May, June, etc. separately. This is useful because the purchasing behavior of users who joined the platform in April might be different from the purchasing behavior of users who joined the platform in June. However, they are not independent of each other hence we would like to pool information across cohorts. Moreover, this comes very handy when trying to make predictions for very young cohorts (cold start problem).</p>
<p>In this example we artificially create <span class="math inline">\(4\)</span> cohorts:</p>
<pre class="python"><code>groups = [&quot;g1&quot;, &quot;g2&quot;, &quot;g3&quot;, &quot;g4&quot;]

data_df[&quot;group&quot;] = rng.choice(a=groups, p=[0.45, 0.35, 0.15, 0.05], size=n_obs)

data_df[&quot;group&quot;].value_counts()</code></pre>
<center>
<pre style="white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace">
  g1    <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">1065</span>
  g2     <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">815</span>
  g3     <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">353</span>
  g4     <span style="color: #008080; text-decoration-color: #008080; font-weight: bold">124</span>
  Name: group, dtype: int64
  </pre>
</center>
<p>For the sake of comparison, let’s fit the <code>lifetimes</code> model each cohort.</p>
<pre class="python"><code>group_summaries = []

for g, group_df in data_df.groupby(&quot;group&quot;):
    # set data
    x_g = group_df[&quot;frequency&quot;].to_numpy()
    t_x_g = group_df[&quot;recency&quot;].to_numpy()
    T_g = group_df[&quot;T&quot;].to_numpy()
    x_zero_g = int_vec(x_g &gt; 0)
    # fit model
    bgf_g = BetaGeoFitter()
    bgf_g.fit(frequency=x_g, recency=t_x_g, T=T_g)
    group_summary_df = (
        bgf_g.summary.assign(group=g)
        .reset_index(drop=False)
        .rename(columns={&quot;index&quot;: &quot;parameter&quot;})
    )
    group_summaries.append(group_summary_df)

group_summaries = pd.concat(group_summaries, axis=0)</code></pre>
<pre class="python"><code>group_summaries</code></pre>
<center>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
parameter
</th>
<th>
coef
</th>
<th>
se(coef)
</th>
<th>
lower 95% bound
</th>
<th>
upper 95% bound
</th>
<th>
group
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
r
</td>
<td>
0.230703
</td>
<td>
0.017721
</td>
<td>
0.195971
</td>
<td>
0.265436
</td>
<td>
g1
</td>
</tr>
<tr>
<th>
1
</th>
<td>
alpha
</td>
<td>
4.312733
</td>
<td>
0.547934
</td>
<td>
3.238784
</td>
<td>
5.386683
</td>
<td>
g1
</td>
</tr>
<tr>
<th>
2
</th>
<td>
a
</td>
<td>
0.989651
</td>
<td>
0.415796
</td>
<td>
0.174691
</td>
<td>
1.804611
</td>
<td>
g1
</td>
</tr>
<tr>
<th>
3
</th>
<td>
b
</td>
<td>
3.291437
</td>
<td>
1.724476
</td>
<td>
-0.088537
</td>
<td>
6.671411
</td>
<td>
g1
</td>
</tr>
<tr>
<th>
0
</th>
<td>
r
</td>
<td>
0.259025
</td>
<td>
0.022853
</td>
<td>
0.214233
</td>
<td>
0.303816
</td>
<td>
g2
</td>
</tr>
<tr>
<th>
1
</th>
<td>
alpha
</td>
<td>
4.754671
</td>
<td>
0.684570
</td>
<td>
3.412914
</td>
<td>
6.096428
</td>
<td>
g2
</td>
</tr>
<tr>
<th>
2
</th>
<td>
a
</td>
<td>
0.564523
</td>
<td>
0.185246
</td>
<td>
0.201440
</td>
<td>
0.927606
</td>
<td>
g2
</td>
</tr>
<tr>
<th>
3
</th>
<td>
b
</td>
<td>
1.457592
</td>
<td>
0.586177
</td>
<td>
0.308686
</td>
<td>
2.606499
</td>
<td>
g2
</td>
</tr>
<tr>
<th>
0
</th>
<td>
r
</td>
<td>
0.271790
</td>
<td>
0.037150
</td>
<td>
0.198976
</td>
<td>
0.344603
</td>
<td>
g3
</td>
</tr>
<tr>
<th>
1
</th>
<td>
alpha
</td>
<td>
4.781757
</td>
<td>
1.090382
</td>
<td>
2.644607
</td>
<td>
6.918906
</td>
<td>
g3
</td>
</tr>
<tr>
<th>
2
</th>
<td>
a
</td>
<td>
2.327516
</td>
<td>
3.830876
</td>
<td>
-5.181000
</td>
<td>
9.836032
</td>
<td>
g3
</td>
</tr>
<tr>
<th>
3
</th>
<td>
b
</td>
<td>
10.054263
</td>
<td>
18.778442
</td>
<td>
-26.751482
</td>
<td>
46.860009
</td>
<td>
g3
</td>
</tr>
<tr>
<th>
0
</th>
<td>
r
</td>
<td>
0.180750
</td>
<td>
0.039690
</td>
<td>
0.102958
</td>
<td>
0.258542
</td>
<td>
g4
</td>
</tr>
<tr>
<th>
1
</th>
<td>
alpha
</td>
<td>
2.806377
</td>
<td>
1.095709
</td>
<td>
0.658788
</td>
<td>
4.953967
</td>
<td>
g4
</td>
</tr>
<tr>
<th>
2
</th>
<td>
a
</td>
<td>
0.821841
</td>
<td>
0.646647
</td>
<td>
-0.445587
</td>
<td>
2.089269
</td>
<td>
g4
</td>
</tr>
<tr>
<th>
3
</th>
<td>
b
</td>
<td>
2.217292
</td>
<td>
2.128679
</td>
<td>
-1.954919
</td>
<td>
6.389502
</td>
<td>
g4
</td>
</tr>
</tbody>
</table>
</div>
</center>
<pre class="python"><code>g = sns.catplot(
    data=group_summaries,
    x=&quot;group&quot;,
    y=&quot;coef&quot;,
    col=&quot;parameter&quot;,
    col_wrap=2,
    kind=&quot;bar&quot;,
    sharex=True,
    sharey=False,
    height=4,
)

g.fig.suptitle(&quot;BG/NBD Model Parameters by Group&quot;, y=1.05, fontsize=16);</code></pre>
<center>
<img src="../images/bg_nbd_pymc_files/bg_nbd_pymc_66_0.png" style="width: 800px;"/>
</center>
<p>Now we specify the hierarchical model.</p>
<pre class="python"><code>group_idx, groups = data_df[&quot;group&quot;].factorize(sort=True)

coords = {
    &quot;group_idx&quot;: group_idx,
    &quot;group&quot;: groups,
}

with pm.Model(coords=coords) as model_h:

    # hyper-priors
    alpha_a = pm.Gamma(name=&quot;alpha_a&quot;, alpha=1, beta=1)
    beta_a = pm.Gamma(name=&quot;beta_a&quot;, alpha=1, beta=1)
    alpha_b = pm.Gamma(name=&quot;alpha_b&quot;, alpha=1, beta=1)
    beta_b = pm.Gamma(name=&quot;beta_b&quot;, alpha=1, beta=1)

    alpha_alpha = pm.Gamma(name=&quot;alpha_alpha&quot;, alpha=5, beta=2)
    beta_alpha = pm.Gamma(name=&quot;beta_alpha&quot;, alpha=2, beta=2)
    alpha_r = pm.Gamma(name=&quot;alpha_r&quot;, alpha=1, beta=1)
    beta_r = pm.Gamma(name=&quot;beta_r&quot;, alpha=1, beta=1)

    # priors
    a_group= pm.Gamma(name=&quot;a&quot;, alpha=alpha_a, beta=beta_a, dims=&quot;group&quot;)
    b_group = pm.Gamma(name=&quot;b&quot;, alpha=alpha_b, beta=beta_b,  dims=&quot;group&quot;)

    alpha_group = pm.Gamma(name=&quot;alpha&quot;, alpha=alpha_alpha, beta=beta_alpha, dims=&quot;group&quot;)
    r_group = pm.Gamma(name=&quot;r&quot;, alpha=alpha_r, beta=beta_r, dims=&quot;group&quot;)

    # cohort-specific parameters
    a = a_group[group_idx]
    b = b_group[group_idx]
    alpha = alpha_group[group_idx]
    r = r_group[group_idx]

    # log-likelihood
    def logp(x, t_x, T, x_zero):
        a1 = at.gammaln(r + x) - at.gammaln(r) + r * at.log(alpha)
        a2 = at.gammaln(a + b) + at.gammaln(b + x) - at.gammaln(b) - at.gammaln(a + b + x)
        a3 = -(r + x) * at.log(alpha + T)
        a4 =  at.log(a) - at.log(b + at.maximum(x, 1) - 1) - (r + x) * at.log(t_x + alpha)
        max_a3_a4 = at.maximum(a3, a4)
        ll_1 = a1 + a2 
        ll_2 = at.log(at.exp(a3 - max_a3_a4) + at.exp(a4 - max_a3_a4) * pm.math.switch(x_zero, 1, 0)) + max_a3_a4
        return at.sum(ll_1 + ll_2)

    likelihood = pm.Potential(name=&quot;likelihood&quot;, var=logp(x, t_x, T, x_zero))


pm.model_to_graphviz(model=model_h)</code></pre>
<center>
<img src="../images/bg_nbd_pymc_files/bg_nbd_pymc_68_0.svg" style="width: 1000px;"/>
</center>
<pre class="python"><code>with model_h:
    trace_h = pm.sampling_jax.sample_numpyro_nuts(
        tune=3000,
        draws=6000,
        chains=4,
        target_accept=0.95,
    )</code></pre>
<pre class="python"><code>axes = az.plot_trace(
    data=trace_h,
    var_names=[&quot;a&quot;, &quot;b&quot;, &quot;alpha&quot;, &quot;r&quot;],
    lines=[(k, {}, [v]) for k, v in bgf.summary[&quot;coef&quot;].items()],
    compact=True,
    backend_kwargs={
        &quot;figsize&quot;: (12, 9),
        &quot;layout&quot;: &quot;constrained&quot;
    },
)
fig = axes[0][0].get_figure()
fig.suptitle(&quot;Hierarchical BG/NBD Model Trace&quot;, fontsize=16);</code></pre>
<center>
<img src="../images/bg_nbd_pymc_files/bg_nbd_pymc_70_0.png" style="width: 1000px;"/>
</center>
<p>Let’s look into the summary:</p>
<pre class="python"><code>az.summary(data=trace_h)</code></pre>
<center>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
mean
</th>
<th>
sd
</th>
<th>
hdi_3%
</th>
<th>
hdi_97%
</th>
<th>
mcse_mean
</th>
<th>
mcse_sd
</th>
<th>
ess_bulk
</th>
<th>
ess_tail
</th>
<th>
r_hat
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
alpha_a
</th>
<td>
1.686
</td>
<td>
0.869
</td>
<td>
0.345
</td>
<td>
3.308
</td>
<td>
0.006
</td>
<td>
0.004
</td>
<td>
19833.0
</td>
<td>
15505.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
beta_a
</th>
<td>
1.610
</td>
<td>
0.964
</td>
<td>
0.113
</td>
<td>
3.346
</td>
<td>
0.007
</td>
<td>
0.005
</td>
<td>
18769.0
</td>
<td>
13989.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
alpha_b
</th>
<td>
2.158
</td>
<td>
1.177
</td>
<td>
0.376
</td>
<td>
4.334
</td>
<td>
0.008
</td>
<td>
0.006
</td>
<td>
19374.0
</td>
<td>
14736.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
beta_b
</th>
<td>
0.727
</td>
<td>
0.492
</td>
<td>
0.035
</td>
<td>
1.612
</td>
<td>
0.004
</td>
<td>
0.003
</td>
<td>
14664.0
</td>
<td>
14192.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
alpha_alpha
</th>
<td>
3.356
</td>
<td>
1.157
</td>
<td>
1.426
</td>
<td>
5.586
</td>
<td>
0.007
</td>
<td>
0.005
</td>
<td>
24029.0
</td>
<td>
16938.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
beta_alpha
</th>
<td>
0.806
</td>
<td>
0.326
</td>
<td>
0.253
</td>
<td>
1.424
</td>
<td>
0.002
</td>
<td>
0.001
</td>
<td>
23975.0
</td>
<td>
16880.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
alpha_r
</th>
<td>
0.985
</td>
<td>
0.490
</td>
<td>
0.203
</td>
<td>
1.884
</td>
<td>
0.003
</td>
<td>
0.002
</td>
<td>
24218.0
</td>
<td>
15896.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
beta_r
</th>
<td>
2.523
</td>
<td>
1.499
</td>
<td>
0.208
</td>
<td>
5.237
</td>
<td>
0.009
</td>
<td>
0.007
</td>
<td>
24994.0
</td>
<td>
15206.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
a[g1]
</th>
<td>
1.073
</td>
<td>
0.427
</td>
<td>
0.444
</td>
<td>
1.820
</td>
<td>
0.004
</td>
<td>
0.003
</td>
<td>
17394.0
</td>
<td>
13936.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
a[g2]
</th>
<td>
0.719
</td>
<td>
0.252
</td>
<td>
0.323
</td>
<td>
1.185
</td>
<td>
0.002
</td>
<td>
0.001
</td>
<td>
21337.0
</td>
<td>
16544.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
a[g3]
</th>
<td>
1.171
</td>
<td>
0.653
</td>
<td>
0.288
</td>
<td>
2.296
</td>
<td>
0.005
</td>
<td>
0.004
</td>
<td>
16840.0
</td>
<td>
15148.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
a[g4]
</th>
<td>
1.053
</td>
<td>
0.604
</td>
<td>
0.196
</td>
<td>
2.121
</td>
<td>
0.005
</td>
<td>
0.003
</td>
<td>
17540.0
</td>
<td>
17338.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
b[g1]
</th>
<td>
3.732
</td>
<td>
1.897
</td>
<td>
1.223
</td>
<td>
7.015
</td>
<td>
0.016
</td>
<td>
0.012
</td>
<td>
17363.0
</td>
<td>
13250.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
b[g2]
</th>
<td>
2.046
</td>
<td>
0.922
</td>
<td>
0.733
</td>
<td>
3.723
</td>
<td>
0.007
</td>
<td>
0.005
</td>
<td>
21084.0
</td>
<td>
16081.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
b[g3]
</th>
<td>
4.662
</td>
<td>
3.055
</td>
<td>
0.865
</td>
<td>
9.795
</td>
<td>
0.026
</td>
<td>
0.019
</td>
<td>
16142.0
</td>
<td>
14831.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
b[g4]
</th>
<td>
3.354
</td>
<td>
2.367
</td>
<td>
0.464
</td>
<td>
7.385
</td>
<td>
0.019
</td>
<td>
0.013
</td>
<td>
17720.0
</td>
<td>
16726.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
alpha[g1]
</th>
<td>
4.348
</td>
<td>
0.547
</td>
<td>
3.375
</td>
<td>
5.414
</td>
<td>
0.003
</td>
<td>
0.002
</td>
<td>
25730.0
</td>
<td>
19111.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
alpha[g2]
</th>
<td>
4.770
</td>
<td>
0.675
</td>
<td>
3.571
</td>
<td>
6.068
</td>
<td>
0.004
</td>
<td>
0.003
</td>
<td>
26851.0
</td>
<td>
19591.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
alpha[g3]
</th>
<td>
4.764
</td>
<td>
1.027
</td>
<td>
2.895
</td>
<td>
6.659
</td>
<td>
0.006
</td>
<td>
0.005
</td>
<td>
25637.0
</td>
<td>
18370.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
alpha[g4]
</th>
<td>
3.352
</td>
<td>
1.132
</td>
<td>
1.402
</td>
<td>
5.454
</td>
<td>
0.007
</td>
<td>
0.005
</td>
<td>
25535.0
</td>
<td>
19124.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
r[g1]
</th>
<td>
0.232
</td>
<td>
0.018
</td>
<td>
0.197
</td>
<td>
0.264
</td>
<td>
0.000
</td>
<td>
0.000
</td>
<td>
26531.0
</td>
<td>
19190.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
r[g2]
</th>
<td>
0.259
</td>
<td>
0.023
</td>
<td>
0.218
</td>
<td>
0.303
</td>
<td>
0.000
</td>
<td>
0.000
</td>
<td>
26711.0
</td>
<td>
20014.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
r[g3]
</th>
<td>
0.272
</td>
<td>
0.036
</td>
<td>
0.205
</td>
<td>
0.338
</td>
<td>
0.000
</td>
<td>
0.000
</td>
<td>
25764.0
</td>
<td>
18768.0
</td>
<td>
1.0
</td>
</tr>
<tr>
<th>
r[g4]
</th>
<td>
0.195
</td>
<td>
0.040
</td>
<td>
0.123
</td>
<td>
0.272
</td>
<td>
0.000
</td>
<td>
0.000
</td>
<td>
25393.0
</td>
<td>
19000.0
</td>
<td>
1.0
</td>
</tr>
</tbody>
</table>
</div>
</center>
<p>Finally, let’s compare the parameter of the <code>lifetimes</code> model with the predictions of the hierarchical model.</p>
<pre class="python"><code>fig, axes = plt.subplots(
    nrows=2, ncols=2, figsize=(12, 11), sharex=False, sharey=False, layout=&quot;constrained&quot;
)

axes = axes.flatten()

for i, parameter in enumerate([&quot;r&quot;, &quot;alpha&quot;, &quot;a&quot;, &quot;b&quot;]):
    # compute summary per parameter
    parameter_hdi = az.hdi(trace_h.posterior[parameter])[parameter].to_numpy()
    parameter_mean = (
        trace_h.posterior[parameter].stack(sample=(&quot;chain&quot;, &quot;draw&quot;)).mean(axis=1)
    )

    model_h_parameter_df = pd.DataFrame(
        data=parameter_hdi, columns=[&quot;lower&quot;, &quot;upper&quot;]
    ).assign(group=groups, mean=parameter_mean)

    lifetimes_parameter_df = group_summaries.query(&quot;parameter == @parameter&quot;)

    parameter_df = model_h_parameter_df.merge(lifetimes_parameter_df, on=&quot;group&quot;)

    # plot
    ax = axes[i]

    for j, row in parameter_df.iterrows():

        ax.vlines(
            x=row[&quot;coef&quot;],
            ymin=row[&quot;lower 95% bound&quot;],
            ymax=row[&quot;upper 95% bound&quot;],
            color=f&quot;C{j}&quot;,
            label=row[&quot;group&quot;],
            alpha=0.7,
        )
        ax.hlines(
            y=row[&quot;mean&quot;],
            xmin=row[&quot;lower&quot;],
            xmax=row[&quot;upper&quot;],
            color=f&quot;C{j}&quot;,
            alpha=0.7,
        )

        ax.scatter(x=row[&quot;coef&quot;], y=row[&quot;mean&quot;], color=f&quot;C{j}&quot;, s=80)

    ax.axline(
        xy1=(row[&quot;mean&quot;], row[&quot;mean&quot;]),
        slope=1,
        color=&quot;black&quot;,
        linestyle=&quot;--&quot;,
        label=&quot;diagonal&quot;,
    )
    ax.axvline(
        x=bgf.summary.reset_index(drop=False)
        .query(&quot;index == @parameter&quot;)[&quot;coef&quot;]
        .item(),
        color=&quot;gray&quot;,
        linestyle=&quot;--&quot;,
        label=&quot;lifetimes (global)&quot;,
    )
    ax.axhline(
        y=bgf.summary.reset_index(drop=False)
        .query(&quot;index == @parameter&quot;)[&quot;coef&quot;]
        .item(),
        color=&quot;gray&quot;,
        linestyle=&quot;--&quot;,
    )
    ax.legend(loc=&quot;lower right&quot;)

    ax.set(title=parameter, xlabel=&quot;lifetimes&quot;, ylabel=&quot;hierarchical model&quot;)

fig.suptitle(&quot;Parameters - Hierarchical BG/NBD Model vs. Lifetimes&quot;, fontsize=16);</code></pre>
<center>
<img src="../images/bg_nbd_pymc_files/bg_nbd_pymc_74_0.png" style="width: 1000px;"/>
</center>
<p>Here we can see the <em>shrinkage</em> effect of the hierarchical model. The model parameters are closer to the global estimates (i.e. the estimates of the model without cohorts).</p>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="../index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="../images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/dockerfile.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/bash.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="../js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-122570825-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

  </body>
</html>

