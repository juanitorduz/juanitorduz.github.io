<!DOCTYPE html>
<html lang="en-us">
  <head>
    <script defer src="https://use.fontawesome.com/releases/v5.13.0/js/all.js"></script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-5NM5EDH834"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-5NM5EDH834');
</script>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.79.1" />


<title>PyData Berlin 2018: On Laplacian Eigenmaps for Dimensionality Reduction - Dr. Juan Camilo Orduz</title>
<meta property="og:title" content="PyData Berlin 2018: On Laplacian Eigenmaps for Dimensionality Reduction - Dr. Juan Camilo Orduz">


  <link href='../images/favicon.ico' rel='icon' type='image/x-icon'/>



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="../css/fonts.css" media="all">
<link rel="stylesheet" href="../css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="../" class="nav-logo">
    <img src="../images/sphere2.gif"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="../about/"> About</a></li>
    
    <li><a href="https://github.com/juanitorduz"><i class='fab fa-github fa-2x'></i>  </a></li>
    
    <li><a href="https://www.linkedin.com/in/juanitorduz/"><i class='fab fa-linkedin fa-2x' style='color:#0077B5;'></i>  </a></li>
    
    <li><a href="https://twitter.com/juanitorduz"><i class='fab fa-twitter fa-2x' style='color:#1DA1F2;'></i>  </a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">6 min read</span>
    

    <h1 class="article-title">PyData Berlin 2018: On Laplacian Eigenmaps for Dimensionality Reduction</h1>

    
    <span class="article-date">2018-07-08</span>
    

    <div class="article-content">
      
<script src="../rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>This summer I had the great oportunity to attend an give a talk at <a href="https://pydata.org/berlin2018/">Pydata Berlin 2018</a>. The topic of my talk was <a href="https://pydata.org/berlin2018/schedule/presentation/33/">On Laplacian Eigenmaps for Dimensionality Reduction</a>. During the <em>Unsupervised Learning &amp; Visualization</em> session Dr. Stefan Kühn presented a very interesting and visual talk on <a href="https://pydata.org/berlin2018/schedule/presentation/55/">Manifold Learning and Dimensionality Reduction for Data Visualization and Feature Engineering</a> and Dr. Evelyn Trautmann gave a nice application of spectral clustering (<a href="http://www.kyb.mpg.de/fileadmin/user_upload/files/publications/attachments/Luxburg07_tutorial_4488%5b0%5d.pdf">here</a> are some nice notes around this subject) in the context of <a href="https://pydata.org/berlin2018/schedule/presentation/48/">Extracting relevant Metrics</a>. During our talks, spectral methods and the <a href="https://en.wikipedia.org/wiki/Laplacian_matrix">Graph Laplacian</a> was the biggest intersection. This was an excelent oportunity to enhance the communication channels between pure math and its applications.</p>
<p>I want to acknowledge the organizers, volunteers and great speakers which made this conference possible.</p>
<hr />
<p><strong>Abstract</strong></p>
<p>The aim of this talk is to describe a non-linear dimensionality reduction algorithm based on spectral techniques introduced in <a href="http://web.cse.ohio-state.edu/~belkin.8/papers/LEM_NC_03.pdf">BN2003</a>. The goal of non-linear dimensionality reduction algorithms, so called <em>manifold learning algorithms</em>, is to construct a representation of data on a low dimensional manifold embedded in a high dimensional space. The particular case we are going to present exploits various relations of geometric and spectral methods (discrete and continuous). Spectral methods are sometimes motivated by Marc Kac’s question <em>Can One Hear the Shape of a Drum?</em> which makes reference to the idea of recovering geometrical properties from the eigenvalues (spectrum) of a matrix (linear operator). Concretely, the approach followed in <a href="http://web.cse.ohio-state.edu/~belkin.8/papers/LEM_NC_03.pdf">BN2003</a> has its foundation on the spectral analysis of the <a href="https://en.wikipedia.org/wiki/Laplacian_matrix">Graph Laplacian</a> of the adjacency graph constructed from the data. The motivation of the construction relies on the continuous limit analogue, the <a href="https://en.wikipedia.org/wiki/Laplace–Beltrami_operator">Laplace-Beltrami</a> operator, in providing an optimal embedding for manifolds. We will also indicate the relation with the associated heat kernel operator. Instead of following a pure formal approach we will present the main geometric and computational ideas of the algorithm. Hence, with basic knowledge of linear algebra (eigenvalues) and differential calculus you will be able to follow the talk. Finally we will show a concrete example in Python using <a href="http://scikit-learn.org/stable/modules/generated/sklearn.manifold.SpectralEmbedding.html">scikit-learn</a>.</p>
<hr />
<strong>Video</strong>
<center>
<iframe width="500" height="300" src="https://www.youtube.com/embed/U31TIICsHiA?rel=0" frameborder="0" allowfullscreen>
</iframe>
</center>
<hr />
<p><strong>Slides</strong></p>
<p><a href="../documents/orduz_pydata2018.pdf">Here</a> you can find the slides of the talk.</p>
<p>Suggestions and comments are always welcome!</p>
<hr />
<div id="jupyter-notebook" class="section level2">
<h2>Jupyter Notebook</h2>
<p>In this notebook we present some simple examples which ilustrate the concepts discussed in the talk on <a href="https://pydata.org/berlin2018/schedule/presentation/33/">Laplacian Eigenmaps for Dimensionality Reduction</a> at <a href="https://pydata.org/berlin2018/">PyData Berlin 2018</a>.</p>
<p>The sample data, visualization code and object descriptions can be found in the documentation.</p>
<p><strong>References:</strong></p>
<ul>
<li><p><a href="http://scikit-learn.org/stable/modules/generated/sklearn.manifold.spectral_embedding.html" class="uri">http://scikit-learn.org/stable/modules/generated/sklearn.manifold.spectral_embedding.html</a></p></li>
<li><p><a href="http://scikit-learn.org/stable/modules/manifold.html#spectral-embedding" class="uri">http://scikit-learn.org/stable/modules/manifold.html#spectral-embedding</a></p></li>
</ul>
</div>
<div id="notebook-setup" class="section level2">
<h2>1. Notebook Setup</h2>
<pre class="python"><code>import matplotlib.pyplot as plt
from mpl_toolkits.mplot3d import Axes3D
import numpy as np
from sklearn import manifold, datasets
from sklearn.utils import check_random_state
from sklearn.decomposition import PCA</code></pre>
</div>
<div id="back-to-our-toy-example" class="section level2">
<h2>2. Back to Our Toy Example</h2>
<center>
<img src="../images/toy_graph.png" alt="html" style="width: 400px;"/>
</center>
<p>Let us compute the graph Laplacian from the adjacency matrix.</p>
<pre class="python"><code># Construct the adjacency matrix from the graph for n_neighbors = 1. 
W = np.array([[0, 1, 1, 1],
              [1, 0, 0, 0], 
              [1, 0, 0, 0], 
              [1, 0, 0, 0]])

# Construct the degree matrix D from W.
D = np.diag(np.apply_along_axis(arr=W,
                                func1d=np.sum,
                                axis=0))
# Compute the graph Laplacian.
L = D - W

print(L)</code></pre>
<pre><code>[[ 3 -1 -1 -1]
 [-1  1  0  0]
 [-1  0  1  0]
 [-1  0  0  1]]</code></pre>
<p>Let us use <code>manifold.spectral_embedding</code> to compute the projection onto \(\).</p>
<pre class="python"><code>y = manifold.spectral_embedding(adjacency=W, 
                                n_components=1, 
                                norm_laplacian=False, 
                                drop_first=True,
                                eigen_solver=&#39;lobpcg&#39;)

print(y)</code></pre>
<pre><code>[[ 0.        ]
 [-0.57735027]
 [ 0.78867513]
 [-0.21132487]]</code></pre>
<p>Let us verify the result. Recall that the first non zero eigenvalue of is \(_1 = 1\).</p>
<pre class="python"><code>lambda_1 = 1

# Check if the previous output coincides with the analytical requirement. 
np.array_equal(a1=np.dot(L, y), 
               a2=lambda_1*np.dot(D, y))</code></pre>
<pre><code>True</code></pre>
<p>Thus, \(y\) is a solution of \(Lf = _1 Df\) with \(_1 =1\).</p>
</div>
<div id="higher-dimensional-examples" class="section level2">
<h2>3 Higher Dimensional Examples</h2>
<div id="s-curve" class="section level3">
<h3>3.1 S Curve</h3>
<p><strong>Reference:</strong> <a href="http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_s_curve.html" class="uri">http://scikit-learn.org/stable/modules/generated/sklearn.datasets.make_s_curve.html</a></p>
<pre class="python"><code># Define the number of points to consider. 
n_points = 3000

# Get the data and color map. 
S_curve, S_colors = datasets.samples_generator.make_s_curve(n_points, random_state=0)

Axes3D

fig = plt.figure(figsize=(12, 12))
ax = fig.add_subplot(111, projection=&#39;3d&#39;)
ax.scatter(S_curve[:, 0], S_curve[:, 1], S_curve[:, 2],
           c=S_colors, 
           cmap=plt.cm.Spectral)
ax.view_init(4, -72);</code></pre>
<center>
<img src="../images/spectral_embedding_v2_files/spectral_embedding_v2_12_0.png" title="fig:" alt="png" />
</center>
<div id="pca" class="section level4">
<h4>3.1.1 PCA</h4>
<p>Let us see the result of PCA with <code>n_components=2</code>.</p>
<pre class="python"><code># Fit PCA object.
S_curve_pca = PCA(n_components=2).fit_transform(S_curve)

# Visualize the result.
fig = plt.figure(figsize=(12, 6))
ax = fig.add_subplot(111)
ax.scatter(S_curve_pca[:, 0], S_curve_pca[:, 1],
           c=S_colors, 
           cmap=plt.cm.Spectral);</code></pre>
<center>
<img src="../images/spectral_embedding_v2_files/spectral_embedding_v2_14_0.png" title="fig:" alt="png" />
</center>
<p>The result is coincides with ouur intuition, it is basically projecting to an axis so that the variance is maximized.</p>
</div>
<div id="spectral-embedding" class="section level4">
<h4>3.1.2 Spectral Embedding</h4>
<p>Now let us use the method <code>SpectralEmbedding</code> from <code>sklearn</code>.</p>
<pre class="python"><code># Set the number of n_neighbors;
n_neighbors = 6 

# Set the dimension of the target space. 
n_components = 2

# Construct the SpectralEmbedding object. 
se = manifold.SpectralEmbedding(n_components=n_components,
                                affinity= &#39;nearest_neighbors&#39;,
                                n_neighbors=n_neighbors)

# Fit the SpectralEmbedding object. 
S_curve_red = se.fit_transform(X=S_curve)

# Visualize the result.
fig = plt.figure(figsize=(12, 6))
ax = fig.add_subplot(111)
ax.scatter(S_curve_red[:, 0], S_curve_red[:, 1],
           c=S_colors, 
           cmap=plt.cm.Spectral);</code></pre>
<center>
<img src="../images/spectral_embedding_v2_files/spectral_embedding_v2_17_0.png" title="fig:" alt="png" />
</center>
<p>We see that <code>SpectralEmbedding</code> <em>unroll</em> the S-curve so that the locallity is preserved on average.</p>
<p>Now let us consider <code>n_neighbors</code> &gt;&gt; 1.</p>
<pre class="python"><code># Set the number of n_neighbors;
n_neighbors = 1500 

# Set the dimension of the target space. 
n_components = 2

# Construct the SpectralEmbedding object. 
se = manifold.SpectralEmbedding(n_components=n_components,
                                affinity= &#39;nearest_neighbors&#39;,
                                n_neighbors=n_neighbors)

# Fit the SpectralEmbedding object. 
S_curve_red = se.fit_transform(X=S_curve)

# Visualize the result.
fig = plt.figure(figsize=(12, 6))
ax = fig.add_subplot(111)
ax.scatter(S_curve_red[:, 0], S_curve_red[:, 1],
           c=S_colors, 
           cmap=plt.cm.Spectral);</code></pre>
<center>
<img src="../images/spectral_embedding_v2_files/spectral_embedding_v2_20_0.png" title="fig:" alt="png" />
</center>
<p>The result is that the problem becomes more <em>global</em>. Thus, we get a similar result as PCA.</p>
</div>
</div>
<div id="two-dimensional-sphere-s2" class="section level3">
<h3>3.2 Two Dimensional Sphere \(S^2\)</h3>
<p><strong>Reference:</strong> <a href="http://scikit-learn.org/stable/auto_examples/manifold/plot_manifold_sphere.html" class="uri">http://scikit-learn.org/stable/auto_examples/manifold/plot_manifold_sphere.html</a></p>
<p>First we generate the data.</p>
<pre class="python"><code># Create our sphere.
n_samples = 5000

angle_parameter = 0.5 # Try 0.01
pole_hole_parameter = 8 # Try 50

random_state = check_random_state(0)
p = random_state.rand(n_samples) * (2 * np.pi - angle_parameter)
t = random_state.rand(n_samples) * np.pi

# Sever the poles from the sphere.
indices = ((t &lt; (np.pi - (np.pi / pole_hole_parameter))) &amp; 
           (t &gt; ((np.pi / pole_hole_parameter))))
colors = p[indices]
x, y, z = np.sin(t[indices]) * np.cos(p[indices]), \
          np.sin(t[indices]) * np.sin(p[indices]), \
          np.cos(t[indices])
        
sphere_data = np.array([x, y, z]).T</code></pre>
<p>Let us visualize the data.</p>
<pre class="python"><code>fig = plt.figure(figsize=(12, 12))
ax = fig.add_subplot(111, projection=&#39;3d&#39;)
ax.scatter(x, y, z,
           c=colors, 
           cmap=plt.cm.Spectral);</code></pre>
<center>
<img src="../images/spectral_embedding_v2_files/spectral_embedding_v2_25_0.png" title="fig:" alt="png" />
</center>
<div id="pca-1" class="section level4">
<h4>3.2.1 PCA</h4>
<p>Again, let us begin using PCA.</p>
<pre class="python"><code>sphere_pca = PCA(n_components=2).fit_transform(sphere_data)

fig = plt.figure(figsize=(10, 10))
ax = fig.add_subplot(111)
ax.scatter(sphere_pca[:, 0], sphere_pca[:, 1],
           c=colors, 
           cmap=plt.cm.Spectral);</code></pre>
<center>
<img src="../images/spectral_embedding_v2_files/spectral_embedding_v2_27_0.png" title="fig:" alt="png" />
</center>
</div>
<div id="spectral-embedding-1" class="section level4">
<h4>3.3.2 Spectral Embedding</h4>
<pre class="python"><code>n_neighbors = 8
n_components = 2

# Fit the object.
se = manifold.SpectralEmbedding(n_components=n_components,
                                affinity=&#39;nearest_neighbors&#39;,
                                n_neighbors=n_neighbors)

sphere_data_red = se.fit_transform(X=sphere_data)</code></pre>
<p>Let us visualize the results.</p>
<pre class="python"><code>fig = plt.figure(figsize=(10, 10))
ax = fig.add_subplot(111)
ax.scatter(sphere_data_red[:, 0], sphere_data_red[:, 1],
           c=colors, 
           cmap=plt.cm.Spectral);</code></pre>
<center>
<img src="../images/spectral_embedding_v2_files/spectral_embedding_v2_31_0.png" title="fig:" alt="png" />
</center>
<p>We see how <code>SpectralEmbedding</code> unrolls the sphere.</p>
<p>Finally, let us take again <code>n_neighbors</code> &gt;&gt; 1.</p>
<pre class="python"><code>n_neighbors = 1000

# Fit the object.
se = manifold.SpectralEmbedding(n_components=n_components,
                                affinity=&#39;nearest_neighbors&#39;,
                                n_neighbors=n_neighbors)

sphere_data_red = se.fit_transform(X=sphere_data)

fig = plt.figure(figsize=(10, 10))
ax = fig.add_subplot(111)
ax.scatter(sphere_data_red[:, 0], sphere_data_red[:, 1],
           c=colors, 
           cmap=plt.cm.Spectral);</code></pre>
<center>
<img src="../images/spectral_embedding_v2_files/spectral_embedding_v2_34_0.png" title="fig:" alt="png" />
</center>
</div>
</div>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="../index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="../images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/dockerfile.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/bash.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="../js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'G-5NM5EDH834', 'auto');
	
	ga('send', 'pageview');
}
</script>

  </body>
</html>

