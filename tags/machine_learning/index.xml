<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine_learning on Dr. Juan Camilo Orduz</title>
    <link>/tags/machine_learning/</link>
    <description>Recent content in Machine_learning on Dr. Juan Camilo Orduz</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sat, 15 Jun 2019 00:00:00 +0000</lastBuildDate>
    <atom:link href="/tags/machine_learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>satRday Berlin 2019: Remedies for Severe Class Imbalance</title>
      <link>/class_imbalance/</link>
      <pubDate>Sat, 15 Jun 2019 00:00:00 +0000</pubDate>
      <guid>/class_imbalance/</guid>
      <description>&lt;p&gt;In this post I present a concrete case study illustrating some techniques to improve model performance in class-imbalanced classification problems. The methodologies described here are based on &lt;em&gt;Chapter 16: Remedies for Severe Class Imbalance&lt;/em&gt; of the (great!) book &lt;a href=&#34;http://appliedpredictivemodeling.com/&#34;&gt;Applied Predictive Modeling&lt;/a&gt; by Max Kuhn and Kjell Johnson. I absolutely recommend this reference to anyone interested in predictive modeling.&lt;/p&gt;&#xA;&lt;p&gt;This notebook should serve as an extension of my talk given at &lt;a href=&#34;https://berlin2019.satrdays.org/&#34;&gt;satRday Berlin 2019: A conference for R users in Berlin&lt;/a&gt;. Here are the slides:&lt;/p&gt;</description>
    </item>
    <item>
      <title>Exploring the Curse of Dimensionality - Part II.</title>
      <link>/exploring-the-curse-of-dimensionality-part-ii./</link>
      <pubDate>Tue, 01 Jan 2019 00:00:00 +0000</pubDate>
      <guid>/exploring-the-curse-of-dimensionality-part-ii./</guid>
      <description>&lt;p&gt;I continue exploring the &lt;strong&gt;curse of dimensionality&lt;/strong&gt;. Following the analysis form &lt;a href=&#34;https://juanitorduz.github.io/exploring-the-curse-of-dimensionality-part-i./&#34;&gt;Part I.&lt;/a&gt;, I want to discuss another consequence of sparse sampling in high dimensions: sample points are close to an edge of the sample. This post is based on &lt;a href=&#34;https://web.stanford.edu/~hastie/ElemStatLearn/&#34;&gt;The Elements of Statistical Learning, Section 2.5&lt;/a&gt;, which I encourage to read!&lt;/p&gt;&#xA;&lt;div id=&#34;uniform-sampling&#34; class=&#34;section level1&#34;&gt;&#xA;&lt;h1&gt;Uniform Sampling&lt;/h1&gt;&#xA;&lt;p&gt;&lt;em&gt;Consider &lt;span class=&#34;math inline&#34;&gt;\(N\)&lt;/span&gt; data points uniformly distributed in a &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt;-dimensional unit ball centered at the origin. Suppose we consider a nearest-neighbor estimate at the origin. The median distance from the origin to the closest data point is given by the expression&lt;/em&gt;&#xA;&lt;span class=&#34;math display&#34;&gt;\[&#xA;d(p,N)  = \left(1-\frac{1}{2}^{1/N}\right)^{1/p}.&#xA;\]&lt;/span&gt;&lt;/p&gt;</description>
    </item>
    <item>
      <title>Exploring the Curse of Dimensionality - Part I.</title>
      <link>/exploring-the-curse-of-dimensionality-part-i./</link>
      <pubDate>Sun, 09 Dec 2018 00:00:00 +0000</pubDate>
      <guid>/exploring-the-curse-of-dimensionality-part-i./</guid>
      <description>&lt;p&gt;In this post I want to present the notion of &lt;strong&gt;curse of dimensionality&lt;/strong&gt; following a suggested exercise (Chapter 4 - Ex. 4) of the book &lt;a href=&#34;http://www-bcf.usc.edu/~gareth/ISL/&#34;&gt;An Introduction to Statistical Learning&lt;/a&gt;, written by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani.&lt;/p&gt;&#xA;&lt;p&gt;&lt;em&gt;When the number of features &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is large, there tends to be a deterioration in the performance of KNN and other local approaches that perform prediction using only observations that are near the test observation for which a prediction must be made. This phenomenon is known as the &lt;strong&gt;curse of dimensionality&lt;/strong&gt;, and it ties into the fact that non-parametric approaches often perform poorly when &lt;span class=&#34;math inline&#34;&gt;\(p\)&lt;/span&gt; is large. We will now investigate this curse.&lt;/em&gt;&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
