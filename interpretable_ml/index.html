<!DOCTYPE html>
<html lang="en-us">
  <head>
    <script defer src="https://use.fontawesome.com/releases/v5.13.0/js/all.js"></script>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.79.1" />


<title>Exploring Tools for Interpretable Machine Learning - Dr. Juan Camilo Orduz</title>
<meta property="og:title" content="Exploring Tools for Interpretable Machine Learning - Dr. Juan Camilo Orduz">


  <link href='../images/favicon.ico' rel='icon' type='image/x-icon'/>



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="../css/fonts.css" media="all">
<link rel="stylesheet" href="../css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="../" class="nav-logo">
    <img src="../images/sphere2.gif"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="../about/"> About</a></li>
    
    <li><a href="https://github.com/juanitorduz"><i class='fab fa-github fa-2x'></i>  </a></li>
    
    <li><a href="https://www.linkedin.com/in/juanitorduz/"><i class='fab fa-linkedin fa-2x' style='color:#0077B5;'></i>  </a></li>
    
    <li><a href="https://twitter.com/juanitorduz"><i class='fab fa-twitter fa-2x' style='color:#1DA1F2;'></i>  </a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">19 min read</span>
    

    <h1 class="article-title">Exploring Tools for Interpretable Machine Learning</h1>

    
    <span class="article-date">2021-07-01</span>
    

    <div class="article-content">
      
<script src="../rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>In this notebook we want to test various ways of getting a better understanding on how non-trivial machine learning models generate predictions and how features interact with each other. This is in general not straight forward and key components are (1) understanding on the input data and (2) domain knowledge on the problem. Two great references on the subject are:</p>
<ul>
<li><a href="https://christophm.github.io/interpretable-ml-book/">Interpretable Machine Learning, A Guide for Making Black Box Models Explainable by Christoph Molnar</a></li>
<li><a href="https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python">Interpretable Machine Learning with Python by Serg Masís</a></li>
</ul>
<p>Note that the methods discussed in this notebook are not related with <em>causality</em>. I strongly recommend to refer to the article <a href="https://towardsdatascience.com/be-careful-when-interpreting-predictive-models-in-search-of-causal-insights-e68626e664b6">Be Careful When Interpreting Predictive Models in Search of Causal Insights</a> by <a href="https://scottlundberg.com/">Scott Lundberg</a> (one of the core developers of <a href="https://shap.readthedocs.io/en/latest/index.html">SHAP</a>). The following are two references I have found particularly useful as an introduction to <em>causal inference</em>:</p>
<ul>
<li><a href="https://xcelab.net/rm/statistical-rethinking/">Statistical Rethinking, A Bayesian Course with Examples in R and Stan</a> by <a href="https://xcelab.net/rm/">Richard McElreath</a>.</li>
<li><a href="https://mixtape.scunning.com/index.html">Causal Inference: The Mixtape</a> by <a href="https://www.scunning.com/">Scott Cunningham</a>.</li>
</ul>
<p><strong>Remark:</strong> The article <a href="https://arxiv.org/abs/2007.04131">General Pitfalls of Model-Agnostic Interpretation Methods for Machine Learning Models</a> is highly recommended to understand the challenges, limitations and recommendations for some of the model-agnostic methods discussed below.</p>
<div id="data" class="section level2">
<h2>Data</h2>
<p>We are going to use the processed <em>Bike Sharing Dataset Data Set</em> described in <a href="https://christophm.github.io/interpretable-ml-book/bike-data.html">Section 3.1</a> in <a href="https://christophm.github.io/interpretable-ml-book/">Interpretable Machine Learning, A Guide for Making Black Box Models Explainable by Christoph Molnar</a>. The prediction task is to predict daily counts of rented bicycles as a function of time and other external regressors like temperature and humidity. Note that the raw data can be downloaded from the <a href="http://archive.ics.uci.edu/ml/datasets/Bike+Sharing+Dataset">UCI Machine Learning Repository</a>. The preprocessing steps are described <a href="https://github.com/christophM/interpretable-ml-book/blob/master/R/get-bike-sharing-dataset.R">here</a>.</p>
<table style="width:46%;">
<colgroup>
<col width="45%" />
</colgroup>
<thead>
<tr class="header">
<th align="left"># Part I: Model Development</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">train mse (linear): {mean_squared_error(y_true=y_train, y_pred=y_train_pred_linear): 0.2f}
test mse (linear): {mean_squared_error(y_true=y_test, y_pred=y_test_pred_linear): 0.2f}</td>
</tr>
</tbody>
</table>
<p>train mse (tree) : {mean_squared_error(y_true=y_train, y_pred=y_train_pred_tree): 0.2f}
test mse (tree) : {mean_squared_error(y_true=y_test, y_pred=y_test_pred_tree): 0.2f}
——————————–
’’’)</p>
<pre><code>
    
    --------------------------------
    train mse (linear):  191343.26
    test  mse (linear):  1275495.73
    --------------------------------
    train mse (tree)  :  22042.71
    test  mse (tree)  :  1209060.94
    --------------------------------
    


Both models have a similar out-sample performance (the tree one does a bit better). For in-sample performance the tree based model has less MSE.

**Warning:** One needs to be careful when using tee based model for time series forecasting as these models are not capable of capturing trend components. In this specific case the trend is no s strong and the overall range of the time series is bounded by the max / min of the training time series. This explains why the tree based model still performs well on the test set.

Let us plot the predictions and error distributions on the training and test sets:


```python
# Compute errors.
error_train_linear = y_train - y_train_pred_linear
error_test_linear = y_test - y_test_pred_linear
error_train_tree = y_train - y_train_pred_tree
error_test_tree = y_test - y_test_pred_tree

fig, ax = plt.subplots(nrows=2, ncols=2, figsize=(10, 9), constrained_layout=True)

ax = ax.flatten()

sns.regplot(x=y_train, y=y_train_pred_linear, color=sns_c[0], label=&#39;linear&#39;, ax=ax[0])
sns.regplot(x=y_train, y=y_train_pred_tree, color=sns_c[1], label=&#39;tree&#39;, ax=ax[0])
ax[0].axline(xy1=(0,0), slope=1, color=&#39;gray&#39;, linestyle=&#39;--&#39;, label=&#39;diagonal&#39;)
ax[0].legend(loc=&#39;upper left&#39;)
ax[0].set(title=&#39;In-Sample Predictions&#39;, xlabel=&#39;y_test&#39;, ylabel=&#39;y_test_pred&#39;)

sns.regplot(x=y_test, y=y_test_pred_linear, color=sns_c[0], label=&#39;linear&#39;, ax=ax[1])
sns.regplot(x=y_test, y=y_test_pred_tree, color=sns_c[1], label=&#39;tree&#39;, ax=ax[1])
ax[1].axline(xy1=(0,0), slope=1, color=&#39;gray&#39;, linestyle=&#39;--&#39;, label=&#39;diagonal&#39;)
ax[1].legend(loc=&#39;upper left&#39;)
ax[1].set(title=&#39;Out-Sample Predictions&#39;, xlabel=&#39;y_test&#39;, ylabel=&#39;y_test_pred&#39;)

sns.kdeplot(x=error_train_linear, color=sns_c[0], label=&#39;linear&#39;, fill=True, alpha=0.1, ax=ax[2])
sns.kdeplot(x=error_train_tree, color=sns_c[1], label=&#39;tree&#39;, fill=True, alpha=0.1, ax=ax[2])
ax[2].axvline(x=error_train_linear.mean(), color=sns_c[0], linestyle=&#39;--&#39;, label=&#39;train_linear_mean&#39;)
ax[2].axvline(x=error_train_tree.mean(), color=sns_c[1], linestyle=&#39;--&#39;, label=&#39;train_tree_mean&#39;)
ax[2].legend(loc=&#39;upper left&#39;)
ax[2].set(title=&#39;In-Sample Errors&#39;, xlabel=&#39;error&#39;)

sns.kdeplot(x=error_test_linear, color=sns_c[0], label=&#39;linear&#39;, fill=True, alpha=0.1, ax=ax[3])
sns.kdeplot(x=error_test_tree, color=sns_c[1], label=&#39;tree&#39;, fill=True, alpha=0.1, ax=ax[3])
ax[3].axvline(x=error_test_linear.mean(), color=sns_c[0], linestyle=&#39;--&#39;, label=&#39;test_linear_mean&#39;)
ax[3].axvline(x=error_test_tree.mean(), color=sns_c[1], linestyle=&#39;--&#39;, label=&#39;test_tree_mean&#39;)
ax[3].legend(loc=&#39;upper left&#39;)
ax[3].set(title=&#39;Out-Sample Errors&#39;, xlabel=&#39;error&#39;);</code></pre>
<center>
<img src="../images/interpretable_ml_files/interpretable_ml_44_0.svg" title="fig:" alt="svg" />
</center>
<p>Now let us visualize the predictions as a time series:</p>
<pre class="python"><code>fig, ax = plt.subplots()
sns.lineplot(
    x=range(y_train_pred_linear.shape[0]),
    y=y_train_pred_linear,
    color=sns_c[0],
    label=&#39;linear&#39;,
    alpha=0.8,
    ax=ax
)
sns.lineplot(
    x=range(y_train_pred_linear.shape[0]),
    y=y_train_pred_tree,
    color=sns_c[1],
    label=&#39;tree&#39;,
    alpha=0.8,
    ax=ax
)
sns.lineplot(
    x=range(y_train.shape[0]),
    y=y_train,
    color=&#39;black&#39;,
    label=&#39;y_train&#39;,
    ax=ax
)
ax.set(title=&#39;In-Sample Predictions&#39;);</code></pre>
<center>
<img src="../images/interpretable_ml_files/interpretable_ml_46_0.svg" title="fig:" alt="svg" />
</center>
<pre class="python"><code>fig, ax = plt.subplots()
sns.lineplot(
    x=range(y_test_pred_linear.shape[0]),
    y=y_test_pred_linear,
    color=sns_c[0],
    marker=&#39;o&#39;,
    markersize=4,
    label=&#39;linear&#39;,
    ax=ax
)
sns.lineplot(
    x=range(y_test_pred_tree.shape[0]),
    y=y_test_pred_tree,
    color=sns_c[1],
    marker=&#39;o&#39;,
    markersize=4,
    label=&#39;tree&#39;,
    ax=ax
)
sns.lineplot(
    x=range(y_test.shape[0]),
    y=y_test,
    color=&#39;black&#39;,
    marker=&#39;o&#39;,
    markersize=4,
    label=&#39;y_test&#39;,
    ax=ax
)
ax.set(title=&#39;Out-Sample Predictions&#39;);</code></pre>
<center>
<img src="../images/interpretable_ml_files/interpretable_ml_47_0.svg" title="fig:" alt="svg" />
</center>
<hr />
</div>
<div id="part-ii-model-interpretation" class="section level1">
<h1>Part II: Model Interpretation</h1>
<p>Now that we have fitted two machine learning models, we would like to try to understand how these models make predictions, e.g. which features are important? are there any interaction effects? what is the effect on certain feature on the final outcome? We will start to answer these questions by looking into the individual model structure and extract some insights based on the algorithm behind the model. After that, we will deep dive into model-agnostic methods.</p>
<div id="model-specific" class="section level2">
<h2>Model Specific</h2>
<p>Let us now dig deeper into model specific methods to understand the model predictions.</p>
<div id="linear-model" class="section level3">
<h3>Linear Model</h3>
<p>Linear models are arguably the most interpretable ones as the parametrization is very transparent. For a given target variable <span class="math inline">\(y\)</span> and regressors <span class="math inline">\(x_{k}\)</span> a linear model has the form</p>
<p><span class="math display">\[
y = \beta_{0} + x_{1}\beta_{1} + \cdots + \beta_{k}x_{k} + \cdots \beta_{p}x_{p} + \varepsilon 
\]</span></p>
<p>where the weights (beta coefficients) <span class="math inline">\(\beta_{i}\)</span> are the parameters to be estimated from the data (<span class="math inline">\(\beta_{0}\)</span> denotes the model intercept) and <span class="math inline">\(\varepsilon \sim N(0, \sigma^{2})\)</span> is an error term. Still, one needs to be careful whenever there are highly correlated variables or multicollinearity. For details on interpretability of linear models see <a href="https://christophm.github.io/interpretable-ml-book/limo.html">Section 4.1, Interpretable Machine Learning</a>.</p>
<p>We want to compare and understand the beta coefficient of our linear model. First let us extract the features feeding the model:</p>
<pre class="python"><code>from itertools import compress

# Polynomial feature names.
polynomial_features = linear_grid_search \
    .best_estimator_[&#39;linear_feature_engineering&#39;][&#39;polynomial&#39;] \
    .get_feature_names(features_ext)

# Mask for variables with zero-variance
variance_threshold_support = linear_grid_search \
    .best_estimator_[&#39;linear_feature_engineering&#39;][&#39;variance_threshold&#39;] \
    .get_support()

linear_features = list(
    compress(data=polynomial_features, selectors=variance_threshold_support)
)</code></pre>
<p>Let us store the linear features after preprocessing in a dataframe.</p>
<pre class="python"><code>linear_x_train = pd.DataFrame(
    data=linear_grid_search.best_estimator_[&#39;linear_feature_engineering&#39;].transform(x_train),
    columns=linear_features
)</code></pre>
<p>Next let us extract the model <span class="math inline">\(\beta\)</span> coefficients.</p>
<pre class="python"><code>linear_model_coef_df = pd.DataFrame(data={
    &#39;linear_features&#39;: linear_features,
    &#39;coef_&#39;: linear_grid_search.best_estimator_[&#39;linear_regressor&#39;].coef_
})

linear_model_coef_df = linear_model_coef_df \
    .assign(abs_coef_ = lambda x: x[&#39;coef_&#39;].abs()) \
    .sort_values(&#39;abs_coef_&#39;, ascending=False) \
    .reset_index(drop=True)

# Get top (abs) beta coefficients.
linear_model_coef_df \
    .head(20) \
    .style.background_gradient(
        cmap=&#39;viridis_r&#39;,
        axis=0,
        subset=[&#39;abs_coef_&#39;]
    )</code></pre>
<center>
<figure>
<img alt="MyImage" src="../images/interpretable_ml_files/lm_beta_table.png" align="middle" style="width: 500px;">
</figure>
</center>
<pre class="python"><code># Let us get the model intercept.
linear_model_intercept = linear_grid_search.best_estimator_[&#39;linear_regressor&#39;].intercept_</code></pre>
<p>These coefficients depend on the scale of each variable (note that we normalized all the features in the pre-processing step). To get a scale-free weight effect we multiply these coefficients with each feature instance, so that the effect on the variable <span class="math inline">\(x_{k}\)</span> on the data instance <span class="math inline">\((y^{i}, x^{i})\)</span> is <span class="math inline">\(\beta_{k}x_{k}^{i}\)</span>.</p>
<pre class="python"><code>linear_model_effects = np.multiply(
    linear_grid_search.best_estimator_[&#39;linear_regressor&#39;].coef_,
    linear_grid_search.best_estimator_[&#39;linear_feature_engineering&#39;].transform(x_train)
)

linear_model_effects_df = pd.DataFrame(
    data=linear_model_effects,
    columns=linear_features
)</code></pre>
<p>Let us plot top weight effects:</p>
<pre class="python"><code>fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 8), constrained_layout=True) 
# Weight effects distribution of the all linear terms.
sns.stripplot(
    data=linear_model_effects_df[features_ext[::-1]],
    orient=&#39;h&#39;,
    color=sns_c[1],
    alpha=0.2,
    ax=ax[0]
)
ax[0].set(
    title=&#39;Linear Features&#39;,
    xlabel=&#39;weight effect&#39;
)
# Weight effects distribution of the terms 
# (including intraction) with highest beta coefficients;
sns.stripplot(
    data=linear_model_effects_df[linear_model_coef_df.head(20)[&#39;linear_features&#39;]],
    orient=&#39;h&#39;,
    color=sns_c[2],
    alpha=0.2,
    ax=ax[1]
)
ax[1].set(
    title=&#39;Features with Highest Beta Coefficients&#39;,
    xlabel=&#39;weight effect&#39;
)
fig.suptitle(&#39;Effect Weight Distribution&#39;);</code></pre>
<center>
<figure>
<img alt="MyImage" src="../images/interpretable_ml_files/interpretable_ml_61_0.svg" align="middle" style="width: 950px;">
</figure>
</center>
<pre class="python"><code>fig, ax = plt.subplots(figsize=(5, 7))
linear_model_effects_df \
    .abs() \
    .mean(axis=0) \
    .sort_values() \
    .tail(25) \
    .plot(
        kind=&#39;barh&#39;,
        ax=ax
    )
ax.set(
    title=&#39;Mean Absolute Weight Effect - Linear Model (Top 25)&#39;,
    xlabel=&#39;weight effect&#39;
);</code></pre>
<center>
<img src="../images/interpretable_ml_files/interpretable_ml_62_0.svg" title="fig:" alt="svg" />
</center>
<p>Note that <code>temp</code>, and <code>days_since_2011</code> with the interaction with <code>yr</code> are the top 3 features. This can be seen as the main components explaining the trend, seasonality and increasing variance.</p>
<p>Let us deep dive into some individual features. For example, lets see at which temperature the effect of this variable is negative:</p>
<pre class="python"><code>fig, ax = plt.subplots(figsize=(9, 6))
sns.scatterplot(
    x=x_train[&#39;temp&#39;],
    y=linear_model_effects_df[&#39;temp&#39;],
    hue=linear_model_effects_df[&#39;temp&#39;].rename(&#39;weight_effect&#39;),
    palette=&#39;coolwarm&#39;,
    ax=ax
)
# Compute sign change point.
cp = x_train[&#39;temp&#39;].iloc[linear_model_effects_df[&#39;temp&#39;].abs().argmin(), ]
ax.axvline(
    x=cp,
    color=&#39;gray&#39;,
    linestyle=&#39;--&#39;,
    label=f&#39;weight effect sign change ({cp: 0.1f})&#39;
)
# Estimated line. We take the inverse z-transform of the estimated beta coefficient.
beta_temp = linear_model_coef_df.query(&#39;linear_features == &quot;temp&quot;&#39;)[&#39;coef_&#39;].values[0]
ax.axline(
    xy1=(x_train[&#39;temp&#39;].mean(), 0),
    slope= beta_temp / x_train[&#39;temp&#39;].std(),
    color=&#39;black&#39;,
    linestyle=&#39;--&#39;,
    label=r&#39;estimated fit (z-transform $\beta_{temp}$)&#39;
)
ax.legend()
ax.set(title=&#39;Weight Effect Linear Model&#39;, xlabel=&#39;temp&#39;, ylabel=&#39;weight_effect&#39;);</code></pre>
<center>
<img src="../images/interpretable_ml_files/interpretable_ml_65_0.svg" title="fig:" alt="svg" />
</center>
<p><strong>Warning:</strong> This plot just shows the effect of the linear term <code>temp</code> and not the interactions.</p>
<p>We can do something similar to visualize the interaction of 2 features. For example for <code>temp</code> and <code>hum</code> we compute the total weight effect as
<span class="math display">\[
\beta_{temp}x_{temp} + \beta_{hum}x_{hum} + \beta_{temp \times hum}x_{temp} \times x_{hum}
\]</span></p>
<pre class="python"><code>import matplotlib.cm as cm

fig, ax = plt.subplots()
# Compute total weight effect.
sns.kdeplot(
    x=x_train[&#39;temp&#39;],
    y=x_train[&#39;hum&#39;],
    levels=10,
    hue=(linear_model_effects_df[&#39;temp&#39;]
         + linear_model_effects_df[&#39;hum&#39;]
         + linear_model_effects_df[&#39;temp hum&#39;]
    ) &gt; 0,
    hue_order=[True, False],
    palette=[
        cm.get_cmap(&#39;coolwarm_r&#39;)(1),
        cm.get_cmap(&#39;coolwarm&#39;)(1)
    ],
    alpha=0.2,
    fill=True,
    ax=ax
)
# Data Density.
sns.scatterplot(
    x=x_train[&#39;temp&#39;],
    y=x_train[&#39;hum&#39;],
    hue=(linear_model_effects_df[&#39;temp&#39;]
         + linear_model_effects_df[&#39;hum&#39;]
         + linear_model_effects_df[&#39;temp hum&#39;]
    ),
    palette=&#39;coolwarm&#39;,
    edgecolor=&#39;black&#39;,
    ax=ax
)
ax.legend(title=&#39;weight_effect&#39;, loc=&#39;lower left&#39;)
ax.set(title=&#39;Temperature and Humidity Interaction Weight Effect Linear Model&#39;);</code></pre>
<center>
<img src="../images/interpretable_ml_files/interpretable_ml_68_0.svg" title="fig:" alt="svg" />
</center>
<p>Similarly for <code>hum</code> and <code>windspeed</code>:</p>
<pre class="python"><code>fig, ax = plt.subplots()
# Compute total weight effect.
sns.kdeplot(
    x=x_train[&#39;hum&#39;],
    y=x_train[&#39;windspeed&#39;],
    levels=10,
    hue=(linear_model_effects_df[&#39;windspeed&#39;]
         + linear_model_effects_df[&#39;hum&#39;]
        + linear_model_effects_df[&#39;hum windspeed&#39;]
    ) &gt; 0,
    hue_order=[True, False],
    palette=[
        cm.get_cmap(&#39;coolwarm_r&#39;)(1),
        cm.get_cmap(&#39;coolwarm&#39;)(1)
    ],
    alpha=0.2,
    fill=True,
    ax=ax
)
# Data Density.
sns.scatterplot(
    x=x_train[&#39;hum&#39;],
    y=x_train[&#39;windspeed&#39;],
    hue=(linear_model_effects_df[&#39;windspeed&#39;]
         + linear_model_effects_df[&#39;hum&#39;]
         + linear_model_effects_df[&#39;hum windspeed&#39;]
    ),
    palette=&#39;coolwarm&#39;,
    edgecolor=&#39;black&#39;,
    ax=ax
)
ax.legend(title=&#39;weight_effect&#39;, loc=&#39;lower left&#39;)
ax.set(title=&#39;Humidity and Wind Speed Interaction Weight Effect Linear Model&#39;);</code></pre>
<center>
<img src="../images/interpretable_ml_files/interpretable_ml_70_0.svg" title="fig:" alt="svg" />
</center>
<p>We can also investigate how the model features affect individual predictions:</p>
<pre class="python"><code># Compare with FIGURE 5.49 in https://christophm.github.io/interpretable-ml-book/shapley.html.
obs_index = (285 - 1) # Python indexing starts in 0 and not 1 as in R.

fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(15, 7), constrained_layout=True)
# All features.
linear_model_effects_df.iloc[obs_index, ] \
    .to_frame() \
    .rename(columns={obs_index: &#39;effect&#39;}) \
    .query(&#39;effect != 0&#39;) \
    .reset_index(drop=False) \
    .sort_values(&#39;effect&#39;, ascending=False) \
    .pipe((sns.barplot, &#39;data&#39;), 
        x=&#39;effect&#39;,
        y=&#39;index&#39;,
        color=sns_c[3],
        ax=ax[0]
    )
ax[0].set(
    title=f&#39;Weight Effects for Observation {obs_index}&#39;,
    xlabel=&#39;weight effect&#39;,
    ylabel=&#39;&#39;
)
# Linear features.
linear_model_effects_df.iloc[obs_index, ] \
    .to_frame() \
    .rename(columns={obs_index: &#39;effect&#39;}) \
    .query(&#39;effect != 0 and index in @features_ext&#39;) \
    .reset_index(drop=False) \
    .sort_values(&#39;effect&#39;, ascending=False) \
    .pipe((sns.barplot, &#39;data&#39;), 
    x=&#39;effect&#39;,
    y=&#39;index&#39;,
    color=sns_c[4],
    ax=ax[1]
)
ax[1].set(
    title=f&#39;Weight Effects for Observation {obs_index} (linear terms)&#39;,
    xlabel=&#39;weight effect&#39;,
    ylabel=&#39;&#39;
);</code></pre>
<center>
<figure>
<img alt="MyImage" src="../images/interpretable_ml_files/interpretable_ml_72_0.svg" align="middle" style="width: 950px;">
</figure>
</center>
<pre class="python"><code># Input Features for this observation.
x_train.iloc[obs_index, ]</code></pre>
<pre><code>season                      WINTER
mnth                           OKT
holiday                 NO HOLIDAY
weekday                        WED
workingday             WORKING DAY
weathersit         RAIN/SNOW/STORM
temp                       17.5367
hum                         90.625
windspeed                   16.626
days_since_2011                284
yr                            2011
Name: 284, dtype: object</code></pre>
<p>Let us verify that these weight effects add up to the model prediction (including the intercept term):</p>
<pre class="python"><code>linear_model_effects_df.iloc[obs_index, ].sum()  \
    + linear_model_intercept \
    - (linear_grid_search.predict(x_train.iloc[obs_index, ].to_frame().T)[0])</code></pre>
<pre><code>0.0</code></pre>
</div>
<div id="tree-model" class="section level3">
<h3>Tree Model</h3>
<p>Single decision trees are also quite explicit about their interpretation. Moving to ensembles can be a bit tricky, I recommend the section <a href="https://xgboost.readthedocs.io/en/latest/tutorials/model.html">Introduction to Boosted Trees</a> on <a href="https://xgboost.readthedocs.io/en/latest/index.html">XGBoost documentation</a>. One of the highest benefits of tree ensembles is the ability to learn complex and non-linear relations from the data.</p>
<p>To begin, let us compute the preprocessing step output of the tree model:</p>
<pre class="python"><code>tree_x_train = pd.DataFrame(
    data=tree_grid_search.best_estimator_[&#39;tree_feature_engineering&#39;].transform(x_train),
    columns=features_ext
)

tree_x_train.shape</code></pre>
<pre><code>(584, 29)</code></pre>
<p>XGBoost model provides various measures of importance, see <a href="https://xgboost.readthedocs.io/en/latest/R-package/discoverYourData.html">Understand your dataset with XGBoost</a>. From the XGBoost documentation:</p>
<blockquote>
<ul>
<li><p><em><code>Gain</code> is the improvement in accuracy brought by a feature to the branches it is on.</em></p></li>
<li><p><em><code>Cover</code> measures the relative quantity of observations concerned by a feature.</em></p></li>
<li><p><em><code>Frequency</code> / <code>Weight</code> is a simpler way to measure the <code>Gain</code>. It just counts the number of times a feature is used in all generated trees.</em></p></li>
</ul>
</blockquote>
<p>See also <a href="https://towardsdatascience.com/be-careful-when-interpreting-your-features-importance-in-xgboost-6e16132588e7">The Multiple faces of ‘Feature importance’ in XGBoost</a>. Let us compute all of them and compare their relative values:</p>
<pre class="python"><code>importance_type = [
    &#39;weight&#39;,
    &#39;gain&#39;,
    &#39;cover&#39;,
    &#39;total_gain&#39;,
    &#39;total_cover&#39;
]
# Compute and format variable importance metrics.
tree_feature_importance_df = pd.concat(
    [
        pd.DataFrame.from_dict(
            data=(
                tree_grid_search
                .best_estimator_[&#39;tree_regressor&#39;]
                .get_booster()
                .get_score(importance_type=t)
            ), 
            orient=&#39;index&#39;,
            columns=[t]
        ) 
        for t in importance_type
    ],
    axis=1
)

tree_feature_importance_df = tree_feature_importance_df \
    .reset_index(drop=False) \
    .assign(
        index = lambda x: x[&#39;index&#39;].str.replace(pat=&#39;f&#39;, repl=&#39;&#39;).astype(int)
    )

# Map genertic features of the form f&lt;NUMBER&gt; to the original feature names.
tree_features_idx_map = tree_feature_importance_df[&#39;index&#39;].apply(lambda idx: features_ext[idx])

# Relative feature importance.
tree_feature_importance_rel_df = tree_feature_importance_df / tree_feature_importance_df.sum(axis=0)
tree_feature_importance_rel_df = tree_feature_importance_rel_df \
    .assign(feature = tree_features_idx_map) \
    .drop(&#39;index&#39;, axis=1)</code></pre>
<p>Let us plot the results:</p>
<pre class="python"><code>fig, ax = plt.subplots(figsize=(7, 15))
sns.barplot(
    x=&#39;value&#39;,
    y=&#39;feature&#39;,
    data=tree_feature_importance_rel_df.melt(id_vars=&#39;feature&#39;),
    hue=&#39;variable&#39;,
    dodge=True,
    ax=ax
)
ax.legend(title=&#39;importance type&#39;)
ax.xaxis.set_major_formatter(
    mtick.FuncFormatter(lambda y, _: f&#39;{y: .0%}&#39;)
)
ax.set(
    title=&#39;Relative Feature Importances of the Tree Model&#39;,
    xlabel=&#39;relative importance&#39;,
    ylabel=&#39;&#39;
);</code></pre>
<center>
<figure>
<img alt="MyImage" src="../images/interpretable_ml_files/interpretable_ml_81_0.svg" align="middle" style="width: 950px;">
</figure>
</center>
<p>It this tree model both <code>days_from_2011</code> and <code>temp</code> are important variables.</p>
<p><strong>Waring:</strong> Zero-importance features are not included.</p>
<pre class="python"><code>print(f&#39;&#39;&#39;
Zero-importance features:
{[x for x in features_ext if x not in tree_feature_importance_rel_df[&#39;feature&#39;].values]}
&#39;&#39;&#39;)</code></pre>
<pre><code>Zero-importance features:
[&#39;mnth_FEB&#39;, &#39;mnth_OKT&#39;, &#39;yr&#39;]</code></pre>
</div>
</div>
<div id="partial-dependence-plot-pdp-individual-conditional-expectation-ice" class="section level2">
<h2>Partial Dependence Plot (PDP) &amp; Individual Conditional Expectation (ICE)</h2>
<p>In this section we describe the first model-agnostic method to understand how features interact to generate predictions in a machine learning model. Some recommended references on the subject are:</p>
<ul>
<li><p>PDP</p>
<ul>
<li><p><a href="https://christophm.github.io/interpretable-ml-book/pdp.html">Section 5.1, Interpretable Machine Learning</a></p></li>
<li><p><a href="https://scikit-learn.org/stable/modules/partial_dependence.html">4.1.1. Partial dependence plots, scikit-learn docs</a></p></li>
</ul></li>
<li><p>ICE</p>
<ul>
<li><p><a href="https://christophm.github.io/interpretable-ml-book/ice.html">Section 5.2, Interpretable Machine Learning</a></p></li>
<li><p><a href="https://scikit-learn.org/stable/modules/partial_dependence.html#individual-conditional-expectation-ice-plot">4.1.2. Individual conditional expectation (ICE) plot, scikit-learn docs</a></p></li>
</ul></li>
</ul>
<p>Let us start by quoting the description of partial dependency plots from the <a href="https://scikit-learn.org/stable/modules/partial_dependence.html">scikit-learn docs</a>:
&gt; <em>Partial dependence plots (PDP) show the dependence between the target response and a set of input features of interest, marginalizing over the values of all other input features.</em></p>
<p>Let us be more concrete. For a regression problem (like in this example) we can estimate the partial dependence function (which is the plot of interest) as follows: Let <span class="math inline">\(x_S\)</span> be the features for which the partial dependence function should be plotted (usually not more than 2 variables) and <span class="math inline">\(x_C\)</span> be other features used in the machine learning model. One can estimate the partial dependence function as</p>
<p><span class="math display">\[
\hat{f}_{x_{S}}(x_{s}) = 
\frac{1}{n}
\sum_{i=1}^{n}
\hat{f}(x_{S}, x_{C}^{i})
\]</span></p>
<p>where <span class="math inline">\(\hat{f}\)</span> is the model prediction function, <span class="math inline">\(x_{C}^{i}\)</span> are actual feature values (not in <span class="math inline">\(S\)</span>) and <span class="math inline">\(n\)</span> is the number points. The following is one of the key assumptions of this method (see <a href="https://christophm.github.io/interpretable-ml-book/pdp.html">Section 5.1, Interpretable Machine Learning</a>)</p>
<blockquote>
<p><em>An assumption of the PDP is that the features in <span class="math inline">\(C\)</span> are not correlated with the features in <span class="math inline">\(S\)</span>. If this assumption is violated, the averages calculated for the partial dependence plot will include data points that are very unlikely or even impossible.</em></p>
</blockquote>
<p>In view of the correlation matrix computed above for the numeric features, we see that the assumption holds true. However, the categorical variables are not independent, e.g. <code>season</code> and <code>mnth</code>.</p>
<p>Similar to a PDP, an individual conditional expectation (ICE) plot shows one line per instance. That is, for each instance in <span class="math inline">\(\{(x_{S}^{i}, x_{C}^{i})\}_{i=1}^{n}\)</span>, we plot <span class="math inline">\(\hat{f}_{S}\)</span> as a function of <span class="math inline">\(x_{S}^{i}\)</span> while leaving <span class="math inline">\(x_{C}^{i}\)</span> fixed. Note that the additional information provided by ICE plots are interaction effects between the features. In PDPs these interactions are untraceable after the aggregation.</p>
<p>Let us plot these curves for the linear model:</p>
<pre class="python"><code>from sklearn.inspection import plot_partial_dependence

features_to_display = [&#39;temp&#39;, &#39;hum&#39;, &#39;windspeed&#39;]

fig, ax = plt.subplots(figsize=(15, 7))
display_linear = plot_partial_dependence(
       estimator=linear_grid_search,
       X=x_train,
       features=features_to_display,
       kind=&#39;both&#39;,
       subsample=50,
       n_jobs=3, 
       grid_resolution=20,
       random_state=42, 
       ax=ax
)
fig.suptitle(
       &#39;Single ICE Plot - Linear Model&#39;, y=0.95
);</code></pre>
<center>
<figure>
<img alt="MyImage" src="../images/interpretable_ml_files/interpretable_ml_85_0.svg" align="middle" style="width: 950px;">
</figure>
</center>
<p>As expected all the PDP curves are straight lines. Note however that there are light lines in the <code>temp</code> ICE plot, which indicate unseen interactions by aggregation in the PDP plot.</p>
<p>Let us now compute the PDP for pairs of numeric features.</p>
<pre class="python"><code>fig, ax = plt.subplots(figsize=(15, 7))
features_to_display = [
    (&#39;temp&#39;, &#39;hum&#39;),
    (&#39;temp&#39;, &#39;windspeed&#39;),
    (&#39;hum&#39;, &#39;windspeed&#39;)
]
display_linear = plot_partial_dependence(
       estimator=linear_grid_search,
       X=x_train,
       features=features_to_display,
       subsample=50,
       n_jobs=3, 
       grid_resolution=20,
       random_state=42, 
       contour_kw={&#39;cmap&#39;: &#39;viridis_r&#39;},
       ax=ax
)

fig.suptitle(
       &#39;Pair ICE Plot - Linear Model&#39;, y=0.95
);</code></pre>
<center>
<figure>
<img alt="MyImage" src="../images/interpretable_ml_files/interpretable_ml_87_0.svg" align="middle" style="width: 950px;">
</figure>
</center>
<p>Note that for the <code>temp</code> plots the relation with the other features seem quite linear (also compare with the weight effects scatter plots of the linear model above). On the other hand the <code>hum</code> vs <code>windspeed</code> PDP is not completely linear. Let us try to understand this by looking into the <span class="math inline">\(\beta\)</span> coefficients:</p>
<pre class="python"><code>linear_model_coef_df.query(
    &#39;linear_features.isin([&quot;temp&quot;, &quot;hum&quot;, &quot;windspeed&quot;, &quot;temp hum&quot;, &quot;temp windspeed&quot;, &quot;hum windspeed&quot;])&#39;
) \
.style.background_gradient(
        cmap=&#39;viridis_r&#39;,
        axis=0,
        subset=[&#39;abs_coef_&#39;]
    )</code></pre>
<center>
<figure>
<img alt="MyImage" src="../images/interpretable_ml_files/interaction_coef.png" align="middle" style="width: 500px;">
</figure>
</center>
<p>The largest <span class="math inline">\(\beta\)</span> coefficient of the interactions is indeed the one corresponding to <code>hum windspeed</code>.</p>
<p>Now let us generate the plots for the tree based model:</p>
<pre class="python"><code>features_to_display = [&#39;temp&#39;, &#39;hum&#39;, &#39;windspeed&#39;]

fig, ax = plt.subplots(figsize=(15, 7))
display_tree = plot_partial_dependence(
       estimator=tree_grid_search,
       X=x_train,
       features=features_to_display,
       kind=&#39;both&#39;,
       subsample=50,
       n_jobs=3, 
       grid_resolution=20,
       random_state=42,
       ax=ax
)
fig.suptitle(
       &#39;Single ICE Plot - Tree Model&#39;, y=0.95
);</code></pre>
<center>
<figure>
<img alt="MyImage" src="../images/interpretable_ml_files/interpretable_ml_92_0.svg" align="middle" style="width: 950px;">
</figure>
</center>
<p>The plot above reproduces <a href="https://christophm.github.io/interpretable-ml-book/ice.html">Figure 5.7 in Section 5.2, Interpretable Machine Learning</a>:</p>
<blockquote>
<p><em>For warm but not too hot weather, the model predicts on average a high number of rented bicycles. Potential bikers are increasingly inhibited in renting a bike when humidity exceeds 60%. In addition, the more wind the fewer people like to cycle, which makes sense. Interestingly, the predicted number of bike rentals does not fall when wind speed increases from 25 to 35 km/h, but there is not much training data, so the machine learning model could probably not learn a meaningful prediction for this range. At least intuitively, I would expect the number of bicycles to decrease with increasing wind speed, especially when the wind speed is very high.</em></p>
</blockquote>
<p>Let us now plot the pair PDP:</p>
<pre class="python"><code>fig, ax = plt.subplots(figsize=(15, 7))

features_to_display = [
    (&#39;temp&#39;, &#39;hum&#39;),
    (&#39;temp&#39;, &#39;windspeed&#39;),
    (&#39;hum&#39;, &#39;windspeed&#39;)
]

display_tree = plot_partial_dependence(
       estimator=tree_grid_search,
       X=x_train,
       features=features_to_display,
       subsample=50,
       n_jobs=3, 
       grid_resolution=20,
       random_state=42, 
       contour_kw={&#39;cmap&#39;: &#39;viridis_r&#39;},
       ax=ax
)
fig.suptitle(
       &#39;Pair ICE Plot - Tree Model&#39;, y=0.95
);</code></pre>
<center>
<figure>
<img alt="MyImage" src="../images/interpretable_ml_files/interpretable_ml_95_0.svg" align="middle" style="width: 950px;">
</figure>
</center>
<p>Let us now plot the first plot above in 3-dimensions (see <a href="https://scikit-learn.org/stable/auto_examples/inspection/plot_partial_dependence.html#id7">scikit-learn: 3D interaction plots</a>):</p>
<pre class="python"><code>from sklearn.inspection import partial_dependence
from mpl_toolkits.mplot3d import Axes3D

features_to_display = (&#39;temp&#39;, &#39;hum&#39;)

pdp = partial_dependence(
    estimator=tree_grid_search,
    X=x_train,
    features=features_to_display, 
    kind=&#39;average&#39;,
    grid_resolution=25
)

XX, YY = np.meshgrid(pdp[&#39;values&#39;][0], pdp[&#39;values&#39;][1])
Z = pdp.average[0].T

fig = plt.figure(figsize=(10, 6))
ax = Axes3D(fig)
surf = ax.plot_surface(
    XX,
    YY,
    Z, 
    rstride=1,
    cstride=1,
    cmap=&#39;viridis_r&#39;,
    edgecolor=&#39;black&#39;
)
ax.view_init(elev=7, azim=-60)
ax.set(
    title=&#39;Patial Dependency Plot temp vs humidity - Tree Model&#39;,
    xlabel=features_to_display[0], 
    ylabel=features_to_display[1],
    zlabel=&#39;partial dependence&#39;
)
fig.colorbar(surf);</code></pre>
<center>
<figure>
<img alt="MyImage" src="../images/interpretable_ml_files/interpretable_ml_97_0.svg" align="middle" style="width: 800px;">
</figure>
</center>
<p>Finally, let us compare the ICE plots of both models together:</p>
<pre class="python"><code>features_to_display = [&#39;temp&#39;, &#39;hum&#39;, &#39;windspeed&#39;]

fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(15, 7))

display_tree = plot_partial_dependence(
       estimator=tree_grid_search,
       X=x_train,
       features=features_to_display,
       kind=&#39;both&#39;,
       subsample=50,
       n_jobs=3, 
       grid_resolution=20,
       random_state=42, 
       line_kw={
           &#39;color&#39;: sns_c[1],
           &#39;label&#39;: &#39;tree_model(average)&#39;
        },
       ax=ax
)

display_linear = plot_partial_dependence(
       estimator=linear_grid_search,
       X=x_train,
       features=features_to_display,
       kind=&#39;both&#39;,
       subsample=50,
       n_jobs=3, 
       grid_resolution=20,
       random_state=42,
       line_kw={
           &#39;color&#39;: sns_c[0], &#39;label&#39;: 
           &#39;linear_model (average)&#39;
        },
       ax=ax
)
fig.suptitle(&#39;Single ICE Plot&#39;, y=0.95);</code></pre>
<center>
<figure>
<img alt="MyImage" src="../images/interpretable_ml_files/interpretable_ml_99_0.svg" align="middle" style="width: 950px;">
</figure>
</center>
<p>Other useful inspection plots are <em>accumulated local effects</em> (ALE) plots, see <a href="https://christophm.github.io/interpretable-ml-book/ale.html">Section 5.3, Interpretable Machine Learning</a>.</p>
</div>
<div id="permutation-importance" class="section level2">
<h2>Permutation Importance</h2>
<p>Next, let us discuss <em>permutation feature importance</em>, see <a href="https://christophm.github.io/interpretable-ml-book/feature-importance.html">Section 5.6, Interpretable Machine Learning</a>:</p>
<blockquote>
<p><em>Permutation feature importance measures the increase in the prediction error of the model after we permuted the feature’s values, which breaks the relationship between the feature and the true outcome.</em></p>
</blockquote>
<p><strong>Remark</strong> I encourage you to read <a href="https://christophm.github.io/interpretable-ml-book/feature-importance.html#feature-importance-data">Section 5.5.2, Interpretable Machine Learning</a> where the author discuss whether we should evaluate the feature importance in the train or the test set. In <a href="https://github.com/PacktPublishing/Interpretable-Machine-Learning-with-Python">Interpretable Machine Learning with Python by Serg Masís</a>, the author provides a similar discussion and decides to evaluate on the test set. In this notebook we do in on the training set in order to be able to reproduce the results in <a href="https://christophm.github.io/interpretable-ml-book/">Interpretable Machine Learning, A Guide for Making Black Box Models Explainable by Christoph Molnar</a>.</p>
<p>Let us compute the permutation feature importance for both models:</p>
<pre class="python"><code>from sklearn.inspection import permutation_importance

linear_pi = permutation_importance(
    estimator=linear_grid_search,
    X=x_train,
    y=y_train,
    n_repeats=10
)

tree_pi = permutation_importance(
    estimator=tree_grid_search,
    X=x_train,
    y=y_train,
    n_repeats=10
)</code></pre>
<p>Now lets plot them side-by-side:</p>
<pre class="python"><code>linear_perm_sorted_idx = linear_pi.importances_mean.argsort()[::-1]
tree_perm_sorted_idx = tree_pi.importances_mean.argsort()[::-1]

fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5), constrained_layout=True)

sns.barplot(
    x=linear_pi.importances_mean[linear_perm_sorted_idx],
    y=x_train.columns[linear_perm_sorted_idx],
    orient=&#39;h&#39;,
    color=sns_c[0],
    ax=ax[0]
)
ax[0].set(title=&#39;Linear Model&#39;);

sns.barplot(
    x=tree_pi.importances_mean[tree_perm_sorted_idx],
    y=x_train.columns[tree_perm_sorted_idx],
    orient=&#39;h&#39;,
    color=sns_c[1],
    ax=ax[1]
)
ax[1].set(title=&#39;Tree Model&#39;)

fig.suptitle(&#39;Permutation Importance&#39;);</code></pre>
<center>
<img src="../images/interpretable_ml_files/interpretable_ml_104_0.svg" title="fig:" alt="svg" />
</center>
<p>It is interesting to see that the permutation importance for these two models have <code>days_since_2021</code> and <code>temp</code> on their top 3 ranking, which partially explain the trend and seasonality components respectively (compare with <a href="https://christophm.github.io/interpretable-ml-book/feature-importance.html">FIGURE 5.32</a>). Also, these rankings mostly agree with the model-dependent feature importance metrics illustrated above, with the difference that these permutation importance rankings can be obtained at feature level and not per individual one-hot-instance.</p>
<p><strong>Warning:</strong> Similarly as partial dependency plots, permutation importance can also be biased by unlikely points in the input data distribution, see <a href="https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance_multicollinear.html#permutation-importance-with-multicollinear-or-correlated-features">Permutation Importance with Multicollinear or Correlated Features</a>.</p>
<p>In order to analyze the permutation importance ranking while considering the correlation, it useful to have a hierarchical clustering diagram generated from a similarity metric. For example, we can use <a href="https://en.wikipedia.org/wiki/Spearman%27s_rank_correlation_coefficient">Spearman’s rank correlation coefficient</a> (to include categorical variables) as suggested in <a href="https://scikit-learn.org/stable/auto_examples/inspection/plot_permutation_importance_multicollinear.html#handling-multicollinear-features">scikit-learn: Handling Multicollinear Features</a></p>
<pre class="python"><code>from scipy.stats import spearmanr
from scipy.cluster import hierarchy

corr = spearmanr(a=x_train).correlation
corr_linkage = hierarchy.ward(y=corr)

fig, ax = plt.subplots(figsize=(7, 6))
dendro = hierarchy.dendrogram(
    Z=corr_linkage,
    labels=x_train.columns, 
    orientation=&#39;right&#39;,
    ax=ax
)
ax.set(title=&quot;Hierarchical Clustering (Ward) based on Spearman&#39;s correlation&quot;);</code></pre>
<center>
<img src="../images/interpretable_ml_files/interpretable_ml_106_0.svg" title="fig:" alt="svg" />
</center>
<p>Note for example how related features like <code>days_since_2011</code> and <code>yr</code> differ on their rankings for the tree model. Similarly for <code>mnth</code> and <code>season</code> in the linear model.</p>
</div>
<div id="shap" class="section level2">
<h2>SHAP</h2>
<p>SHAP (SHapley Additive exPlanations) (<a href="https://christophm.github.io/interpretable-ml-book/shap.html">Section 5.11, Interpretable Machine Learning</a>) are based on the concept of <a href="https://en.wikipedia.org/wiki/Shapley_value">Shapley Values</a> (see <a href="https://christophm.github.io/interpretable-ml-book/shapley.html">Section 5.9, Interpretable Machine Learning</a>), which have their origin in game theory. The main idea is to consider the model as a game (prediction task) and each model feature as a player. Features can be grouped in teams (coalitions) to play thr game, i.e. generate predictions. It is important to remark that the “gain” is the actual prediction for this for the coalition minus the average prediction for all instances, so that:</p>
<blockquote>
<p><em>The Shapley value is the average contribution of a feature value to the prediction in different coalitions.</em></p>
<p><strong>Warning:</strong> <em>The Shapley value is NOT the difference in prediction when we would remove the feature from the model.</em></p>
</blockquote>
<p>Please visit the references provided for a detailed explanation on Shapley Values.</p>
<p>How are SHAP values connected to Shapley Values? The idea is to compute SHAP values from local linear models (compare with <a href="https://christophm.github.io/interpretable-ml-book/lime.html">LIME</a>). Concretely (<a href="https://christophm.github.io/interpretable-ml-book/shap.html#kernelshap">KernelShap</a>), for each instance <span class="math inline">\(x\)</span></p>
<blockquote>
<ol style="list-style-type: decimal">
<li><p>Sample coalitions <span class="math inline">\(z_{k}&#39;\in\{0, 1\}^{M}\)</span>, where <span class="math inline">\(M\)</span>, is the maximum coalition size.</p></li>
<li><p>Get prediction for each <span class="math inline">\(z_{k}&#39;\)</span>. For features not in the coalition we replace their values with random samples from the dataset (background data).</p></li>
<li><p>Compute the weight for each <span class="math inline">\(z_{k}&#39;\)</span>, with the SHAP kernel,</p></li>
</ol>
<p><span class="math display">\[\pi_{x}(z&#39;) = \frac{(M-1)}{\binom{M}{|z&#39;|}|z&#39;|(M-|z&#39;|)}\]</span></p>
<ol start="4" style="list-style-type: decimal">
<li><p>Fit weighted linear model.</p></li>
<li><p>Return Shapley values, i.e. the coefficients from the linear model.</p></li>
</ol>
</blockquote>
<p>We are going to use the python <a href="https://shap.readthedocs.io/en/latest/index.html">SHAP</a> package.</p>
<div id="linear-model-1" class="section level3">
<h3>Linear Model</h3>
<p>We can use the general <a href="https://github.com/slundberg/shap/blob/master/shap/explainers/_kernel.py">KernelShap</a>, a generic way to compute the SHAP values as follows:</p>
<pre class="python"><code>import shap

# Get background data: to sample from whenever a feature is missing in a coalition. 
linear_x_train_summary = shap.kmeans(X=linear_x_train, k=20)

linear_shap_explainer = shap.KernelExplainer(
  model=linear_grid_search.best_estimator_[&#39;linear_regressor&#39;].predict,
  data=linear_x_train_summary,
)</code></pre>
<p>Nevertheless, we can also use optimized implementations for each model type.</p>
<pre class="python"><code>linear_shap_explainer = shap.LinearExplainer(
    model=linear_grid_search.best_estimator_[&#39;linear_regressor&#39;],
    masker=shap.maskers.Independent(data=linear_x_train, max_samples=500) # Background data.
)

linear_shap_values = linear_shap_explainer(linear_x_train)</code></pre>
<p>Let us plot the SHAP values for the linear model:</p>
<pre class="python"><code>shap.summary_plot(
    shap_values=linear_shap_values,
    features=linear_x_train, 
    show=False
)
plt.title(f&#39;Linear SHAP&#39;);</code></pre>
<center>
<figure>
<img alt="MyImage" src="../images/interpretable_ml_files/interpretable_ml_114_0.svg" align="middle" style="width: 950px;">
</figure>
</center>
<p>We can also plot the mean of the absolute value of the SHAP values to get a summary:</p>
<pre class="python"><code>shap.plots.bar(shap_values=linear_shap_values, max_display=20, show=False)
plt.title(&#39;SHAP Values Aggregation Linear Model&#39;);</code></pre>
<center>
<figure>
<img alt="MyImage" src="../images/interpretable_ml_files/interpretable_ml_116_0.svg" align="middle" style="width: 950px;">
</figure>
</center>
<p>Here are some remarks on the results:
- <code>temp mnth_JUL</code> interaction term is the one with highest SHAP values. This feature has the largest <span class="math inline">\(\beta\)</span> coefficient in absolute value.
- The feature <code>weathersit_RAIN/SNOW/STORM</code> has a large beta coefficient (absolute value), but it is not on the top SHAP features.
- The SHAP values ranking is similar to the one obtained using permutation importance.</p>
<p>Let us now plot the SHAP values for as a function of <code>temp</code>:</p>
<pre class="python"><code>idx_1 = np.argwhere(np.array(linear_features) == &#39;temp&#39;)[0][0]

shap.plots.scatter(
    shap_values=linear_shap_values[:, idx_1]
)</code></pre>
<center>
<figure>
<img alt="MyImage" src="../images/interpretable_ml_files/interpretable_ml_119_0.svg" align="middle" style="width: 700px;">
</figure>
</center>
<p>The plot is very similar to the weight effect plot above except we have now the scaled <code>tmp</code> on the x-axis. Note that this line passes through the origin (as expected).</p>
<p>We can also plot the explanation for an individual data instance. For example:</p>
<pre class="python"><code>shap.plots.waterfall(
    shap_values=linear_shap_values[obs_index],
    max_display=20,
    show=False
)
plt.title(f&#39;Linear SHAP effects Observation {obs_index}&#39;);</code></pre>
<center>
<figure>
<img alt="MyImage" src="../images/interpretable_ml_files/interpretable_ml_122_0.svg" align="middle" style="width: 950px;">
</figure>
</center>
<p>Note that the top negative features are <code>weathersit_RAIN/SNOW/STORM</code> and <code>hum</code> as in the weight effect plot for the linear model above. Note however that the top positive feature is <code>mnth_JUL temp</code>, which is quite strange as the data instance is from <code>mnth=OKT</code>. Here is a way to get just the features with non-zero entries:</p>
<pre class="python"><code>from shap._explanation import Explanation

mask = linear_shap_values[obs_index].data != 0
linear_shap_values_mask = linear_shap_values[obs_index][mask]

explanation_mask = Explanation(
        values=linear_shap_values_mask.values, 
        base_values=linear_shap_values_mask.base_values,
        data=linear_shap_values_mask.data,
        feature_names=np.array(linear_shap_values[obs_index].feature_names)[mask]
    )

shap.plots.waterfall(
    shap_values=explanation_mask,
    max_display=20,
    show=False
)
plt.title(f&#39;Linear SHAP effects Observation {obs_index} (non-zero features)&#39;);</code></pre>
<center>
<figure>
<img alt="MyImage" src="../images/interpretable_ml_files/interpretable_ml_124_0.svg" align="middle" style="width: 950px;">
</figure>
</center>
</div>
<div id="tree-model-1" class="section level3">
<h3>Tree Model</h3>
<p>We can proceed similarly for the tree model:</p>
<pre class="python"><code>tree_shap_explainer = shap.TreeExplainer(
    model=tree_grid_search.best_estimator_[&#39;tree_regressor&#39;],
    masker=shap.maskers.Independent(data=tree_x_train, max_samples=500)

)

tree_shap_values = tree_shap_explainer(tree_x_train)</code></pre>
<pre class="python"><code>shap.summary_plot(
    shap_values=tree_shap_values,
    features=tree_x_train,
    show=False
)
plt.title(f&#39;Tree SHAP&#39;);</code></pre>
<center>
<figure>
<img alt="MyImage" src="../images/interpretable_ml_files/interpretable_ml_127_0.svg" align="middle" style="width: 950px;">
</figure>
</center>
<pre class="python"><code>shap.plots.bar(
    shap_values=tree_shap_values,
    max_display=20,
    show=False
)
plt.title(&#39;SHAP Values Aggregation Tree Model&#39;);</code></pre>
<center>
<figure>
<img alt="MyImage" src="../images/interpretable_ml_files/interpretable_ml_128_0.svg" align="middle" style="width: 950px;">
</figure>
</center>
<p>Here are some remarks on the results:</p>
<ul>
<li>The SHAP ranking looks quite similar to the tree-native feature importance metrics.</li>
<li>Similarly, this SHAP ranking is quite similar to the one obtained via permutation importance.</li>
</ul>
<p>Let us now plot the SAHP values as a function of <code>temp</code> and <code>hum</code>.</p>
<pre class="python"><code>idx_1 = np.argwhere(np.array(features_ext) == &#39;temp&#39;)[0][0]
idx_2 = np.argwhere(np.array(features_ext) == &#39;hum&#39;)[0][0]

shap.plots.scatter(
    shap_values=tree_shap_values[:, idx_1],
    color=tree_shap_values[:, idx_2]
)</code></pre>
<center>
<figure>
<img alt="MyImage" src="../images/interpretable_ml_files/interpretable_ml_131_0.svg" align="middle" style="width: 700px;">
</figure>
</center>
<p>This plot looks quite similar to the PDP above.</p>
<p>Now let us take a look into the SHAP explanation for the single data instance:</p>
<pre class="python"><code>shap.plots.waterfall(
    shap_values=tree_shap_values[obs_index],
    max_display=20,
    show=False
)
plt.title(f&#39;Tree SHAP effects Observation {obs_index}&#39;);</code></pre>
<center>
<figure>
<img alt="MyImage" src="../images/interpretable_ml_files/interpretable_ml_134_0.svg" align="middle" style="width: 950px;">
</figure>
</center>
<p>The explanation looks very similar to the linear model: <code>weathersit_RAIN/SNOW/STORM</code>, <code>hum</code>, <code>days_since_2011</code> and <code>windspeed</code> the highest negative weight effects and <code>temp</code> the main positive weight effect.</p>
<p>There are many other plots available, see <a href="https://shap.readthedocs.io/en/latest/api_examples.html#plots">SHAP documentation</a>.</p>
</div>
</div>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="../index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="../images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/dockerfile.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/bash.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="../js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-122570825-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

  </body>
</html>

