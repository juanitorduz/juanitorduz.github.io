<!DOCTYPE html>
<html lang="en-us">
  <head>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.58.3" />


<title>Introduction to Bayesian Modeling with PyMC3 - Dr. Juan Camilo Orduz</title>
<meta property="og:title" content="Introduction to Bayesian Modeling with PyMC3 - Dr. Juan Camilo Orduz">


  <link href='../images/favicon.ico' rel='icon' type='image/x-icon'/>



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/styles/atom-one-light.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="../css/fonts.css" media="all">
<link rel="stylesheet" href="../css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="../" class="nav-logo">
    <img src="../images/sphere2.gif"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="../about/">About</a></li>
    
    <li><a href="https://github.com/juanitorduz">GitHub</a></li>
    
    <li><a href="https://www.linkedin.com/in/juanitorduz/">LinkedIn</a></li>
    
    <li><a href="https://twitter.com/juanitorduz">Twitter</a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">15 min read</span>
    

    <h1 class="article-title">Introduction to Bayesian Modeling with PyMC3</h1>

    
    <span class="article-date">2017-08-13</span>
    

    <div class="article-content">
      


<p>This post is devoted to give an introduction to Bayesian modeling using <a href="https://pymc-devs.github.io/pymc3/notebooks/getting_started.html">PyMC3</a>, an open source probabilistic programming framework written in Python. Part of this material was presented in the Python Users Berlin (PUB) meet up.</p>
<center>
<img src="../images/PyMC3_banner.svg" alt="html" style="width: 400px;"/>
</center>
<p>Why PyMC3? As described in the documentation:</p>
<ul>
<li><p>PyMC3’s user-facing features are written in pure Python, it leverages <a href="http://deeplearning.net/software/theano/">Theano</a> to transparently transcode models to C and compile them to machine code, thereby boosting performance.</p></li>
<li><p>Theano is a library that allows expressions to be defined using generalized vector data structures called tensors, which are tightly integrated with the popular <a href="http://www.numpy.org/">NumPy</a> ndarray data structure.</p></li>
</ul>
<p>In addition, from a practical point of view, PyMC3 syntax is very transpartent from the mathematical point of view.</p>
<p>This post is not aimed to give a full treatment of the <del>mathematical details</del>, as there are many good (complete and detailed) references around these topics. Also, we are not going to dive deep into PyMC3 as all the details can be found in the documentation. Instead, we are interested in giving an overview of the basic mathematical consepts combinded with examples (writen in Python code) which should make clear why <a href="https://en.wikipedia.org/wiki/Monte_Carlo_method">Monte Carlo</a> simulations are useful in Bayesian modeling.</p>
<div id="mathematical-background" class="section level1">
<h1>1. Mathematical Background</h1>
</div>
<div id="bayes-theorem" class="section level1">
<h1>1.1 Bayes Theorem</h1>
<div id="frequentist-vs-bayesian" class="section level2">
<h2>Frequentist vs Bayesian</h2>
<p><em>The essential difference between frequentist inference and Bayesian inference is the same as the difference between the two interpretations of what a “probability” means</em>.</p>
<p><strong>Frequentist inference</strong> is a method of statistical inference in which conclusions from data is obtained by emphasizing the frequency or proportion of the data.</p>
<p><strong>Bayesian inference</strong> is a method of statistical inference in which Bayes’ theorem is used to update the probability for a hypothesis as more evidence or information becomes available.</p>
</div>
<div id="conditional-probability" class="section level2">
<h2>Conditional Probability</h2>
<p>Let <span class="math inline">\(A\)</span> and <span class="math inline">\(B\)</span> be two events, then the <em>conditional probability</em> of <span class="math inline">\(A\)</span> given <span class="math inline">\(B\)</span> is defined as the ratio</p>
<p><span class="math display">\[\begin{equation}
P(A|B):=\frac{P(A\cap B)}{P(B)}
\end{equation}\]</span></p>
<p><em>Remark:</em> Formally we have a <a href="https://en.wikipedia.org/wiki/Probability_space">probability space</a> <span class="math inline">\((\Omega, \mathcal{F}, P)\)</span>, where <span class="math inline">\(\Omega\)</span> is the sample space, <span class="math inline">\(\mathcal{F}\)</span> is a <span class="math inline">\(\sigma\)</span>-algebra on <span class="math inline">\(\Omega\)</span> and <span class="math inline">\(P\)</span> is a probability measure. The events <span class="math inline">\(A\)</span>, and <span class="math inline">\(B\)</span> are elements of <span class="math inline">\(\mathcal{F}\)</span> and we assume that <span class="math inline">\(P(B)\neq 0\)</span>.</p>
<p>Observe in particular</p>
<p><span class="math display">\[\begin{equation}
P(A|B)P(B)=P(A\cap B)=P(B\cap A) = P(B|A)P(A)
\end{equation}\]</span></p>
</div>
<div id="bayes-theorem-1" class="section level2">
<h2>Bayes Theorem</h2>
<p>From the last formula we obtain the relation</p>
<p><span class="math display">\[\begin{equation}
P(A|B)=\frac{P(B|A)P(A)}{P(B)}
\end{equation}\]</span></p>
<p>which is known as <a href="https://en.wikipedia.org/wiki/Bayes%27_theorem">Bayes Theorem</a>.</p>
<p><strong>Example:</strong> Suppose you are in the U-Bahn and you see a person with long hair. You want to know the probablity that this person is a woman. Consider the events <span class="math inline">\(A=\)</span> woman <span class="math inline">\(B=\)</span> long hair. You want to compute <span class="math inline">\(P(A|B)\)</span>. Suppose that you estimate <span class="math inline">\(P(A)=0.5\)</span>, <span class="math inline">\(P(B)=0.4\)</span> and <span class="math inline">\(P(B|A)=0.7\)</span> (the probability that a woman has long hair). Then, given these prior estimated probabilities, Bayes theorem gives</p>
<p><span class="math display">\[\begin{equation}
P(A|B)=\frac{P(B|A)P(A)}{P(B)} = \frac{0.7\times 0.5}{0.4} = 0.875.
\end{equation}\]</span></p>
</div>
<div id="bayesian-approach-to-data-analysis" class="section level2">
<h2>Bayesian Approach to Data Analysis</h2>
<p>Assume that you have a sample of observations <span class="math inline">\(y_1,..., y_n\)</span> of a random variable <span class="math inline">\(Y\sim f(y|\theta)\)</span>, where <span class="math inline">\(\theta\)</span> is a parameter for the distribution. Here we consider <span class="math inline">\(\theta\)</span> as a random variable as well. Following Bayes Theorem (its continuous version) we can write:</p>
<p><span class="math display">\[\begin{equation}
f(\theta|y)=\frac{f(y|\theta)f(\theta)}{f(y)} = 
\displaystyle{\frac{f(y|\theta)f(\theta)}{\int f(y|\theta)f(\theta)d\theta}}
\end{equation}\]</span></p>
<ul>
<li><p>The function <span class="math inline">\(f(y|\theta)\)</span> is called the <em>likelihood</em>.</p></li>
<li><p><span class="math inline">\(f(\theta)\)</span> is the <em>prior</em> distribution of <span class="math inline">\(\theta\)</span>.</p></li>
</ul>
<p>Note that <span class="math inline">\(f(y)\)</span> <em>does not</em> depend on <span class="math inline">\(\theta\)</span> (just on the data), thus it can be considered as a “normalizing constant”. In addition, it is often the case that the integral above is not easy to compute. Nevertheless, it is enough to consider the relation:</p>
<p><span class="math display">\[\begin{equation}
f(\theta|y)  \propto \text{likelihood} \times \text{prior}.
\end{equation}\]</span></p>
<p>(Here <span class="math inline">\(\propto\)</span> denotes the proportionality relation)</p>
</div>
<div id="example-poisson-data" class="section level2">
<h2>Example: Poisson Data</h2>
<p>In order to give a better sense of the relation above we are going to study a concrete example. Consider <span class="math inline">\(n\)</span> samples of <span class="math inline">\(Y\sim \text{Poiss}(\lambda)\)</span>. Recall that the <a href="https://en.wikipedia.org/wiki/Poisson_distribution">Poisson distribution</a> is given by:</p>
<p><span class="math display">\[
\displaystyle{
f(y_i|\lambda)=\frac{e^{-\lambda}\lambda^{y_i}}{y_i!}
}
\]</span></p>
<p>where <span class="math inline">\(\lambda&gt;0\)</span>. It is easy to verify that the <em>expected value</em> and <em>variance</em> are <span class="math inline">\(\text{E}(Y)=\lambda\)</span> and <span class="math inline">\(\text{Var}(Y)=\lambda\)</span> respectively. Parallel to the formal discussion, we are going to implement a numerical simulation:</p>
<pre class="python"><code>import numpy as np
import scipy.stats as ss

# We set a seed so that the results are reproducible.
np.random.seed(5)

# number of samples.
n = 100

# true parameter.
lam_true = 2

# sample array.
y = np.random.poisson(lam=lam_true, size=n)

y</code></pre>
<pre><code>array([2, 4, 1, 0, 2, 2, 2, 2, 1, 1, 3, 2, 0, 1, 3, 3, 4, 2, 0, 0, 3, 6,
       1, 2, 1, 2, 5, 2, 3, 0, 1, 3, 1, 4, 1, 2, 4, 0, 6, 4, 1, 2, 2, 0,
       1, 2, 4, 4, 1, 3, 0, 3, 3, 2, 4, 2, 2, 1, 1, 2, 5, 2, 3, 0, 1, 1,
       1, 3, 4, 1, 3, 4, 2, 1, 2, 4, 2, 2, 1, 0, 2, 2, 3, 0, 3, 3, 4, 2,
       2, 1, 2, 1, 3, 0, 1, 0, 3, 3, 1, 2])</code></pre>
<pre class="python"><code># mean of the sample.
y.mean()</code></pre>
<pre><code>2.06</code></pre>
<pre class="python"><code>import matplotlib.pyplot as plt
import seaborn as sns; sns.set()
%matplotlib inline

# Histogram of the sample.
plt.figure(figsize=(8, 6))
plt.hist(y, bins=15)
plt.title(&#39;Histogram of Simulated Data&#39;);</code></pre>
<center>
<img src="../images/intro_PyMC3_files/intro_PyMC3_8_0.png" alt="png" />
</center>
<div id="prior-gamma-distribution" class="section level3">
<h3>Prior: Gamma Distribution</h3>
<p>Let us consider a <a href="https://en.wikipedia.org/wiki/Gamma_distribution">gamma</a> prior distribution for the parameter <span class="math inline">\(\lambda \sim \Gamma(a,b)\)</span>. Recall that the density function for the gamma distribution is</p>
<p><span class="math display">\[\begin{equation}
f(\lambda)=\frac{b^a}{\Gamma(a)}\lambda^{a-1} e^{-b\lambda}
\end{equation}\]</span></p>
<p>where <span class="math inline">\(a&gt;0\)</span> is the <em>shape</em> parameter and <span class="math inline">\(b&gt;0\)</span> is the <em>rate parameter</em>. One verifies that</p>
<p><span class="math display">\[
\text{E}(\lambda)=\frac{a}{b}
\quad
\text{and}
\quad
\text{Var}(\lambda)=\frac{a}{b^2}
\]</span></p>
<p>Let us plot a gamma distribution for parameters <span class="math inline">\(a=3.5\)</span> and <span class="math inline">\(b=2\)</span>.</p>
<pre class="python"><code># Parameters of the prior gamma distribution.
a = 3.5 # shape
b = 2 # rate = 1/scale

x = np.linspace(start=0, stop=10, num=100)

plt.figure(figsize=(8, 6))
plt.plot(x, ss.gamma.pdf(x,a=a,scale=1/b), &#39;r-&#39;)
plt.title(&#39;Gamma Density Function for a={} and b={}&#39;.format(a,b))

# Define the prior distribution.
prior = lambda x: ss.gamma.pdf(x, a=a, scale=1/b)</code></pre>
<center>
<img src="../images/intro_PyMC3_files/intro_PyMC3_10_0.png" alt="png" />
</center>
</div>
<div id="likelihood" class="section level3">
<h3>Likelihood</h3>
<p>As the observations are independent the <a href="https://en.wikipedia.org/wiki/Likelihood_function">likelihood</a> function is</p>
<p><span class="math display">\[\begin{align}
f(y|\lambda)=&amp;\prod_{i=1}^{n} \frac{e^{-\lambda}\lambda^{y_i}}{y_i!}
=\frac{e^{-n\lambda}\lambda^{\sum_{i=1}^n y_i}}{\prod_{i=1}^{n}y_i!}
\end{align}\]</span></p>
<pre class="python"><code>import scipy.special as sp

# Define the likelihood function.
def likelihood(lam,y):
    
    factorials = np.apply_along_axis(
        lambda x: sp.gamma(x+1),
        axis=0,
        arr=y
    )
    
    numerator = np.exp(-lam*y.size)*(lam**y.sum())
    
    denominator = np.multiply.reduce(factorials)
    
    return numerator/denominator  </code></pre>
</div>
<div id="posterior-distribution-for-lambda-up-to-a-constant" class="section level3">
<h3>Posterior distribution for <span class="math inline">\(\lambda\)</span> up to a constant</h3>
<p>As we are just interested in the structure of the posterior distribution, up to a constant, we see</p>
<p><span class="math display">\[\begin{align}
f(\lambda|y)\propto &amp; \text{likelihood} \times \text{prior}\\
\propto &amp; \quad f(y|\lambda)f(\lambda)\\
\propto &amp; \quad e^{-n\lambda}\lambda^{\sum_{i=1}^n y_i} \lambda^{a-1} e^{-b\lambda}\\
\propto &amp; \quad \lambda^{\left(\sum_{i=1}^n y_i+a\right)-1} e^{-(n+b)\lambda}\\
\end{align}\]</span></p>
<pre class="python"><code># Define the posterior distribution.
# (up to a constant)
def posterior_up_to_constant(lam,y):
    return likelihood(lam,y)*prior(lam)

# Plot of the prior and (scaled) posterior distribution
# for the parameter lambda.
#
# We multiply the posterior distrubution function
# by the amplitude factor 2.5e74 to make it comparable
# with the prior gamma distribution.
plt.figure(figsize=(8, 6))
plt.plot(x, 2.0e74*posterior_up_to_constant(x,y), label=&#39;posterior&#39;)
plt.plot(x, ss.gamma.pdf(x,a=a,scale=1/b), &#39;r-&#39;, label=&#39;prior&#39;)
plt.legend();</code></pre>
<center>
<img src="../images/intro_PyMC3_files/intro_PyMC3_14_0.png" alt="png" />
</center>
</div>
<div id="true-posterior-distribution-for-lambda" class="section level3">
<h3>True posterior distribution for <span class="math inline">\(\lambda\)</span></h3>
<p>In fact, as <span class="math inline">\(f(\lambda|y) \propto\: \lambda^{\left(\sum_{i=1}^n y_i+a\right)-1} e^{-(n+b)\lambda}\)</span>, one verifies that the posterior distribution is again a gamma</p>
<p><span class="math display">\[\begin{align}
f(\lambda|y) = \Gamma\left(\sum_{i=1}^n y_i+a, n+b\right)
\end{align}\]</span></p>
<p>This means that the gamma and Poisson distribution form a <a href="https://en.wikipedia.org/wiki/Conjugate_prior">conjugate pair</a>.</p>
<pre class="python"><code>def posterior(lam,y):
    
    shape = a + y.sum()
    rate = b + y.size
    
    return ss.gamma.pdf(lam, shape, scale=1/rate)

plt.figure(figsize=(8, 6))
plt.plot(x, posterior(x,y))
plt.plot(x, ss.gamma.pdf(x,a=a,scale=1/b), &#39;r-&#39;)
plt.title(&#39;Prior and Posterior Distributions&#39;);</code></pre>
<center>
<img src="../images/intro_PyMC3_files/intro_PyMC3_16_0.png" alt="png" />
</center>
<p>We indeed see how the posterior distribution is concentrated around the true parameter <span class="math inline">\(\lambda=2\)</span>.</p>
<p>Note that the posterior mean is</p>
<p><span class="math display">\[\begin{align}
\frac{\sum_{i=1}^n y_i+a}{n+b} = \frac{b}{n+b}\frac{a}{b}+\frac{n}{n+b}\frac{\sum_{i=1}^n y_i}{n}
\end{align}\]</span></p>
<p>That is, it is a weighted average of the prior mean <span class="math inline">\(a/b\)</span> and the sample average <span class="math inline">\(\bar{y}\)</span>. As <span class="math inline">\(n\)</span> increases,</p>
<p><span class="math display">\[\begin{align}
\lim_{n\rightarrow +\infty}\frac{b}{n+b}\frac{a}{b}+\frac{n}{n+b}\frac{\sum_{i=1}^n y_i}{n} = \bar{y}.
\end{align}\]</span></p>
<pre class="python"><code># Posterior gamma parameters.
shape = a + y.sum()
rate = b + y.size

# Posterior mean.
shape/rate</code></pre>
<pre><code>2.053921568627451</code></pre>
</div>
</div>
</div>
<div id="markov-chain-monte-carlo-mcmc-approach" class="section level1">
<h1>1.2 Markov Chain Monte Carlo (MCMC) Approach</h1>
<p>In the last example the posterior distribution was easy to identify. However, in paractice this is not usually the case and therefore, via Bayes Theorem, we would only know the posterior distribution up to a constant. This motivates the idea of using Monte Carlo simulation methods. How can we sample from a distribution that we do not know? The Metropolis–Hastings algorithm, explained next, is one approach to tackle this problem.</p>
<div id="metropolishastings-algorithm" class="section level2">
<h2>Metropolis–Hastings algorithm</h2>
<p>Let <span class="math inline">\(\phi\)</span> be a function that is proportional to the desired probability distribution <span class="math inline">\(f\)</span>.</p>
<p><strong>Initialization:</strong></p>
<p>Pick <span class="math inline">\(x_{0}\)</span> to be the first sample, and choose an arbitrary probability density</p>
<p><span class="math display">\[\begin{equation}
g(x_{n+1}| x_{n})
\end{equation}\]</span></p>
<p>that suggests a candidate for the next sample value <span class="math inline">\(x_{n+1}\)</span>. Assume <span class="math inline">\(g\)</span> is symmetric.</p>
<p><strong>For each iteration:</strong></p>
<p>Generate a candidate <span class="math inline">\(x\)</span> for the next sample by picking from the distribution <span class="math inline">\(g(x|x_n)\)</span>. Calculate the <em>acceptance ratio</em></p>
<p><span class="math display">\[\begin{equation}
\alpha := \frac{f(x)}{f(x_n)} = \frac{\phi(x)}{\phi(x_n)}
\end{equation}\]</span></p>
<p>If <span class="math inline">\(\alpha \geq 1\)</span>, automatically accept the candidate by setting</p>
<p><span class="math display">\[\begin{equation}
x_{n+1} = x.
\end{equation}\]</span></p>
<p>Otherwise, accept the candidate with probability <span class="math inline">\(\alpha\)</span>. If the candidate is rejected, set</p>
<p><span class="math display">\[\begin{equation}
x_{n+1} = x_{n}.
\end{equation}\]</span></p>
<p>Why does this algorithm solve the initial problem? The full explanation is beyond the scope of this post (some references are provided at the end). It relies in the in the following result.</p>
</div>
<div id="ergodic-theorem-for-markov-chains" class="section level2">
<h2>Ergodic Theorem for Markov Chains</h2>
<p><strong>Theorem (Ergodic Theorem for Markov Chains)</strong> If <span class="math inline">\(\{x^{(1)} , x^{(2)} , . . .\}\)</span> is an <em>irreducible</em>, <em>aperiodic</em> and <em>recurrent</em> <a href="https://en.wikipedia.org/wiki/Markov_chain">Markov chain</a>, then there is a unique probability distribution <span class="math inline">\(\pi\)</span> such that as <span class="math inline">\(N\longrightarrow\infty\)</span>,</p>
<ul>
<li><span class="math inline">\(P(x^{(N)} ∈ A) \longrightarrow \pi(A)\)</span>.</li>
<li><span class="math inline">\(\displaystyle{\frac{1}{N}\sum_{n=1}^{N} g(x^{(n)}) \longrightarrow \int g(x)\pi(x) dx }\)</span>.</li>
</ul>
<p><em>Recall:</em></p>
<ul>
<li><p>A Markov chain is said to be <strong>irreducible</strong> if it is possible to get to any state from any state.</p></li>
<li><p>A state <span class="math inline">\(n\)</span> has <strong>period</strong> <span class="math inline">\(k\)</span> if any return to state <span class="math inline">\(n\)</span> must occur in multiples of <span class="math inline">\(k\)</span> time steps.</p></li>
<li><p>If <span class="math inline">\(k=1\)</span>, then the state is said to be <strong>aperiodic</strong>.</p></li>
<li><p>A state <span class="math inline">\(n\)</span> is said to be <strong>transient</strong> if, given that we start in state <span class="math inline">\(n\)</span>, there is a non-zero probability that we will never return to <span class="math inline">\(i\)</span>.</p></li>
<li><p>A state <span class="math inline">\(n\)</span> is <strong>recurrent</strong> if it is not transient.</p></li>
</ul>
</div>
</div>
<div id="pymc3-syntax" class="section level1">
<h1>2. PyMC3 Syntax</h1>
<p>Now we perform a MCMC simulation for the data described above. Note how easy is to write the model from the mathematical description.</p>
<pre class="python"><code>import pymc3 as pm

model = pm.Model()

with model:
    
    # Define the prior of the parameter lambda.
    lam = pm.Gamma(&#39;lambda&#39;, alpha=a, beta=b)
    
    # Define the likelihood function.
    y_obs = pm.Poisson(&#39;y_obs&#39;, mu=lam, observed=y)
    
    # Consider 2000 draws and 3 chains.
    trace = pm.sample(draws=2000, chains=3)</code></pre>
<pre><code>Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (3 chains in 4 jobs)
NUTS: [lambda]
Sampling 3 chains: 100%|██████████| 7500/7500 [00:01&lt;00:00, 5660.80draws/s]</code></pre>
<p>If we do a trace plot we can see two results:</p>
<ul>
<li><p>We see the simulated posterior distribution for 3 independent Markov Chains (so that, when combined, avoid the dependence on the initial point). The 3 different chains correspond to the color blue, green and orange.</p></li>
<li><p>The sample value of lambda for each iteration.</p></li>
</ul>
<pre class="python"><code>pm.traceplot(trace);</code></pre>
<center>
<img src="../images/intro_PyMC3_files/intro_PyMC3_26_1.png" alt="png" />
</center>
<p>We can also see the mean and quantile information for the posterior distribution.</p>
<pre class="python"><code>pm.plot_posterior(trace);</code></pre>
<center>
<img src="../images/intro_PyMC3_files/intro_PyMC3_28_0.png" alt="png" />
</center>
</div>
<div id="bayesian-hierarchical-modeling-a-chocolate-cookies-example" class="section level1">
<h1>3. Bayesian Hierarchical Modeling: A Chocolate Cookies Example</h1>
<center>
<img src="../images/monster.jpg" alt="html" style="width: 400px;"/>
</center>
<p>Now we are going to treat a more complicated example which illustrated a hierarchical mdel.
This case of study is taken from the (strongly recomended!) online course:</p>
<p><strong>Bayesian Statistics: Techniques and Models:</strong></p>
<p><a href="https://www.coursera.org/learn/mcmc-bayesian-statistics" class="uri">https://www.coursera.org/learn/mcmc-bayesian-statistics</a></p>
<p>There, the MCMC simulations are done with <a href="http://mcmc-jags.sourceforge.net/">JAGS</a> in <a href="https://www.r-project.org/">R</a>. As a matter of fact, this course motivated me to explore an analogous tool for Python.</p>
<div id="the-data" class="section level2">
<h2>3.1 The data</h2>
<p>Assume there is a big factory producing chocolate cookies around the world. The cookies follow a unique recipe, but you want to study the chocolate chips distribution for cookies produced in 5 different locations.</p>
<ul>
<li><p>On the one hand side you would assume that the distribution across the locations is similar, as they all come from a unique recipe. This is why you may not want to model each location separately.</p></li>
<li><p>On the other hand, in reality, as the locations are not exacly the same you might expect some differences between each location. This is why you may not want to model all locations at once.</p></li>
</ul>
<p>To overcome these restrictions, a hierarchical can be a feasible approach.</p>
<pre class="python"><code>import pandas as pd

# We begin reading the data into a pandas dataframe.
cookies = pd.read_csv(&#39;data/cookies.dat&#39;, sep=&#39; &#39;)

cookies.head()</code></pre>
<center>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
chips
</th>
<th>
location
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
12
</td>
<td>
1
</td>
</tr>
<tr>
<th>
1
</th>
<td>
12
</td>
<td>
1
</td>
</tr>
<tr>
<th>
2
</th>
<td>
6
</td>
<td>
1
</td>
</tr>
<tr>
<th>
3
</th>
<td>
13
</td>
<td>
1
</td>
</tr>
<tr>
<th>
4
</th>
<td>
12
</td>
<td>
1
</td>
</tr>
</tbody>
</table>
</div>
</center>
<pre class="python"><code># Verify the number of unique locations.
cookies[&#39;location&#39;].unique()</code></pre>
<p>array([1, 2, 3, 4, 5])</p>
<p>Let us start with some visualization of the data.</p>
<pre class="python"><code># Histogram distribution of chocolate chips
# for all cookies.
fig, ax = plt.subplots(figsize=(8, 6))
sns.distplot(cookies[&#39;chips&#39;], bins=15, ax=ax);
ax.set(title=&#39;Chips Distribution (All Locations)&#39;);</code></pre>
<center>
<img src="../images/intro_PyMC3_files/intro_PyMC3_34_0.png" alt="png" />
</center>
<pre class="python"><code># Histogram distribution of chocolate chips
# for cookies in each location.
g = sns.FacetGrid(data=cookies, col=&#39;location&#39;, col_wrap=2, height=3, aspect=2)
g = g.map(sns.distplot, &#39;chips&#39;)</code></pre>
<center>
<img src="../images/intro_PyMC3_files/intro_PyMC3_35_0.png" alt="png" />
</center>
<pre class="python"><code># Box plot for different locations.
fig, ax = plt.subplots(figsize=(10,6))
cookies.boxplot(column=&#39;chips&#39;, by=&#39;location&#39;, ax=ax);</code></pre>
<center>
<p><img src="../images/intro_PyMC3_files/intro_PyMC3_36_0.png" alt="png" />
<c/enter></p>
</div>
<div id="the-model-hierarchical-approach" class="section level2">
<h2>3.2 The model: Hierarchical Approach</h2>
<ul>
<li>Hierarchical Model:</li>
</ul>
<p>We model the chocolate chip counts by a Poisson distribution with parameter <span class="math inline">\(\lambda\)</span>. Motivated by the example above, we choose a gamma prior.</p>
<p><span class="math display">\[\begin{align}
\text{chips} \sim  \text{Poiss}(\lambda)
\quad\quad\quad
\lambda \sim  \Gamma(a,b)
\end{align}\]</span></p>
<ul>
<li>Parametrization:</li>
</ul>
<p>We parametrize the shape and scale of the gamma prior with the mean <span class="math inline">\(\mu\)</span> and variance <span class="math inline">\(\sigma^2\)</span>.</p>
<p><span class="math display">\[\begin{align}
a=\frac{\mu^2}{\sigma^2}
\quad\quad\quad
b=\frac{\mu}{\sigma^2}
\end{align}\]</span></p>
<ul>
<li>Prior Distributions:</li>
</ul>
<p>We further impose prior for these parameters</p>
<p><span class="math display">\[\begin{align}
\mu  \sim  \Gamma(2,1/5)
\quad\quad\quad
\sigma  \sim  \text{Exp}(1)
\end{align}\]</span></p>
<pre class="python"><code>x = np.linspace(start=0, stop=50, num=100)

fig = plt.figure(figsize=(15, 5))
plt.subplot(1, 2, 1)
plt.plot(x, ss.gamma.pdf(x,a=2,scale=5), &#39;r-&#39;)
plt.title(&#39;Prior Distribution for mu \n Gamma Density Function for a={} and b={}&#39;.format(2, 1/5))

plt.subplot(1, 2, 2)
x = np.linspace(0,10)
plt.plot(x, ss.expon.pdf(x,1), &#39;r-&#39;)
plt.title(&#39;Prior Distribution for sigma2 \n Exponential Density Function&#39;)
plt.xlim(1,10);</code></pre>
<center>
<img src="../images/intro_PyMC3_files/intro_PyMC3_38_1.png" alt="png" />
</center>
<p>Let us write the model in PyMC3. Note how the syntax mimics the mathematical formulation.</p>
<pre class="python"><code>model = pm.Model()

with model:
    
    # Prior distribution for mu.
    mu = pm.Gamma(&#39;mu&#39;, alpha=2.0, beta=1.0/5)
    
    # Prior distribution for sigma2.
    sigma = pm.Exponential(&#39;sigma&#39;, 1.0)
    
    # Parametrization for the shape parameter.
    alpha =  mu**2/sigma**2
    
    # Parametrization for the scale parameter.
    beta = mu/sigma**2
    
    # Prior distribution for lambda.
    lam = pm.Gamma(
        &#39;lam&#39;, 
        alpha=alpha, 
        beta=beta, 
        shape=cookies.location.values.max()
    )
    
    # Likelihood function for the data.
    chips = [
        pm.Poisson(&#39;chips_{}&#39;.format(i),lam[i], 
        observed=cookies[cookies.location==i+1].chips.values) 
        for i in range(cookies.location.values.max())
    ] 
    
    # Parameters of the simulation:
    # Number of iterations and independent chains.
    n_draws, n_chains = 1000, 3
    
    n_sim = n_draws*n_chains
    
    trace = pm.sample(draws=n_draws, chains=n_chains)  </code></pre>
<pre><code>Auto-assigning NUTS sampler...
Initializing NUTS using jitter+adapt_diag...
Multiprocess sampling (3 chains in 4 jobs)
NUTS: [lam, sigma, mu]
Sampling 3 chains: 100%|██████████| 4500/4500 [00:02&lt;00:00, 1815.91draws/s]</code></pre>
</div>
<div id="diagnostics" class="section level2">
<h2>3.3 Diagnostics</h2>
<p>Many <a href="https://pymc-devs.github.io/pymc3/api/diagnostics.html">diagnostic</a> options are described in the PyMC3 documentation.</p>
<pre class="python"><code>pm.traceplot(trace);</code></pre>
<center>
<img src="../images/intro_PyMC3_files/intro_PyMC3_42_1.png" alt="png" />
</center>
<p>From the traceplot we see that the chains have converged. We can also have a detailed summary of the posterior distribution for each parameter:</p>
<pre class="python"><code>pm.summary(trace)</code></pre>
<center>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
mean
</th>
<th>
sd
</th>
<th>
mc_error
</th>
<th>
hpd_2.5
</th>
<th>
hpd_97.5
</th>
<th>
n_eff
</th>
<th>
Rhat
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
mu
</th>
<td>
9.104871
</td>
<td>
0.952266
</td>
<td>
0.019789
</td>
<td>
7.285942
</td>
<td>
11.142347
</td>
<td>
2522.978566
</td>
<td>
1.001637
</td>
</tr>
<tr>
<th>
sigma
</th>
<td>
2.075416
</td>
<td>
0.716848
</td>
<td>
0.013315
</td>
<td>
0.988354
</td>
<td>
3.541359
</td>
<td>
2510.182129
</td>
<td>
0.999958
</td>
</tr>
<tr>
<th>
lam__0
</th>
<td>
9.277816
</td>
<td>
0.529706
</td>
<td>
0.008511
</td>
<td>
8.353204
</td>
<td>
10.406595
</td>
<td>
4050.792217
</td>
<td>
0.999807
</td>
</tr>
<tr>
<th>
lam__1
</th>
<td>
6.221088
</td>
<td>
0.461659
</td>
<td>
0.007158
</td>
<td>
5.306228
</td>
<td>
7.100886
</td>
<td>
3812.117511
</td>
<td>
0.999808
</td>
</tr>
<tr>
<th>
lam__2
</th>
<td>
9.520956
</td>
<td>
0.544345
</td>
<td>
0.008374
</td>
<td>
8.516351
</td>
<td>
10.607144
</td>
<td>
3723.180815
</td>
<td>
0.999935
</td>
</tr>
<tr>
<th>
lam__3
</th>
<td>
8.932577
</td>
<td>
0.531530
</td>
<td>
0.009103
</td>
<td>
7.861659
</td>
<td>
9.922982
</td>
<td>
3353.991802
</td>
<td>
0.999881
</td>
</tr>
<tr>
<th>
lam__4
</th>
<td>
11.753710
</td>
<td>
0.639027
</td>
<td>
0.010974
</td>
<td>
10.547972
</td>
<td>
13.023027
</td>
<td>
3529.810115
</td>
<td>
0.999560
</td>
</tr>
</tbody>
</table>
</div>
</center>
<p>We can also see this visually.</p>
<pre class="python"><code>pm.plot_posterior(trace);</code></pre>
<center>
<img src="../images/intro_PyMC3_files/intro_PyMC3_46_0.png" alt="html" style="width: 1000px;"/>
</center>
<p>We can verify the convergence of the chains formally using the <a href="http://ugrad.stat.ubc.ca/R/library/coda/html/gelman.diag.html">Gelman Rubin test</a>. Values close to 1.0 mean convergence.</p>
<pre class="python"><code>pm.gelman_rubin(trace)</code></pre>
<p>{‘mu’: 1.0016372262426347,
‘sigma’: 0.9999579325854048,
‘lam’: array([0.9998075 , 0.99980835, 0.99993469, 0.99988124, 0.99956018])}</p>
<p>We can also test for correlation between samples in the chains. We are aiming for zero auto-correlation to get “random” samples from the posterior distribution.</p>
<pre class="python"><code># Auto-correlation of the parameter sigma for the 3 chains.
pm.autocorrplot(trace, var_names=[&#39;sigma&#39;], max_lag=20);</code></pre>
<center>
<img src="../images/intro_PyMC3_files/intro_PyMC3_50_0.png" alt="html" style="width: 1000px;"/>
</center>
<pre class="python"><code># We can also consider all the variables simultaneously. 
pm.autocorrplot(trace, max_lag=20);</code></pre>
<center>
<img src="../images/intro_PyMC3_files/intro_PyMC3_51_0.png" alt="html" style="width: 1000px;"/>
</center>
<p>From these plots we see that the auto-correlation is not problematic. Indeed, we can test this through the <em>effective sample size</em>, which sould be close to the total sumber of samples <code>n_sim</code>.</p>
<pre class="python"><code>pm.diagnostics.effective_n(trace)</code></pre>
<p>{‘mu’: 2522.9785658157243,
‘sigma’: 2510.182129406361,
‘lam’: array([4050.79221675, 3812.11751115, 3723.18081493, 3353.99180192,
3529.81011549])}</p>
<p>Finally, we can compute the <a href="https://en.wikipedia.org/wiki/Watanabe–Akaike_information_criterion">Watanabe–Akaike information criterion</a>.</p>
<pre class="python"><code>pm.waic(trace, model)</code></pre>
<p>WAIC_r(WAIC=789.764634126058, WAIC_se=21.395864846123082, p_WAIC=5.844326163346711, var_warn=1)</p>
</div>
<div id="residual-analysis" class="section level2">
<h2>3.4 Residual analysis</h2>
<p>In order to evaluate the model results we analyze the behaviour of the residuals.</p>
<pre class="python"><code># Compute the mean of the simulation.
lambda_mean = np.apply_along_axis(np.mean, axis=0, trace[&#39;lam&#39;])

# Compute for each sample the posterior mean.
cookies[&#39;yhat&#39;] = cookies.location.apply(lambda x: lambda_mean[x-1])

# Compute the residuals.
cookies[&#39;resid&#39;] = cookies.apply(lambda x: x.chips - x.yhat, axis=1)

cookies.head()</code></pre>
<center>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
chips
</th>
<th>
location
</th>
<th>
yhat
</th>
<th>
resid
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
12
</td>
<td>
1
</td>
<td>
9.277816
</td>
<td>
2.722184
</td>
</tr>
<tr>
<th>
1
</th>
<td>
12
</td>
<td>
1
</td>
<td>
9.277816
</td>
<td>
2.722184
</td>
</tr>
<tr>
<th>
2
</th>
<td>
6
</td>
<td>
1
</td>
<td>
9.277816
</td>
<td>
-3.277816
</td>
</tr>
<tr>
<th>
3
</th>
<td>
13
</td>
<td>
1
</td>
<td>
9.277816
</td>
<td>
3.722184
</td>
</tr>
<tr>
<th>
4
</th>
<td>
12
</td>
<td>
1
</td>
<td>
9.277816
</td>
<td>
2.722184
</td>
</tr>
</tbody>
</table>
</div>
</center>
<pre class="python"><code># Cookies Residuals
fig, ax = plt.subplots(figsize=(8, 6))
cookies.reset_index().plot.scatter(x=&#39;index&#39;, y=&#39;resid&#39;, ax=ax)
ax.axhline(y=0.0, color=&#39;r&#39;, linestyle=&#39;--&#39;)
ax.set(title=&#39;Cookies Residuals&#39;, xlabel=&#39;Observation&#39;);</code></pre>
<center>
<img src="../images/intro_PyMC3_files/intro_PyMC3_58_1.png" alt="png" />
</center>
<p>We do not see a particular partern in the scatter plot of the residuals against the observation.</p>
<pre class="python"><code>fig, ax = plt.subplots(figsize=(8, 6))
cookies.plot.scatter(x=&#39;yhat&#39;, y=&#39;resid&#39;, ax=ax)
ax.axhline(y=0.0, color=&#39;red&#39;, linestyle=&#39;--&#39;)
ax.set(title=&#39;Cookies Residuals&#39;);</code></pre>
<center>
<img src="../images/intro_PyMC3_files/intro_PyMC3_60_1.png" alt="png" />
</center>
</div>
<div id="predictions" class="section level2">
<h2>3.5 Predictions</h2>
<p>Finally, we are going to illustrate how to use the simulation results to derive predictions.</p>
<div id="for-a-known-location" class="section level3">
<h3>3.5.1 For a known location</h3>
<p>Let us consider Location 1. We want, for example, to compute the posterior probability the next cookie in this location has less than 7 chips.</p>
<pre class="python"><code># We generate n_sim samples of a Poisson distribution for 
# each value for lam_0 (location 1) simulation..
y_pred_location_1 = np.random.poisson(lam=trace[&#39;lam&#39;][:,0] , size=n_sim)

fig, ax = plt.subplots(figsize=(8, 6))
sns.distplot(y_pred_location_1, bins=30, ax=ax)
ax.set(title=&#39;Chocolate Chips Distribution for Location 1&#39;);</code></pre>
<center>
<img src="../images/intro_PyMC3_files/intro_PyMC3_62_0.png" alt="png" />
</center>
<pre class="python"><code># Probability the next cookie in location has less than 7 chips.
(y_pred_location_1 &lt; 7).astype(int).mean()</code></pre>
<p>0.17866666666666667</p>
</div>
<div id="for-a-new-location" class="section level3">
<h3>3.5.1 For a new location</h3>
<p>Now assume we want to open a new location. First, we want to compute the posterior probability that this new location has <span class="math inline">\(\lambda &gt; 15\)</span>.</p>
<pre class="python"><code># Posterior distribution of for a an b 
# from the simulated values of mu and sigma2.
post_a = trace[&#39;mu&#39;]**2/trace[&#39;sigma&#39;]**2
post_b = trace[&#39;mu&#39;]/trace[&#39;sigma&#39;]**2

# We now generate samples of a gamma distribution 
# with these generated parameters of a and b.
lambda_pred_dist = np.random.gamma(post_a, 1/post_b, n_sim) 

fig, ax = plt.subplots(figsize=(8, 6))
sns.distplot(lambda_pred_dist, bins=30, ax=ax)
ax.set(title=&#39;Lambda Predicted Distribution&#39;);</code></pre>
<center>
<img src="../images/intro_PyMC3_files/intro_PyMC3_65_0.png" alt="png" />
</center>
<pre class="python"><code># Posterior probability a new location has lambda &gt; 15.
(lambda_pred_dist &gt; 15).astype(int).mean()</code></pre>
<p>0.019333333333333334</p>
<p>Now we answer a question at the next level of the hierarchical model. We want to calculate the posterior probability for a cookie produced in a new location to have more than 15 chocolate chips.</p>
<pre class="python"><code># Posterior distribution of the chips.
# Here we use the values of lambda obtained above.
cookies_pred_dist = np.random.poisson(lam=lambda_pred_dist, size=n_sim)

fig, ax = plt.subplots(figsize=(8, 6))
sns.distplot(cookies_pred_dist, bins=30, ax=ax)
ax.set(title=&#39;Chocolate Chips Distribution New Location&#39;);</code></pre>
<center>
<img src="../images/intro_PyMC3_files/intro_PyMC3_68_0.png" alt="png" />
</center>
<pre class="python"><code># Posterior probability that a cookie produced 
# in a new location has more than 15 chocolate chips.
(cookies_pred_dist&gt;15).astype(int).mean()</code></pre>
<p>0.054</p>
</div>
</div>
</div>
<div id="references-and-further-reading" class="section level1">
<h1>4 References and Further Reading</h1>
<p>Here we provide some suggested references used in this post and also to go deeper in the subject.</p>
<div id="bayesian-probability" class="section level2">
<h2>4.1 Bayesian Probability</h2>
<ul>
<li><p><a href="https://www.coursera.org/learn/bayesian-statistics">Coursera: Bayesian Statistics: From Concept to Data Analysis</a></p></li>
<li><p><a href="https://www.coursera.org/learn/mcmc-bayesian-statistics">Coursera: Bayesian Statistics: Techniques and Models</a></p></li>
<li><p><a href="http://www.springer.com/us/book/9780387922997">A First Course in Bayesian Statistical Methods, Peter D. Hoff</a></p></li>
<li><p><a href="http://www.springer.com/la/book/9780387400846">An Introduction to Bayesian Analysis: Theory and Methods, Ghosh, Jayanta K., Delampady, Mohan, Samanta, Tapas</a></p></li>
</ul>
</div>
<div id="pymc3" class="section level2">
<h2>4.2 PyMC3</h2>
<ul>
<li><p><a href="https://pymc-devs.github.io/pymc3/index.html">Documentation</a></p></li>
<li><p><a href="https://arxiv.org/abs/1507.08050">Probabilistic Programming in Python using PyMC, John Salvatier, Thomas Wiecki, Christopher Fonnesbeck</a></p></li>
</ul>
</div>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="../index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="../images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/yaml.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/python.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/dockerfile.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/9.12.0/languages/bash.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="../js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.5/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'UA-122570825-1', 'auto');
	
	ga('send', 'pageview');
}
</script>

  </body>
</html>

