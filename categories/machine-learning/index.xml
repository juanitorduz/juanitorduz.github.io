<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Machine Learning on Dr. Juan Camilo Orduz</title>
    <link>/categories/machine-learning/</link>
    <description>Recent content in Machine Learning on Dr. Juan Camilo Orduz</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 01 Jul 2021 00:00:00 +0000</lastBuildDate><atom:link href="/categories/machine-learning/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Exploring Tools for Interpretable Machine Learning</title>
      <link>/interpretable_ml/</link>
      <pubDate>Thu, 01 Jul 2021 00:00:00 +0000</pubDate>
      
      <guid>/interpretable_ml/</guid>
      <description>In this notebook we want to test various ways of getting a better understanding on how non-trivial machine learning models generate predictions and how features interact with each other. This is in general not straight forward and key components are (1) understanding on the input data and (2) domain knowledge on the problem. Two great references on the subject are:
 Interpretable Machine Learning, A Guide for Making Black Box Models Explainable by Christoph Molnar Interpretable Machine Learning with Python by Serg Mas√≠s  Note that the methods discussed in this notebook are not related with causality.</description>
    </item>
    
    <item>
      <title>Feature Engineering: patsy as FormulaTransformer</title>
      <link>/formula_transformer/</link>
      <pubDate>Sat, 01 May 2021 00:00:00 +0000</pubDate>
      
      <guid>/formula_transformer/</guid>
      <description>In this notebook I want to describe how to create features inside scikit-learn pipelines using patsy-like formulas. I have used this approach to generate features in a previous post: GLM in PyMC3: Out-Of-Sample Predictions, so I will consider the same data set here for the sake of comparison.
Prepare Notebook import matplotlib.pyplot as plt import numpy as np import pandas as pd import patsy import seaborn as sns from sklearn.</description>
    </item>
    
    <item>
      <title>Seasonal Bump Functions</title>
      <link>/bump_func/</link>
      <pubDate>Thu, 11 Apr 2019 00:00:00 +0000</pubDate>
      
      <guid>/bump_func/</guid>
      <description>Motivated by the nice talk on Winning with Simple, even Linear, Models by Vincent D. Warmerdam, I briefly describe how to construct certain class of bump functions to encode seasonal variables in R.
Prepare Notebook library(glue) library(lubridate) library(magrittr) library(tidyverse)  Generate Data Let us generate a time sequence variable stored in a tibble.
# Define time sequence. t &amp;lt;- seq.Date(from = as.Date(&amp;quot;2017-07-01&amp;quot;), to = as.Date(&amp;quot;2019-04-01&amp;quot;), by = &amp;quot;day&amp;quot;) # Store it in a tibble.</description>
    </item>
    
    <item>
      <title>Probability that a given observation is part of a bootstrap sample?</title>
      <link>/bootstrap/</link>
      <pubDate>Wed, 29 Nov 2017 00:00:00 +0000</pubDate>
      
      <guid>/bootstrap/</guid>
      <description>We study the problem of computing the probability that a given observation is part of a bootstrap sample. We include some numerical simulations.</description>
    </item>
    
  </channel>
</rss>
