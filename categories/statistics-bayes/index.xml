<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Statistics, Bayes on Dr. Juan Camilo Orduz</title>
    <link>/categories/statistics-bayes/</link>
    <description>Recent content in Statistics, Bayes on Dr. Juan Camilo Orduz</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Mon, 04 Jan 2021 00:00:00 +0000</lastBuildDate>
    <atom:link href="/categories/statistics-bayes/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>GLM in PyMC3: Out-Of-Sample Predictions</title>
      <link>/glm_pymc3/</link>
      <pubDate>Mon, 04 Jan 2021 00:00:00 +0000</pubDate>
      <guid>/glm_pymc3/</guid>
      <description>&lt;p&gt;In this notebook I explore the &lt;a href=&#34;https://docs.pymc.io/api/glm.html&#34;&gt;glm&lt;/a&gt; module of &lt;a href=&#34;https://docs.pymc.io/&#34;&gt;PyMC3&lt;/a&gt;. I am particularly interested in the model definition using &lt;a href=&#34;https://patsy.readthedocs.io/en/latest/&#34;&gt;patsy&lt;/a&gt; formulas, as it makes the model evaluation loop faster (easier to include features and/or interactions). There are many good resources on this subject, but most of them evaluate the model in-sample. For many applications we require doing predictions on out-of-sample data. This experiment was motivated by the discussion of the thread &lt;a href=&#34;https://discourse.pymc.io/t/out-of-sample-predictions-with-the-glm-sub-module/773&#34;&gt;“Out of sample” predictions with the GLM sub-module&lt;/a&gt; on the (great!) forum &lt;a href=&#34;https://discourse.pymc.io/&#34;&gt;discourse.pymc.io/&lt;/a&gt;, thank you all for your input!&lt;/p&gt;</description>
    </item>
  </channel>
</rss>
