<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bayesian on Dr. Juan Camilo Orduz</title>
    <link>/categories/bayesian/</link>
    <description>Recent content in Bayesian on Dr. Juan Camilo Orduz</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Thu, 03 Mar 2022 00:00:00 +0000</lastBuildDate><atom:link href="/categories/bayesian/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>BG/NBD Model in PyMC</title>
      <link>/bg_nbd_pymc/</link>
      <pubDate>Thu, 03 Mar 2022 00:00:00 +0000</pubDate>
      
      <guid>/bg_nbd_pymc/</guid>
      <description>In this notebook we show how to port the BG/NBD model from the the lifetimes (developed mainly by Cameron Davidson-Pilon) package to pymc. The BG/NBD model, introduced in the seminal paper “Counting Your Customers” the Easy Way: An Alternative to the Pareto/NBD Model by Peter S. Fader, Bruce G. S. Hardie and Ka Lok Lee in 2005, is used to
 predict future purchasing patterns, which can then serve as an input into “lifetime value” calculations, in the “non-contractual” setting (i.</description>
    </item>
    
    <item>
      <title>Media Effect Estimation with PyMC: Adstock, Saturation &amp; Diminishing Returns</title>
      <link>/pymc_mmm/</link>
      <pubDate>Fri, 11 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>/pymc_mmm/</guid>
      <description>In this notebook we present a concrete example of estimating the media effects via bayesian methods, following the strategy outlined in Google’s paper Jin, Yuxue, et al. “Bayesian methods for media mix modeling with carryover and shape effects.” (2017). This example can be considered the continuation of the post Media Effect Estimation with Orbit’s KTR Model. However, it is not strictly necessary to read before as we make this notebook self-contained.</description>
    </item>
    
    <item>
      <title>Media Effect Estimation with Orbit&#39;s KTR Model</title>
      <link>/orbit_mmm/</link>
      <pubDate>Fri, 04 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>/orbit_mmm/</guid>
      <description>In this notebook we want to experiment to the new KTR model included in the new orbit’s release (1.1). In particular, we are interested in its applications to media effects estimation in the context of media mix modeling. This is one of the applications for the KTR model by the Uber’s team, see the corresponding paper Edwin, Ng, et al. “Bayesian Time Varying Coefficient Model with Applications to Marketing Mix Modeling”.</description>
    </item>
    
    <item>
      <title>Unobserved Components Model as a Bayesian Model with PyMC</title>
      <link>/uc_pymc/</link>
      <pubDate>Fri, 10 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>/uc_pymc/</guid>
      <description>In this notebook I want to deep-dive into the idea of wrapping a statsmodels UnobservedComponents model as a bayesian model with PyMC described in the (great!) post Fast Bayesian estimation of SARIMAX models. This is a nice excuse to get into some internals of how PyMC works. I hope this can serve as a complement to the original post mentioned above. This post has two parts: In the first one we fit a UnobservedComponents model to a simulated time series.</description>
    </item>
    
    <item>
      <title>Simple Bayesian Linear Regression with TensorFlow Probability</title>
      <link>/tfp_lm/</link>
      <pubDate>Tue, 06 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/tfp_lm/</guid>
      <description>In this post we show how to fit a simple linear regression model using TensorFlow Probability by replicating the first example on the getting started guide for PyMC3. We are going to use Auto-Batched Joint Distributions as they simplify the model specification considerably. Moreover, there is a great resource to get deeper into this type of distribution: Auto-Batched Joint Distributions: A Gentle Tutorial, which I strongly recommend (see this post to get a brief introduction on TensorFlow probability distributions).</description>
    </item>
    
    <item>
      <title>A Simple Hamiltonian Monte Carlo Example with TensorFlow Probability</title>
      <link>/tfp_hcm/</link>
      <pubDate>Fri, 24 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/tfp_hcm/</guid>
      <description>In this post we want to revisit a simple bayesian inference example worked out in this blog post. This time we want to use TensorFlow Probability (TFP) instead of PyMC3.
References:
 Statistical Rethinking is an amazing reference for Bayesian analysis. It also has a sequence of online lectures freely available on YouTube.
 An introduction to probabilistic programming, now available in TensorFlow Probability
 There are many examples on the TensorFlow’s GitHub repository.</description>
    </item>
    
    <item>
      <title>Introduction to Bayesian Modeling with PyMC3</title>
      <link>/intro_pymc3/</link>
      <pubDate>Sun, 13 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/intro_pymc3/</guid>
      <description>We give an introduction to PyMC3, a probabilistic programming framework written in Python. We revise the basic mahematical theory and present two concrete examples.</description>
    </item>
    
  </channel>
</rss>
