<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Bayesian on Dr. Juan Camilo Orduz</title>
    <link>/categories/bayesian/</link>
    <description>Recent content in Bayesian on Dr. Juan Camilo Orduz</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 04 Feb 2022 00:00:00 +0000</lastBuildDate><atom:link href="/categories/bayesian/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Media Effect Estimation with Orbit&#39;s KTR Model</title>
      <link>/orbit_mmm/</link>
      <pubDate>Fri, 04 Feb 2022 00:00:00 +0000</pubDate>
      
      <guid>/orbit_mmm/</guid>
      <description>In this notebook we want to experiment to the new KTR model included in the new orbit’s release (1.1). In particular, we are interested in its applications to media effects estimation in the context of media mix modeling. This is one of the applications for the KTR model by the Uber’s team, see the corresponding paper Edwin, Ng, et al. “Bayesian Time Varying Coefficient Model with Applications to Marketing Mix Modeling”.</description>
    </item>
    
    <item>
      <title>Unobserved Components Model as a Bayesian Model with PyMC</title>
      <link>/uc_pymc/</link>
      <pubDate>Fri, 10 Dec 2021 00:00:00 +0000</pubDate>
      
      <guid>/uc_pymc/</guid>
      <description>In this notebook I want to deep-dive into the idea of wrapping a statsmodels UnobservedComponents model as a bayesian model with PyMC described in the (great!) post Fast Bayesian estimation of SARIMAX models. This is a nice excuse to get into some internals of how PyMC works. I hope this can serve as a complement to the original post mentioned above. This post has two parts: In the first one we fit a UnobservedComponents model to a simulated time series.</description>
    </item>
    
    <item>
      <title>Simple Bayesian Linear Regression with TensorFlow Probability</title>
      <link>/tfp_lm/</link>
      <pubDate>Tue, 06 Oct 2020 00:00:00 +0000</pubDate>
      
      <guid>/tfp_lm/</guid>
      <description>In this post we show how to fit a simple linear regression model using TensorFlow Probability by replicating the first example on the getting started guide for PyMC3. We are going to use Auto-Batched Joint Distributions as they simplify the model specification considerably. Moreover, there is a great resource to get deeper into this type of distribution: Auto-Batched Joint Distributions: A Gentle Tutorial, which I strongly recommend (see this post to get a brief introduction on TensorFlow probability distributions).</description>
    </item>
    
    <item>
      <title>A Simple Hamiltonian Monte Carlo Example with TensorFlow Probability</title>
      <link>/tfp_hcm/</link>
      <pubDate>Fri, 24 Jul 2020 00:00:00 +0000</pubDate>
      
      <guid>/tfp_hcm/</guid>
      <description>In this post we want to revisit a simple bayesian inference example worked out in this blog post. This time we want to use TensorFlow Probability (TFP) instead of PyMC3.
References:
 Statistical Rethinking is an amazing reference for Bayesian analysis. It also has a sequence of online lectures freely available on YouTube.
 An introduction to probabilistic programming, now available in TensorFlow Probability
 There are many examples on the TensorFlow’s GitHub repository.</description>
    </item>
    
    <item>
      <title>Introduction to Bayesian Modeling with PyMC3</title>
      <link>/intro_pymc3/</link>
      <pubDate>Sun, 13 Aug 2017 00:00:00 +0000</pubDate>
      
      <guid>/intro_pymc3/</guid>
      <description>We give an introduction to PyMC3, a probabilistic programming framework written in Python. We revise the basic mahematical theory and present two concrete examples.</description>
    </item>
    
  </channel>
</rss>
