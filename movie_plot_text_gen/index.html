<!DOCTYPE html>
<html lang="en-us">
  <head>
    <script defer src="https://use.fontawesome.com/releases/v6.5.1/js/all.js"></script>

<script async src="https://www.googletagmanager.com/gtag/js?id=G-5NM5EDH834"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-5NM5EDH834');
</script>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1.0">
<meta name="generator" content="Hugo 0.79.1" />


<title>Movie Plots Text Generation with Keras - Dr. Juan Camilo Orduz</title>
<meta property="og:title" content="Movie Plots Text Generation with Keras - Dr. Juan Camilo Orduz">


  <link href='../images/favicon.ico' rel='icon' type='image/x-icon'/>



  








<link href='//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/styles/github-dark.min.css' rel='stylesheet' type='text/css' />



<link rel="stylesheet" href="../css/fonts.css" media="all">
<link rel="stylesheet" href="../css/main.css" media="all">



  </head>
  <body>
    <div class="wrapper">
      <header class="header">
        <nav class="nav">
  <a href="../" class="nav-logo">
    <img src="../images/tattoo_logo.jpg"
         width="50"
         height="50"
         alt="Logo">
  </a>

  <ul class="nav-links">
    
    <li><a href="../about/"> About</a></li>
    
    <li><a href="https://github.com/juanitorduz"><i class='fab fa-github fa-2x'></i>  </a></li>
    
    <li><a href="https://www.linkedin.com/in/juanitorduz/"><i class='fab fa-linkedin fa-2x' style='color:#0a66c2;'></i>  </a></li>
    
    <li><a href="https://twitter.com/juanitorduz"><i class='fab fa-twitter fa-2x' style='color:#1DA1F2;'></i>  </a></li>
    
    <li><a href="https://bayes.club/@juanitorduz"><i class='fab fa-mastodon fa-2x' style='color:#6364FF;'></i>  </a></li>
    
    <li><a href="https://ko-fi.com/juanitorduz"><i class='fa-solid fa-mug-hot fa-2x' style='color:#FF5E5B;'></i>  </a></li>
    
  </ul>
</nav>

      </header>


<main class="content" role="main">

  <article class="article">
    
    <span class="article-duration">15 min read</span>
    

    <h1 class="article-title">Movie Plots Text Generation with Keras</h1>

    
    <span class="article-date">2019-01-13</span>
    

    <div class="article-content">
      
<script src="../rmarkdown-libs/header-attrs/header-attrs.js"></script>


<p>In this post I show some text generation experiments I ran using <a href="https://en.wikipedia.org/wiki/Long_short-term_memory">LSTM</a> with <a href="https://keras.io">Keras</a>. For the preprocessing and tokenization I used <a href="https://spacy.io">SpaCy</a>. The aim is not to present a completed project, but rather a first step which should be then iterated.</p>
<div id="resources" class="section level2">
<h2>Resources</h2>
<p>There are many great resources and blog posts about the subject (and similar experiments). Here I mention the ones I found particularly useful for the general theory:</p>
<ul>
<li>Online Resources:
<ul>
<li><p><a href="https://www.deeplearning.ai">Deep Learning Specialization</a>, <a href="https://www.coursera.org">Coursera</a> by <a href="https://www.andrewng.org">Andrew Ng</a>.</p></li>
<li><p><a href="https://www.udemy.com/nlp-natural-language-processing-with-python/">NLP with Python</a>, <a href="https://www.udemy.com/">Udemy</a> by <a href="https://www.linkedin.com/in/jmportilla/">Jose Portilla</a>.</p></li>
</ul></li>
</ul>
<p><strong>Remark:</strong> From this last course I took most of the code in this experiment (check out the complete series of videos!).</p>
<ul>
<li><p>Books:</p>
<ul>
<li><a href="https://www.manning.com/books/deep-learning-with-r">Deep Learning with R</a>, by <a href="https://www.linkedin.com/in/fchollet/">François Chollet</a> and <a href="https://www.linkedin.com/in/jjallaire/">J. J. Allaire</a>.</li>
</ul></li>
</ul>
</div>
<div id="the-data-set" class="section level2">
<h2>The Data Set</h2>
<p><a href="https://www.kaggle.com/jrobischon/wikipedia-movie-plots">Wikipedia Movie Plots</a></p>
<ul>
<li><p>Source: <a href="https://www.kaggle.com/">Kaggle</a>.</p></li>
<li><p>Description: <em>Plot summary descriptions scraped from Wikipedia</em>.</p></li>
</ul>
<p>Our aim is to train a text generator algorithm able to write plots for horror movies (why horror? no particular reason).</p>
</div>
<div id="read-data" class="section level2">
<h2>Read Data</h2>
<pre class="python"><code>import numpy as np
import pandas as pd</code></pre>
<pre class="python"><code># Read data. 
movies_raw_df = pd.read_csv(&#39;wiki_movie_plots_deduped.csv&#39;)

movies_raw_df.head()</code></pre>
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

     .dataframe thead th {
        text-align: left;
        font-size: 14px;
    }

    .dataframe tbody tr th {
        vertical-align: top;
        font-size: 14px;
    }
    
    .dataframe tbody tr td {
        vertical-align: top;
        font-size: 14px;
    }
</style>
<table border="1" class="dataframe">
<thead>
<tr style="text-align: right;">
<th>
</th>
<th>
Release Year
</th>
<th>
Title
</th>
<th>
Origin/Ethnicity
</th>
<th>
Director
</th>
<th>
Cast
</th>
<th>
Genre
</th>
<th>
Wiki Page
</th>
<th>
Plot
</th>
</tr>
</thead>
<tbody>
<tr>
<th>
0
</th>
<td>
1901
</td>
<td>
Kansas Saloon Smashers
</td>
<td>
American
</td>
<td>
Unknown
</td>
<td>
NaN
</td>
<td>
unknown
</td>
<td>
<a href="https://en.wikipedia.org/wiki/Kansas_Saloon_Sm" class="uri">https://en.wikipedia.org/wiki/Kansas_Saloon_Sm</a>…
</td>
<td>
A bartender is working at a saloon, serving dr…
</td>
</tr>
<tr>
<th>
1
</th>
<td>
1901
</td>
<td>
Love by the Light of the Moon
</td>
<td>
American
</td>
<td>
Unknown
</td>
<td>
NaN
</td>
<td>
unknown
</td>
<td>
<a href="https://en.wikipedia.org/wiki/Love_by_the_Ligh" class="uri">https://en.wikipedia.org/wiki/Love_by_the_Ligh</a>…
</td>
<td>
The moon, painted with a smiling face hangs ov…
</td>
</tr>
<tr>
<th>
2
</th>
<td>
1901
</td>
<td>
The Martyred Presidents
</td>
<td>
American
</td>
<td>
Unknown
</td>
<td>
NaN
</td>
<td>
unknown
</td>
<td>
<a href="https://en.wikipedia.org/wiki/The_Martyred_Pre" class="uri">https://en.wikipedia.org/wiki/The_Martyred_Pre</a>…
</td>
<td>
The film, just over a minute long, is composed…
</td>
</tr>
<tr>
<th>
3
</th>
<td>
1901
</td>
<td>
Terrible Teddy, the Grizzly King
</td>
<td>
American
</td>
<td>
Unknown
</td>
<td>
NaN
</td>
<td>
unknown
</td>
<td>
<a href="https://en.wikipedia.org/wiki/Terrible_Teddy,_" class="uri">https://en.wikipedia.org/wiki/Terrible_Teddy,_</a>…
</td>
<td>
Lasting just 61 seconds and consisting of two …
</td>
</tr>
<tr>
<th>
4
</th>
<td>
1902
</td>
<td>
Jack and the Beanstalk
</td>
<td>
American
</td>
<td>
George S. Fleming, Edwin S. Porter
</td>
<td>
NaN
</td>
<td>
unknown
</td>
<td>
<a href="https://en.wikipedia.org/wiki/Jack_and_the_Bea" class="uri">https://en.wikipedia.org/wiki/Jack_and_the_Bea</a>…
</td>
<td>
The earliest known adaptation of the classic f…
</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="filter-data" class="section level2">
<h2>Filter Data</h2>
<pre class="python"><code>movies_to_select = ((movies_raw_df[&#39;Genre&#39;] == &#39;horror&#39;) &amp;
                    # Restrict to American movies. 
                    (movies_raw_df[&#39;Origin/Ethnicity&#39;] == &#39;American&#39;) &amp;
                    # Only movies from 2000.
                    (movies_raw_df[&#39;Release Year&#39;] &gt; 1999))</code></pre>
<p>The last two conditions are just to make the data set smaller (as this is just an experiment).</p>
<pre class="python"><code>horror_df = movies_raw_df[movies_to_select][&#39;Plot&#39;]

horror_df.head()</code></pre>
<pre><code>13617    In November 1999, tourists and fans of The Bla...
13640    Matthew Van Helsing, the alleged descendant of...
13681    A small group of fervent Roman Catholics belie...
13731    Cotton Weary, now living in Los Angeles and th...
13763    Amy Mayfield, a student at a prestigious film ...
Name: Plot, dtype: object</code></pre>
<pre class="python"><code>horror_df.shape</code></pre>
<pre><code>(260,)</code></pre>
</div>
<div id="preprocess-data" class="section level2">
<h2>Preprocess Data</h2>
<p>We are going to use <a href="https://spacy.io">SpaCy</a> for the tokenization.</p>
<pre class="python"><code># Join all plots into a string.
horror_str = horror_df.str.cat(sep=&#39; &#39;)</code></pre>
<pre class="python"><code>import spacy

# Load language model. 
nlp = spacy.load(&#39;en&#39;, disable = [&#39;parser&#39;, &#39;tagger&#39;, &#39;ner&#39;])</code></pre>
<p>If the data set is big, it might be necessary to increase <code>nlp.max_length</code>.</p>
<p>We write a function to extract the tokens (words).</p>
<pre class="python"><code>def get_tokens(doc_text):
    # This pattern is a modification of the defaul filter from the
    # Tokenizer() object in keras.preprocessing.text. 
    # It just indicates which patters no skip.
    skip_pattern = &#39;\r\n \n\n \n\n\n!&quot;-#$%&amp;()--.*+,-./:;&lt;=&gt;?@[\\]^_`{|}~\t\n\r &#39;
    
    tokens = [token.text.lower() for token in nlp(doc_text) if token.text not in skip_pattern]
    
    return tokens</code></pre>
<pre class="python"><code># Get tokens.
tokens = get_tokens(horror_str)</code></pre>
<pre class="python"><code># Let us see the first tokens.
tokens[0:9]</code></pre>
<pre><code>[&#39;in&#39;, &#39;november&#39;, &#39;1999&#39;, &#39;tourists&#39;, &#39;and&#39;, &#39;fans&#39;, &#39;of&#39;, &#39;the&#39;, &#39;blair&#39;]</code></pre>
<pre class="python"><code># Compute the number of tokens list.
len(tokens) </code></pre>
<pre><code>165870</code></pre>
</div>
<div id="feature-extraction" class="section level2">
<h2>Feature Extraction</h2>
<p>The idea to construct the feature matrix for the model is to generate sequences of words of length <code>len_0</code> + 1, where the first <code>len_0</code> words define the features and the last word the target. That is, with a sequence of words of length <code>len_0</code> we predict the next word. The model is then set as a multi-class classification problem.</p>
<p>For this use case, we are going to set <code>len_0</code> = 25.</p>
<p>For example, the first observation is:</p>
<pre class="python"><code>len_0 = 25

tokens[0:len_0]</code></pre>
<pre><code>[&#39;in&#39;,
 &#39;november&#39;,
 &#39;1999&#39;,
 &#39;tourists&#39;,
 &#39;and&#39;,
 &#39;fans&#39;,
 &#39;of&#39;,
 &#39;the&#39;,
 &#39;blair&#39;,
 &#39;witch&#39;,
 &#39;project&#39;,
 &#39;descend&#39;,
 &#39;on&#39;,
 &#39;the&#39;,
 &#39;small&#39;,
 &#39;town&#39;,
 &#39;of&#39;,
 &#39;burkittsville&#39;,
 &#39;maryland&#39;,
 &#39;where&#39;,
 &#39;the&#39;,
 &#39;film&#39;,
 &#39;is&#39;,
 &#39;set&#39;,
 &#39;local&#39;]</code></pre>
<p>with target:</p>
<pre class="python"><code>tokens[len_0:len_0 + 1]</code></pre>
<pre><code>[&#39;resident&#39;]</code></pre>
<p>We now generate the sequences:</p>
<pre class="python"><code>train_len = len_0 + 1

text_sequences = []

for i in range(train_len, len(tokens)):
    # Construct sequence.
    seq = tokens[i - train_len: i]
    # Append.
    text_sequences.append(seq)</code></pre>
<p>For instance, the first sequence is:</p>
<pre class="python"><code>&#39; &#39;.join(text_sequences[0])</code></pre>
<p><em>in november 1999 tourists and fans of the blair witch project descend on the small town of burkittsville maryland where the film is set local resident</em></p>
<pre class="python"><code>len(text_sequences[0])</code></pre>
<pre><code>26</code></pre>
<p>Let us see the first five sequences:</p>
<pre class="python"><code>for i in range(0, 5):
    print(&#39; &#39;.join(text_sequences[i]))
    print(&#39;-----&#39;)</code></pre>
<pre><code>in november 1999 tourists and fans of the blair witch project descend on the small town of burkittsville maryland where the film is set local resident
-----
november 1999 tourists and fans of the blair witch project descend on the small town of burkittsville maryland where the film is set local resident jeff
-----
1999 tourists and fans of the blair witch project descend on the small town of burkittsville maryland where the film is set local resident jeff a
-----
tourists and fans of the blair witch project descend on the small town of burkittsville maryland where the film is set local resident jeff a former
-----
and fans of the blair witch project descend on the small town of burkittsville maryland where the film is set local resident jeff a former psychiatric
-----</code></pre>
</div>
<div id="vectorization" class="section level2">
<h2>Vectorization</h2>
<p>The next step is to encode these character sequences as numerical features. We do this using the <a href="https://keras.io/preprocessing/text/"><code>Tokenizer</code></a> object from Keras.</p>
<pre class="python"><code>from keras.preprocessing.text import Tokenizer

tokenizer = Tokenizer()

tokenizer.fit_on_texts(text_sequences)</code></pre>
<pre class="python"><code># Get numeric sequences.
sequences = tokenizer.texts_to_sequences(text_sequences)</code></pre>
<p>For example, the first (numerical) sequence is</p>
<pre class="python"><code>sequences[0]</code></pre>
<pre><code>[8,
 12586,
 12585,
 2397,
 2,
 12584,
 5,
 1,
 5558,
 630,
 2195,
 2927,
 20,
 1,
 449,
 157,
 5,
 12583,
 7487,
 42,
 1,
 117,
 7,
 362,
 231,
 2928]</code></pre>
<p>Let us verify the first word of the sequence is indeed “in”.</p>
<pre class="python"><code>tokenizer.index_word[8]</code></pre>
<pre><code>&#39;in&#39;</code></pre>
<p>Let us save the vocabulary size = # unique tokens.</p>
<pre class="python"><code>vocabulary_size = len(tokenizer.word_counts)

vocabulary_size</code></pre>
<pre><code>12586</code></pre>
<pre class="python"><code># We store the sequences in a numpy array.
sequences = np.array(sequences)</code></pre>
<pre class="python"><code>sequences</code></pre>
<pre><code>array([[    8, 12586, 12585, ...,   362,   231,  2928],
       [12586, 12585,  2397, ...,   231,  2928,   297],
       [12585,  2397,     2, ...,  2928,   297,     4],
       ...,
       [   20,     4,  1551, ...,     1,    59,     5],
       [    4,  1551,  1684, ...,    59,     5,     6],
       [ 1551,  1684,    22, ...,     5,     6,   169]])</code></pre>
</div>
<div id="x---y-split" class="section level2">
<h2>X - y Split</h2>
<p>We now construct the observation matrix <code>X</code> and the label vector <code>y</code>.</p>
<pre class="python"><code>from keras.utils import to_categorical

# select all but last word indices.
X = sequences[:, :-1]
X</code></pre>
<pre><code>array([[    8, 12586, 12585, ...,     7,   362,   231],
       [12586, 12585,  2397, ...,   362,   231,  2928],
       [12585,  2397,     2, ...,   231,  2928,   297],
       ...,
       [   20,     4,  1551, ...,    22,     1,    59],
       [    4,  1551,  1684, ...,     1,    59,     5],
       [ 1551,  1684,    22, ...,    59,     5,     6]])</code></pre>
<pre class="python"><code>X.shape</code></pre>
<pre><code>(165844, 25)</code></pre>
<pre class="python"><code>seq_len = X.shape[1]</code></pre>
<pre class="python"><code># select all last word indices.
y = sequences[:, -1]
y</code></pre>
<pre><code>array([2928,  297,    4, ...,    5,    6,  169])</code></pre>
<pre class="python"><code># Convert to categorical (we add + 1 because Keras needs a placeholder).
y = to_categorical(y, num_classes=(vocabulary_size + 1))
y</code></pre>
<pre><code>array([[0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       ...,
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.],
       [0., 0., 0., ..., 0., 0., 0.]], dtype=float32)</code></pre>
</div>
<div id="model-definition" class="section level2">
<h2>Model Definition</h2>
<p>Next, we define the (Sequential) network architecture:</p>
<ul>
<li><a href="https://keras.io/layers/embeddings/">Embedding</a> layer.</li>
<li>Two <a href="https://keras.io/layers/recurrent/#lstm">LSTM</a> layers.</li>
<li>One Dense layer with <code>relu</code> activation function.</li>
<li>One final Dense layer with <code>softmax</code> activation function to output class probabilities.</li>
</ul>
<p>As a reminder, here is a schema of an LSTM layer:</p>
<center>
<figure>
<img src="../images/LSTM_Schema.png" height="350" alt="Alt text that describes the graphic" title="Title text"/>
</figure>
</center>
<p>Image Source: <a href="https://www.manning.com/books/deep-learning-with-r">Deep Learning with R</a>, page 188.</p>
<p>For the optimization:</p>
<ul>
<li>loss =‘categorical_crossentropy’</li>
<li>optimizer = ‘adam’,</li>
<li>metrics = [‘accuracy’]</li>
</ul>
<pre class="python"><code>from keras.models import Sequential
from keras.layers import Dense, LSTM, Embedding

def create_model(vocabulary_size, seq_len):
    
    model = Sequential()
    
    model.add(Embedding(input_dim=vocabulary_size, 
                        output_dim=seq_len, 
                        input_length=seq_len))
    
    model.add(LSTM(units=50, return_sequences=True))
    
    model.add(LSTM(units=50))
    
    model.add(Dense(units=50, activation=&#39;relu&#39;))
    
    model.add(Dense(units=vocabulary_size, activation=&#39;softmax&#39;))
    
    model.compile(loss=&#39;categorical_crossentropy&#39;, 
                  optimizer=&#39;adam&#39;, 
                  metrics=[&#39;accuracy&#39;])
    
    model.summary()
    
    return model</code></pre>
<pre class="python"><code># Let us create the model and see summary.
model = create_model(vocabulary_size=(vocabulary_size + 1), seq_len=seq_len)</code></pre>
<pre><code>    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    embedding_1 (Embedding)      (None, 25, 25)            314675    
    _________________________________________________________________
    lstm_1 (LSTM)                (None, 25, 50)            15200     
    _________________________________________________________________
    lstm_2 (LSTM)                (None, 50)                20200     
    _________________________________________________________________
    dense_1 (Dense)              (None, 50)                2550      
    _________________________________________________________________
    dense_2 (Dense)              (None, 12587)             641937    
    =================================================================
    Total params: 994,562
    Trainable params: 994,562
    Non-trainable params: 0
    _________________________________________________________________</code></pre>
</div>
<div id="model-fit" class="section level2">
<h2>Model Fit</h2>
<p>I fit the model in my local machine. With a <code>batch_size</code> of \(128\) it took \(700\) epochs to get an accuracy of around \(0.5\) (which is ok as our aim is not correcly classify all sequences) and it took around \(8\) hours.</p>
<pre class="python"><code>model.fit(x=X, y=y, batch_size=128, epochs=700, verbose=1)</code></pre>
<pre class="python"><code># Get model metrics.
loss, accuracy =  model.evaluate(x=X, y=y)</code></pre>
<pre class="python"><code>print(f&#39;Loss: {loss}\nAccuracy: {accuracy}&#39;)</code></pre>
<pre><code>Loss: 2.388542058993397
Accuracy: 0.4952485468285764</code></pre>
</div>
<div id="save-model" class="section level2">
<h2>Save Model</h2>
<p>First we save the <code>tokenizer</code>.</p>
<pre class="python"><code>from pickle import dump

dump(tokenizer, open(&#39;tokenizer&#39;, &#39;wb&#39;))</code></pre>
<p>Next we save the model.</p>
<pre class="python"><code>model.save(&#39;model.h5&#39;)</code></pre>
<p>We can reload the model as:</p>
<pre class="python"><code>from keras.models import load_model

model = load_model(&#39;model.h5&#39;)</code></pre>
</div>
<div id="generate-new-text" class="section level2">
<h2>Generate New Text</h2>
<p>Now we can use the model to generate new word sequences:</p>
<pre class="python"><code>from keras.preprocessing.sequence import pad_sequences 

def generate_text(model, tokenizer, seq_len, seed_text, num_gen_words):
    # List to store the generated words. 
    output_text = []
    # Set seed_text as input_text. 
    input_text = seed_text
    
    for i in range(num_gen_words):
        # Encode input text. 
        encoded_text = tokenizer.texts_to_sequences([input_text])[0]
        # Add if the input tesxt does not have length len_0.
        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating=&#39;pre&#39;)
        # Do the prediction. Here we automatically choose the word with highest probability. 
        pred_word_ind = model.predict_classes(pad_encoded, verbose=0)[0]
        # Convert from numeric to word. 
        pred_word = tokenizer.index_word[pred_word_ind]
        # Attach predicted word. 
        input_text += &#39; &#39; + pred_word
        # Append new word to the list. 
        output_text.append(pred_word)
        
    return &#39; &#39;.join(output_text)</code></pre>
<p>Let us see how this works in practice.</p>
<div id="example-1" class="section level3">
<h3>Example 1</h3>
<p>Select a subset of out training set.</p>
<pre class="python"><code>sample_text = horror_df.iloc[100][:383]
print(sample_text)</code></pre>
<blockquote>
<p><em>Officer Frank Williams (Steven Vidler) and his partner Blaine investigate an abandoned house, where they find a young woman with her eyes ripped out. A large figure with an axe then murders Blaine and Frank has his arm chopped off before he is able to shoot the attacker in the head. Afterwards, detectives find seven bodies in the house, all of which have had their eyes ripped out.</em></p>
</blockquote>
<p>Set <code>seed_text</code> to be the start of the <code>sample test</code>.</p>
<pre class="python"><code>seed_text = sample_text[:190]
print(seed_text)</code></pre>
<blockquote>
<p><em>Officer Frank Williams (Steven Vidler) and his partner Blaine investigate an abandoned house, where they find a young woman with her eyes ripped out. A large figure with an axe then murders</em></p>
</blockquote>
<p>Generate text:</p>
<pre class="python"><code>generated_text = generate_text(model=model, 
                               tokenizer=tokenizer,
                               seq_len=seq_len, 
                               seed_text=seed_text, 
                               num_gen_words=40)

print(seed_text + &#39; &#39; + generated_text + &#39;...&#39;)</code></pre>
<blockquote>
<p><em>Officer Frank Williams (Steven Vidler) and his partner Blaine investigate an abandoned house, where they find a young woman with her eyes ripped out. A large figure with an axe then murders blaine and cyrus is a sapling jeff reveals that he is the group of the family ’s house in the process to lure arthur flee in the house and goes into the morning and hyenas is taken to the house…</em></p>
</blockquote>
</div>
<div id="example-2" class="section level3">
<h3>Example 2</h3>
<p>Let us give a “horror-like” seed text.</p>
<pre class="python"><code>seed_text = &#39;the film starts in a dark house where a group of teenagers friends meet to spend the weekend when they suddenly hear&#39;</code></pre>
<pre class="python"><code>generated_text = generate_text(model=model, 
                               tokenizer=tokenizer,
                               seq_len=seq_len, 
                               seed_text=seed_text, 
                               num_gen_words=80)

print(seed_text + &#39; &#39; + generated_text + &#39;...&#39;)</code></pre>
<blockquote>
<p><em>the film starts in a dark house where a group of teenagers friends meet to spend the weekend when they suddenly hear voices he confronts john ’s mother and lina pull themselves into the morning because the group split her face towards the scene and attempts to shoot the supervision of the truth jessabelle ’s throat with regular injections of them and remains by itself blocks nazi isolating youths ’s grave and tortures him are inferior in coffins and a group of the entire bat acting in los angeles with the ghosts of the same time and lures her drugs in 1408…</em></p>
</blockquote>
</div>
<div id="example-3" class="section level3">
<h3>Example 3</h3>
<p>Now let us start with a “comedy-type” seed text.</p>
<pre class="python"><code>seed_text = movies_raw_df[movies_raw_df[&#39;Genre&#39;] == &#39;comedy&#39;][&#39;Plot&#39;].iloc[330]
print(seed_text)</code></pre>
<blockquote>
<p><em>Cocky college football star Francis Finnegan has his eye on the attractive Gloria van Dayham, as does his rival, Larry Stacey.
Francis gets a job in a department store owned by Stacey’s father, where salesgirl June Cort develops an attraction to him. Finnegan proposes that Stacey’s store sponsor a football team, which causes rival shop owner Whimple to do likewise. The team’s head cheerleader, Mimi, falls for team mascot Joe, meanwhile, and everybody pairs off with the perfect partner after the big game.</em></p>
</blockquote>
<pre class="python"><code>generated_text = generate_text(model=model, 
                               tokenizer=tokenizer,
                               seq_len=seq_len, 
                               seed_text=seed_text, 
                               num_gen_words=90)

print(seed_text + &#39; &#39; + generated_text + &#39;...&#39;)</code></pre>
<blockquote>
<p><em>Cocky college football star Francis Finnegan has his eye on the attractive Gloria van Dayham, as does his rival, Larry Stacey.
Francis gets a job in a department store owned by Stacey’s father, where salesgirl June Cort develops an attraction to him. Finnegan proposes that Stacey’s store sponsor a football team, which causes rival shop owner Whimple to do likewise. The team’s head cheerleader, Mimi, falls for team mascot Joe, meanwhile, and everybody pairs off with the perfect partner after the big game. and kills ziko is eaten but jenna learn of thrill village on a punk couple of the house and puts her to the island and erin are instructed to get beside chaos ambrosia might can trust him he gone to be and those refusing to wake that he is the ancient wooden zombie afloat in mexico and awakens in the woods and enters the road into fright she inspects iris ’s corpse which is stripped and raped by fend into chains her to the remote wall is revealed of tamara…</em></p>
</blockquote>
<p>Overall, the generated text seem to have structure. Nevrtheless, somethimes these sentences do not make a lot of sense. Quoting from <a href="https://www.manning.com/books/deep-learning-with-r">Deep Learning with R</a>, page 260:</p>
<p><strong>But, of course, don’t expect to ever generate any meaningfull text, other than by random chance: all you’re doing is sampling data from a statistical model of which <del>characters</del> words come after which <del>characters</del> words.</strong></p>
</div>
<div id="adding-temperature-parameter" class="section level3">
<h3>Adding Temperature Parameter</h3>
<p>Actually, in the <code>generate_text</code> function we are not sampling, but rather selecting the word with highest probability. We can relax this by really sampling over the learned distribution. Moreover, we can introduce a parameter, known as <code>temperature</code> \(T\), which spreads it to get more “creative” results. Concretely, let \(x\) be the vector distribution. Consider the transformation</p>
<p><span class="math display">\[
f_{T}(x) = C \exp(\log(x)/T), 
\]</span></p>
<p>where \(C&gt;0\) is just a normalization constant. Note that for \(T=1\) we just get the identity transformation. Observe that we can simplify \(f_T\) using the relaton.</p>
<p><span class="math display">\[
\exp(\log(x)/T) = (\exp(\log(x)))^{1/T} = x^{1/T}
\]</span></p>
<p>Let us include these changes into a new text generation function.</p>
<pre class="python"><code>def generate_text2(model, tokenizer, seq_len, seed_text, num_gen_words, temperature):
    
    output_text = []
    
    input_text = seed_text
    
    for i in range(num_gen_words):
        # Encode input text. 
        encoded_text = tokenizer.texts_to_sequences([input_text])[0]
         # Add if the input tesxt does not have length len_0.
        pad_encoded = pad_sequences([encoded_text], maxlen=seq_len, truncating=&#39;pre&#39;)
        # Get learned distribution.
        pred_distribution = model.predict(pad_encoded, verbose=0)[0]
        
        # Apply temperature transformation.
        new_pred_distribution = np.power(pred_distribution, (1 / temperature)) 
        new_pred_distribution = new_pred_distribution / new_pred_distribution.sum()
        
        # Sample from modified distribution.
        choices = range(new_pred_distribution.size)
 
        pred_word_ind = np.random.choice(a=choices, p=new_pred_distribution)
        
        # Convert from numeric to word. 
        pred_word = tokenizer.index_word[pred_word_ind]
        # Attach predicted word. 
        input_text += &#39; &#39; + pred_word
        # Append new word to the list. 
        output_text.append(pred_word)
        
    return &#39; &#39;.join(output_text)</code></pre>
</div>
<div id="example-2---revisited" class="section level3">
<h3>Example 2 - Revisited</h3>
<pre class="python"><code>seed_text = &#39;the film starts in a dark house where a group of teenagers friends meet to spend the weekend when they suddenly hear&#39;</code></pre>
<ul>
<li><code>temperature</code> = \(0.9\)</li>
</ul>
<pre class="python"><code>generated_text = generate_text2(model=model, 
                                tokenizer=tokenizer,
                                seq_len=seq_len, 
                                seed_text=seed_text, 
                                num_gen_words=80, 
                                temperature=0.9)

print(seed_text + &#39; &#39; + generated_text + &#39; ...&#39;)</code></pre>
<blockquote>
<p><em>the film starts in a dark house where a group of teenagers friends meet to spend the weekend when they suddenly hear noises the next day owen stabs his seventh when sophie shoots her in the search such he subdues him exist pulls her bloody late eliot then discover that he can do so of kristi so will be letting her that that he had some friends constantly slicing away by confronted by nikki who to the hospital anna is stuck in a switch who grip thomas drains a incident that he strikes justin from frustration and learn it do so that …</em></p>
</blockquote>
<ul>
<li><code>temperature</code> = \(0.5\)</li>
</ul>
<pre class="python"><code>generated_text = generate_text2(model=model, 
                                tokenizer=tokenizer,
                                seq_len=seq_len, 
                                seed_text=seed_text, 
                                num_gen_words=82, 
                                temperature=0.5)

print(seed_text + &#39; &#39; + generated_text + &#39; ...&#39;)</code></pre>
<blockquote>
<p><em>the film starts in a dark house where a group of teenagers friends meet to spend the weekend when they suddenly hear noises hunter and sarah descend the bodies and history of falburn the disappearance of vampires the killer learns that she did n’t make a large pipe and return to the house and he distracts him dead the man enters the house and kills him on the cellar door by breaking into the house and points to their wounds but carrie runs into town and bearing time taggart becomes convinced that the united states light and convinces to the twelve treatment as well ….</em></p>
</blockquote>
<ul>
<li><code>temperature</code> = \(0.1\)</li>
</ul>
<pre class="python"><code>generated_text = generate_text2(model=model, 
                                tokenizer=tokenizer,
                                seq_len=seq_len, 
                                seed_text=seed_text, 
                                num_gen_words=82, 
                                temperature = 0.1)

print(seed_text + &#39; &#39; + generated_text + &#39; ...&#39;)</code></pre>
<blockquote>
<p><em>the film starts in a dark house where a group of teenagers friends meet to spend the weekend when they suddenly hear voices he meets the old woman who is bitten by a large swarm of lunges likely of her farm for help he is captured and discovers their way out the operation is not a brief struggle with the parapsychologists in a field tubbs finds stones of the house and the group straps him to the hospital where he is attacked by a skinhead gang garret volunteers to reveal that the two and informs them that the film ends which is why is …</em></p>
</blockquote>
</div>
</div>
<div id="whats-next" class="section level2">
<h2>What’s next?</h2>
<p>As mentioned in the introduction, this is just the first step in the model iteration. Next iterations can include (among many other things):</p>
<ul>
<li>Experiment with different number of neurons per layer.</li>
<li>Try new network architectures, for example adding a 1D-convnet layer.</li>
<li>Train on a bigger data set.</li>
</ul>
<p>Of course, training on GPU would allow us to iterate faster.</p>
</div>

    </div>
  </article>

  


</main>

      <footer class="footer">
        <ul class="footer-links">
          <li>
            <a href="../index.xml" type="application/rss+xml" target="_blank">RSS feed</a>
          </li>
          <li>
            <a href="https://gohugo.io/" class="footer-links-kudos">Made with <img src="../images/hugo-logo.png" alt="Img link to Hugo website" width="22" height="22"></a>
          </li>
        </ul>
      </footer>

    </div>
    



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/highlight.min.js"></script>



<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/r.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/yaml.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/python.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/dockerfile.min.js"></script>
<script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.9.0/languages/bash.min.js"></script>
<script>hljs.configure({languages: []}); hljs.initHighlightingOnLoad();</script>



    
<script src="../js/math-code.js"></script>
<script async src="//cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.6/MathJax.js?config=TeX-MML-AM_CHTML"></script>


    
<script type="application/javascript">
var doNotTrack = false;
if (!doNotTrack) {
	(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
	(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
	m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
	})(window,document,'script','https://www.google-analytics.com/analytics.js','ga');
	ga('create', 'G-5NM5EDH834', 'auto');
	
	ga('send', 'pageview');
}
</script>

  </body>
</html>

